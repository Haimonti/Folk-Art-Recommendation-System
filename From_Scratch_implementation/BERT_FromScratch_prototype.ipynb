{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1BGKDeHeP7sE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "        self.pad_token = '[PAD]'\n",
        "        self.unk_token = '[UNK]'\n",
        "        self.cls_token = '[CLS]'\n",
        "        self.mask_token = '[MASK]'\n",
        "\n",
        "        # Ensure special tokens are in the vocabulary\n",
        "        self.vocab[self.pad_token] = 0\n",
        "        self.vocab[self.unk_token] = 1\n",
        "        self.vocab[self.cls_token] = 2\n",
        "        self.vocab[self.mask_token] = 3\n",
        "\n",
        "        self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        # Tokenize the text and map to token IDs\n",
        "        tokens = text.split()\n",
        "        token_ids = [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
        "        return torch.tensor(token_ids)\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        # Decode token ids back to text\n",
        "        tokens = [self.reverse_vocab.get(id.item(), self.unk_token) for id in token_ids]\n",
        "        return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "q77Znc6mQJgJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT Model for Embedding Extraction\n",
        "class BERTEmbeddingExtractor(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, max_len):\n",
        "        super(BERTEmbeddingExtractor, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.position_embedding = nn.Embedding(max_len, embedding_dim)\n",
        "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=8),\n",
        "            num_layers=6\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, embedding_dim))  # CLS token for embedding extraction\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        seq_len = input_ids.size(1)\n",
        "        positions = torch.arange(0, seq_len).unsqueeze(0).to(input_ids.device)\n",
        "        token_embeddings = self.embedding(input_ids)\n",
        "        position_embeddings = self.position_embedding(positions)\n",
        "        embeddings = token_embeddings + position_embeddings\n",
        "        embeddings = self.layer_norm(embeddings)\n",
        "        transformer_output = self.transformer(embeddings)\n",
        "        return transformer_output  # Return embeddings directly (not logits)\n"
      ],
      "metadata": {
        "id": "KIS1WlM_QJjY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset for MLM Training\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_len=50):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        token_ids = self.tokenizer.encode(text)\n",
        "        # Apply padding/truncation\n",
        "        token_ids = token_ids[:self.max_len]\n",
        "        token_ids = torch.cat([token_ids, torch.tensor([self.tokenizer.vocab[self.tokenizer.pad_token]] * (self.max_len - len(token_ids)))])\n",
        "\n",
        "        # Create masked version of the input for MLM\n",
        "        masked_token_ids = token_ids.clone()\n",
        "        mask_indices = random.sample(range(self.max_len), k=int(self.max_len * 0.15))  # Mask 15% of the tokens\n",
        "        for idx in mask_indices:\n",
        "            masked_token_ids[idx] = self.tokenizer.vocab[self.tokenizer.mask_token]  # Apply [MASK]\n",
        "\n",
        "        return masked_token_ids, token_ids  # masked input, original target"
      ],
      "metadata": {
        "id": "m9ruwLkTQJmZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLM Loss Function\n",
        "def mlm_loss(predictions, targets, mask_token_id):\n",
        "    mask = (targets != mask_token_id).float()  # Ignore the masked positions in the target\n",
        "    loss = torch.nn.CrossEntropyLoss(reduction='none')(predictions.view(-1, predictions.size(-1)), targets.view(-1))\n",
        "    loss = loss * mask.view(-1)  # Apply mask to ignore the padded/unused tokens\n",
        "    return loss.sum() / mask.sum()  # Normalize the loss\n",
        "\n",
        "# Training Loop\n",
        "def train_model(model, dataloader, optimizer, tokenizer, epochs=5):\n",
        "    model.train()\n",
        "    mask_token_id = tokenizer.vocab[tokenizer.mask_token]\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (masked_input, original_target) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            masked_input = masked_input.to(next(model.parameters()).device)\n",
        "            original_target = original_target.to(next(model.parameters()).device)\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(masked_input)\n",
        "            predictions = output.view(-1, output.size(-1))  # Flatten the output to match the shape of the target\n",
        "\n",
        "            # Compute loss\n",
        "            loss = mlm_loss(predictions, original_target, mask_token_id)\n",
        "            loss.backward()\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(dataloader)}], Loss: {running_loss / 100:.4f}\")\n",
        "                running_loss = 0.0"
      ],
      "metadata": {
        "id": "tKN1VmCXUpR6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_embeddings_to_csv(model, texts, tokenizer, csv_file=\"embeddings.csv\"):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for text in texts:\n",
        "            input_ids = tokenizer.encode(text)  # Only need input_ids\n",
        "            input_ids = input_ids.unsqueeze(0).to(next(model.parameters()).device)  # Add batch dimension\n",
        "            output = model(input_ids)\n",
        "            cls_embedding = output[:, 0, :].squeeze().cpu().numpy()  # [CLS] token embedding\n",
        "            embeddings.append(cls_embedding)\n",
        "\n",
        "    # Save embeddings to a CSV\n",
        "    df = pd.DataFrame(embeddings)\n",
        "    df.to_csv(csv_file, index=False)\n",
        "    print(f\"Embeddings saved to {csv_file}\")"
      ],
      "metadata": {
        "id": "eeIjICN4QJpN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Execution\n",
        "def main():\n",
        "    # Define a small vocabulary and custom tokenizer for testing\n",
        "    vocab = {'the': 4, 'quick': 5, 'brown': 6, 'fox': 7, 'jumps': 8, 'over': 9, 'lazy': 10, 'dog': 11}\n",
        "    tokenizer = CustomTokenizer(vocab)\n",
        "\n",
        "    # Define the model\n",
        "    model = BERTEmbeddingExtractor(vocab_size=len(vocab), embedding_dim=128, max_len=50).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "    [\"the quick brown fox\", \"jumps over the lazy dog\", \"the quick fox jumps over the dog\", \"lazy dogs are jumping\"]\n",
        "    # Create dataset and dataloader\n",
        "    texts =\n",
        "    dataset = TextDataset(texts, tokenizer)\n",
        "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "    # Define optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # Train the model\n",
        "    train_model(model, dataloader, optimizer, tokenizer, epochs=5)\n",
        "\n",
        "    # Save embeddings after training\n",
        "    save_embeddings_to_csv(model, texts, tokenizer, csv_file=\"embeddings_after_training.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXAAftLYSwLt",
        "outputId": "eb540359-858b-429d-cfff-b832a8db1878"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings saved to embeddings_after_training.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "45Nduo4bSwrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nj1CB-40Swo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vfK8yzQFSwmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YjMS0RJHSwjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hI-cRPfySwg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jd1CaBjwSweH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BeaPi2DISwbY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}