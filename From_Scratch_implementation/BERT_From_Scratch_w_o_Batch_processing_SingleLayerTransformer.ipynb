{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgvApZsqOQTA",
        "outputId": "4f90ce62-269b-4da7-8dd2-9dabf96c1f64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (2.5.1)\n",
            "Requirement already satisfied: filelock in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
            "\u001b[33mWARNING: Error parsing dependencies of textract: .* suffix can only be used with `==` or `!=` operators\n",
            "    extract-msg (<=0.29.*)\n",
            "                 ~~~~~~~^\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: dgl in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from dgl) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from dgl) (3.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from dgl) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from dgl) (4.65.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from dgl) (5.9.0)\n",
            "Requirement already satisfied: torchdata>=0.5.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from dgl) (0.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->dgl) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->dgl) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->dgl) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->dgl) (2023.7.22)\n",
            "Requirement already satisfied: torch>=2 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torchdata>=0.5.0->dgl) (2.5.1)\n",
            "Requirement already satisfied: filelock in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (2023.4.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (2.1.1)\n",
            "\u001b[33mWARNING: Error parsing dependencies of textract: .* suffix can only be used with `==` or `!=` operators\n",
            "    extract-msg (<=0.29.*)\n",
            "                 ~~~~~~~^\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: torch_geometric in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torch_geometric) (3.8.3)\n",
            "Requirement already satisfied: fsspec in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torch_geometric) (2023.4.0)\n",
            "Requirement already satisfied: jinja2 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: numpy in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torch_geometric) (1.24.3)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torch_geometric) (5.9.0)\n",
            "Requirement already satisfied: pyparsing in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: requests in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from torch_geometric) (4.65.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from aiohttp->torch_geometric) (2.0.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from aiohttp->torch_geometric) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from aiohttp->torch_geometric) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.8.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from jinja2->torch_geometric) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from requests->torch_geometric) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from requests->torch_geometric) (2023.7.22)\n",
            "\u001b[33mWARNING: Error parsing dependencies of textract: .* suffix can only be used with `==` or `!=` operators\n",
            "    extract-msg (<=0.29.*)\n",
            "                 ~~~~~~~^\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: tensorflow in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (2.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.25.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.3.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.24.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: rich in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/priyapatil/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.0)\n",
            "\u001b[33mWARNING: Error parsing dependencies of textract: .* suffix can only be used with `==` or `!=` operators\n",
            "    extract-msg (<=0.29.*)\n",
            "                 ~~~~~~~^\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install dgl\n",
        "!pip install torch_geometric\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import os\n",
        "from natsort import natsorted\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import FastText\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import dgl.data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# import torch_geometric\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, Dense,Bidirectional\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# import torch_geometric\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "from dgl.nn import GraphConv\n",
        "from IPython.display import FileLink\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from natsort import natsorted\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OuW0s7AFOg8X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from natsort import natsorted\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaHYKRjcOg5l",
        "outputId": "b8a48109-64ae-4420-8b4e-307f2b835685"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/priyapatil/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/priyapatil/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fzEErygTOg2r"
      },
      "outputs": [],
      "source": [
        "# Define punctuation to remove\n",
        "stopwords_list = stopwords.words('english')\n",
        "additional_stopwords = ['33', '52']\n",
        "stopwords_list.extend(additional_stopwords)\n",
        "\n",
        "remove = string.punctuation + '’‘–—“”'\n",
        "\n",
        "# Function to remove punctuation\n",
        "def pun_remove(data):\n",
        "    for char in remove:\n",
        "        data = data.replace(char, ' ')\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GlTlzuguOgz4"
      },
      "outputs": [],
      "source": [
        "class MySentences:\n",
        "    def __init__(self, root_directory):\n",
        "        self.root_directory = root_directory\n",
        "\n",
        "    def __iter__(self):\n",
        "        # Get a sorted list of subdirectories in the root directory\n",
        "        file_list = [file for file in os.listdir(self.root_directory) if not file.startswith('.DS_Store')]\n",
        "        file_list = natsorted(file_list)\n",
        "\n",
        "        for subdirectory in file_list:\n",
        "            subdirectory_path = os.path.join(self.root_directory, subdirectory, 'txt')\n",
        "            if os.path.exists(subdirectory_path):\n",
        "                # Get a sorted list of files in the subdirectory\n",
        "                file_list_t = [file for file in os.listdir(subdirectory_path) if not file.startswith('.DS_Store')]\n",
        "                file_list_t = natsorted(file_list_t)\n",
        "\n",
        "                for filename in file_list_t:\n",
        "                    file_path = os.path.join(subdirectory_path, filename)\n",
        "                    with open(file_path, 'r') as file:\n",
        "                        A = []\n",
        "                        for line in file:\n",
        "                            line = line.lower()\n",
        "                            line = pun_remove(line)  # Custom function to remove punctuation\n",
        "                            line = word_tokenize(line)  # Tokenize the line\n",
        "                            # Remove stopwords\n",
        "                            line_Without_stopwords = [word for word in line if word not in stopwords_list]\n",
        "                            if line_Without_stopwords:\n",
        "                                A.append(line_Without_stopwords)\n",
        "                        # Flatten and combine tokens into a sentence\n",
        "                        A = list(np.concatenate(A))\n",
        "                        yield ' '.join(A)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pQ-SXE2vOgxJ"
      },
      "outputs": [],
      "source": [
        "# Vocabulary and Tokenizer\n",
        "class CustomTokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "        self.pad_token = '[PAD]'\n",
        "        self.unk_token = '[UNK]'\n",
        "        self.cls_token = '[CLS]'\n",
        "        self.mask_token = '[MASK]'\n",
        "\n",
        "        # Add special tokens to vocab\n",
        "        self.vocab[self.pad_token] = len(self.vocab)\n",
        "        self.vocab[self.unk_token] = len(self.vocab)\n",
        "        self.vocab[self.cls_token] = len(self.vocab)\n",
        "        self.vocab[self.mask_token] = len(self.vocab)\n",
        "\n",
        "        self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        tokens = text.split()\n",
        "        token_ids = [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
        "        return torch.tensor(token_ids, dtype=torch.long)\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        tokens = [self.reverse_vocab.get(id.item(), self.unk_token) for id in token_ids]\n",
        "        return \" \".join(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kf0bXefiOgul"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "class TextDataset:\n",
        "    def __init__(self, texts, tokenizer, max_len=50):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def process_all(self):\n",
        "        token_ids_list = []\n",
        "        for text in self.texts:\n",
        "            token_ids = self.tokenizer.encode(text)\n",
        "            token_ids = token_ids[:self.max_len]\n",
        "            token_ids = torch.cat([\n",
        "                token_ids,\n",
        "                torch.tensor([self.tokenizer.vocab[self.tokenizer.pad_token]] * (self.max_len - len(token_ids)))\n",
        "            ])\n",
        "            token_ids_list.append(token_ids)\n",
        "\n",
        "        # Stack all token IDs\n",
        "        return torch.stack(token_ids_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "drePBGZHOgrz"
      },
      "outputs": [],
      "source": [
        "class BERTEmbeddingExtractor(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, max_len):\n",
        "        super(BERTEmbeddingExtractor, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4),\n",
        "            num_layers=1,\n",
        "        )\n",
        "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.transformer(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def extract_embeddings(self, x):\n",
        "        \"\"\"\n",
        "        Extract embeddings from the embedding layer.\n",
        "        \"\"\"\n",
        "        return self.embeddings(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOarTFZSP6fo",
        "outputId": "d250ee34-cb13-48eb-8f5f-1a0ebe52c61c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/priyapatil/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5GBQBTGOgpF",
        "outputId": "294fa53b-0984-43fb-a0e3-4fc60cf693d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['hail durga tara destroyer sorrows invincible strong southern winds nagendranandini daughter king mountains chandi ten arms stretch ten directions third eye shines brightly middle forehead lakshmi saraswati left kartik ganesh along lion asura joya bijoya accompany mother', 'one day goddess durga bestowed grace revealed cache precious gems pomegranate grove kalketu found treasure pomegranate tree cleared forest named gujarat established town two pots gems remained kalketu filled jars sealing carefully carried jars home beautiful khullona phullora waited doorway thatched hut twelve years ascetic languishing jail meanwhile khullona given birth son srimonto srimonto grew bright young man wished go search father son light life cried khullona leave even moment shall feel abandoned must go pray durga invoked chandi durga appeared khullona handed srimonto goddess', 'shouting hail durga srimonto boarded boat boatmen set sail sailing peacefully waters magora struck misfortune caught terrible storm srimonto witnessed strange sight beautiful lady seated lotus swallowing elephant kamal kamini form durga atop hundred petalled lotus mother ganesh swallowing elephant srimonto moved wondrous sight bowed offered salutations million times']\n",
            "Total number of documents: 106\n"
          ]
        }
      ],
      "source": [
        "# Prepare data\n",
        "zip_file_path = '/Users/priyapatil/Desktop/Image_data'\n",
        "# extract_to = '/Extracted_image_data'\n",
        "sentences = MySentences(zip_file_path)\n",
        "docs = [''.join(sentence) for sentence in sentences]\n",
        "print(docs[:3])\n",
        "print(f\"Total number of documents: {len(docs)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PwEA-U7mOgmv"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary\n",
        "all_tokens = [token for doc in docs for token in word_tokenize(doc)]\n",
        "word_counts = Counter(all_tokens)\n",
        "vocab = {word: idx for idx, (word, _) in enumerate(word_counts.most_common())}\n",
        "\n",
        "tokenizer = CustomTokenizer(vocab)\n",
        "dataset = TextDataset(docs, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "C86mT7WcOgjv"
      },
      "outputs": [],
      "source": [
        "# Process all data\n",
        "inputs = dataset.process_all()\n",
        "inputs = inputs.long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7LDGteLPBET",
        "outputId": "cf837f14-c7c4-4c0b-85d5-46d43c330693"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/priyapatil/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BERTEmbeddingExtractor(vocab_size=len(tokenizer.vocab), embedding_dim=100, max_len=50).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XC-N_uE1PA-M"
      },
      "outputs": [],
      "source": [
        "# Extract embeddings for the entire dataset\n",
        "inputs = inputs\n",
        "embeddings = model.extract_embeddings(inputs)\n",
        "embeddings = embeddings.detach().cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "s8EfwhwYPO1s"
      },
      "outputs": [],
      "source": [
        "# Mean pooling\n",
        "sequence_embeddings = embeddings.mean(axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKsdj2u3POx-",
        "outputId": "30b533fd-e374-46be-b214-cd2465a61bac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings saved to embeddings.csv\n"
          ]
        }
      ],
      "source": [
        "# Save embeddings to CSV\n",
        "embeddings_df = pd.DataFrame(sequence_embeddings)\n",
        "embeddings_df.to_csv(\"embeddings.csv\", index=False)\n",
        "print(\"Embeddings saved to embeddings.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = pd.read_csv(\"/Users/priyapatil/Desktop/RecSys/ProjectData/AnimalLabels.csv\")\n",
        "labels['majority_vote'] = labels.mode(axis=1, numeric_only=True).astype(int)\n",
        "Label_df = labels[['majority_vote']]\n",
        "\n",
        "train_df, test_df, train_labels, test_labels = train_test_split(\n",
        "    embeddings_df, Label_df, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(84, 100) (22, 100) (84, 1) (22, 1)\n"
          ]
        }
      ],
      "source": [
        "print(train_df.shape,test_df.shape,train_labels.shape,test_labels.shape)\n",
        "\n",
        "similarity_matrix = cosine_similarity(train_df)\n",
        "threshold = 0.5  # Define threshold for creating edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create edges based on similarity threshold\n",
        "src, dst = [], []\n",
        "for i in range(similarity_matrix.shape[0]):\n",
        "    for j in range(i + 1, similarity_matrix.shape[1]):\n",
        "        if similarity_matrix[i, j] > threshold:\n",
        "            src.append(i)\n",
        "            dst.append(j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "g = dgl.graph((src + dst, dst + src))  # Undirected graph\n",
        "\n",
        "# Add node features (embeddings) to the graph\n",
        "g.ndata['feat'] = torch.tensor(train_df.values, dtype=torch.float32)\n",
        "\n",
        "# Random labels for the training graph (replace with actual labels)\n",
        "num_nodes = train_df.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GraphSAGENet(nn.Module):\n",
        "    def __init__(self, in_feats, hidden_feats, out_feats):\n",
        "        super(GraphSAGENet, self).__init__()\n",
        "        self.conv1 = dgl.nn.SAGEConv(in_feats, hidden_feats, aggregator_type='mean')\n",
        "        self.conv2 = dgl.nn.SAGEConv(hidden_feats, out_feats, aggregator_type='mean')\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        x = self.conv1(g, inputs)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(g, x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "    \n",
        "model = GraphSAGENet(in_feats=train_df.shape[1], hidden_feats=128, out_feats=3)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 1.8684\n",
            "Epoch 2, Loss: 0.0104\n",
            "Epoch 3, Loss: 0.0001\n",
            "Epoch 4, Loss: 0.0000\n",
            "Epoch 5, Loss: 0.0000\n",
            "Epoch 6, Loss: 0.0000\n",
            "Epoch 7, Loss: 0.0000\n",
            "Epoch 8, Loss: 0.0000\n",
            "Epoch 9, Loss: 0.0000\n",
            "Epoch 10, Loss: 0.0000\n",
            "Epoch 11, Loss: 0.0000\n",
            "Epoch 12, Loss: 0.0000\n",
            "Epoch 13, Loss: 0.0000\n",
            "Epoch 14, Loss: 0.0000\n",
            "Epoch 15, Loss: 0.0000\n",
            "Epoch 16, Loss: 0.0000\n",
            "Epoch 17, Loss: 0.0000\n",
            "Epoch 18, Loss: 0.0000\n",
            "Epoch 19, Loss: 0.0000\n",
            "Epoch 20, Loss: 0.0000\n",
            "Epoch 21, Loss: 0.0000\n",
            "Epoch 22, Loss: 0.0000\n",
            "Epoch 23, Loss: 0.0000\n",
            "Epoch 24, Loss: 0.0000\n",
            "Epoch 25, Loss: 0.0000\n",
            "Epoch 26, Loss: 0.0000\n",
            "Epoch 27, Loss: 0.0000\n",
            "Epoch 28, Loss: 0.0000\n",
            "Epoch 29, Loss: 0.0000\n",
            "Epoch 30, Loss: 0.0000\n",
            "Epoch 31, Loss: 0.0000\n",
            "Epoch 32, Loss: 0.0000\n",
            "Epoch 33, Loss: 0.0000\n",
            "Epoch 34, Loss: 0.0000\n",
            "Epoch 35, Loss: 0.0000\n",
            "Epoch 36, Loss: 0.0000\n",
            "Epoch 37, Loss: 0.0000\n",
            "Epoch 38, Loss: 0.0000\n",
            "Epoch 39, Loss: 0.0000\n",
            "Epoch 40, Loss: 0.0000\n",
            "Epoch 41, Loss: 0.0000\n",
            "Epoch 42, Loss: 0.0000\n",
            "Epoch 43, Loss: 0.0000\n",
            "Epoch 44, Loss: 0.0000\n",
            "Epoch 45, Loss: 0.0000\n",
            "Epoch 46, Loss: 0.0000\n",
            "Epoch 47, Loss: 0.0000\n",
            "Epoch 48, Loss: 0.0000\n",
            "Epoch 49, Loss: 0.0000\n",
            "Epoch 50, Loss: 0.0000\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(g, g.ndata['feat'])\n",
        "    train_labels_tensor = torch.tensor(train_labels.values, dtype=torch.long)\n",
        "    train_labels_tensor = train_labels_tensor.argmax(dim=1)\n",
        "    loss = F.nll_loss(out, train_labels_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "similarity_matrix_test = cosine_similarity(test_df)\n",
        "threshold = 0.5  # Define threshold for creating edges\n",
        "\n",
        "# Create edges based on similarity threshold\n",
        "src_test, dst_test = [], []\n",
        "for i in range(similarity_matrix_test.shape[0]):\n",
        "    for j in range(i + 1, similarity_matrix_test.shape[1]):\n",
        "        if similarity_matrix_test[i, j] > threshold:\n",
        "            src_test.append(i)\n",
        "            dst_test.append(j)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "g_test = dgl.graph((src_test + dst_test, dst_test + src_test))  # Undirected graph\n",
        "\n",
        "# Add node features (embeddings) to the graph\n",
        "g_test.ndata['feat'] = torch.tensor(test_df.values, dtype=torch.float32)\n",
        "\n",
        "num_nodes_test = test_df.shape[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_output = model(g_test, g_test.ndata['feat'])\n",
        "    test_predictions = test_output.argmax(dim=1)  # Predicted classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to NumPy for evaluation\n",
        "predictions = test_predictions.numpy()\n",
        "test_labels = test_labels.values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6818\n",
            "F1 Score: 0.5528\n"
          ]
        }
      ],
      "source": [
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "f1 = f1_score(test_labels, predictions, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
