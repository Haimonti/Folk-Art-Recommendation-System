{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04d36a3f-62ce-4bdc-8caf-5bcf55f4e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, random, zipfile, numpy as np, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from safetensors.torch import load_file\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "\n",
    "CONFIG = {\n",
    "    \"base_seed\": 171717,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\n",
    "    # trials\n",
    "    \"num_trials\": 5,\n",
    "    \"trial_seed_stride\": 1000,\n",
    "\n",
    "    # extraction\n",
    "    \"batch_size\": 32,\n",
    "\n",
    "    # TF-IDF\n",
    "    \"tfidf_max_features\": 512,\n",
    "\n",
    "    # finetuned model folders in Drive\n",
    "    \"clip_finetuned_drive_dir\": \"clip_finetuned_softmax\",\n",
    "    \"sigclip_finetuned_drive_dir\": \"sigmoidclip_finetuned_bce\",\n",
    "    \"llama_sigclip_drive_dir\": \"llamasigclip_assets\",\n",
    "\n",
    "    # ckpts if you still need\n",
    "    \"vae_ckpt_relpath\": \"multimodal_vae_400.pth\",\n",
    "    \"llamavae_ckpt_relpath\": \"llamavae_text_vae.pth\",\n",
    "    \n",
    "    # Fixed preference profiles\n",
    "    \"num_profiles\": 1500,\n",
    "    \"interaction_k\": 5,\n",
    "    \"pref_threshold\": 0.2,\n",
    "\n",
    "    # evaluation\n",
    "    \"top_k\": 5,\n",
    "    \"kfold_splits\": 5,\n",
    "    \"rec_std_mode\": \"profiles\",\n",
    "    \n",
    "    \"llama_prompt_model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"llama_prompt_max_new_tokens\": 96,\n",
    "}\n",
    "\n",
    "\n",
    "DEVICE = CONFIG[\"device\"]\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "\n",
    "# REPRODUCIBILITY HELPERS\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_global_seed(CONFIG[\"base_seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e39b6060-a1f1-4c77-b7c9-a05cee7a1015",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH   = \"all_labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf1a48ac-84b0-428b-a35a-1fd092961909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (189, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>identifier</th>\n",
       "      <th>scroll_id</th>\n",
       "      <th>panel_id</th>\n",
       "      <th>animal_label</th>\n",
       "      <th>myth_label</th>\n",
       "      <th>tree_label</th>\n",
       "      <th>image_path</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>scroll_001-pulin-panel_01</td>\n",
       "      <td>1_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>/workspace/folk/all_data/s1_1/img/s1.jpg</td>\n",
       "      <td>Hail Durga, Ma Tara, destroyer of sorrows. Inv...</td>\n",
       "      <td>1_1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>scroll_001-pulin-panel_02</td>\n",
       "      <td>1_1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/workspace/folk/all_data/s1_1/img/s2.jpg</td>\n",
       "      <td>One day, the goddess Durga bestowed her grace ...</td>\n",
       "      <td>1_1_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>scroll_001-pulin-panel_03</td>\n",
       "      <td>1_1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>/workspace/folk/all_data/s1_1/img/s3.jpg</td>\n",
       "      <td>Shouting “Hail Durga,” Srimonto boarded the bo...</td>\n",
       "      <td>1_1_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>scroll_001-pulin-panel_04</td>\n",
       "      <td>1_1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/workspace/folk/all_data/s1_1/img/s4.jpg</td>\n",
       "      <td>Finally, the boat docked at Ratnamala quay to ...</td>\n",
       "      <td>1_1_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>scroll_001-pulin-panel_05</td>\n",
       "      <td>1_1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/workspace/folk/all_data/s1_1/img/s5.jpg</td>\n",
       "      <td>The king Shalbahon was seated on a jewelled th...</td>\n",
       "      <td>1_1_5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 identifier scroll_id  panel_id  animal_label  \\\n",
       "0           0  scroll_001-pulin-panel_01       1_1         1             1   \n",
       "1           1  scroll_001-pulin-panel_02       1_1         2             0   \n",
       "2           2  scroll_001-pulin-panel_03       1_1         3             0   \n",
       "3           3  scroll_001-pulin-panel_04       1_1         4             0   \n",
       "4           4  scroll_001-pulin-panel_05       1_1         5             0   \n",
       "\n",
       "   myth_label  tree_label                                image_path  \\\n",
       "0           1           0  /workspace/folk/all_data/s1_1/img/s1.jpg   \n",
       "1           1           1  /workspace/folk/all_data/s1_1/img/s2.jpg   \n",
       "2           1           0  /workspace/folk/all_data/s1_1/img/s3.jpg   \n",
       "3           0           0  /workspace/folk/all_data/s1_1/img/s4.jpg   \n",
       "4           0           0  /workspace/folk/all_data/s1_1/img/s5.jpg   \n",
       "\n",
       "                                                text     id  \n",
       "0  Hail Durga, Ma Tara, destroyer of sorrows. Inv...  1_1_1  \n",
       "1  One day, the goddess Durga bestowed her grace ...  1_1_2  \n",
       "2  Shouting “Hail Durga,” Srimonto boarded the bo...  1_1_3  \n",
       "3  Finally, the boat docked at Ratnamala quay to ...  1_1_4  \n",
       "4  The king Shalbahon was seated on a jewelled th...  1_1_5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 151  Test size: 38\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "df[\"id\"] = df[\"scroll_id\"].astype(str) + \"_\" + df[\"panel_id\"].astype(str)\n",
    "label_cols = [\"animal_label\", \"myth_label\", \"tree_label\"]\n",
    "print(\"Data shape:\", df.shape)\n",
    "display(df.head())\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=CONFIG[\"base_seed\"],\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df  = test_df.reset_index(drop=True)\n",
    "print(\"Train size:\", len(train_df), \" Test size:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de116f8f-a37d-42a9-a917-c31b17d51d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load(\"user_pref_profiles.npz\", allow_pickle=True)\n",
    "interacted_ids = data[\"interacted_ids\"]\n",
    "preferred_mat = data[\"preferred_mat\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a4e33e3-6a04-4122-9dff-e653b470b108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing interacted ids in train_df: 0\n"
     ]
    }
   ],
   "source": [
    "train_ids = set(train_df[\"id\"].values)\n",
    "missing = [x for x in interacted_ids.flatten() if x not in train_ids]\n",
    "print(\"Missing interacted ids in train_df:\", len(missing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07e64753-f28b-435c-8b8f-7aebce04616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "REAL_PREFIX = str(Path(\"workspace/folk\").resolve())\n",
    "OLD_PREFIX  = \"/workspace/folk\"\n",
    "\n",
    "def fix_image_path(p):\n",
    "    p = str(p)\n",
    "    if Path(p).exists():\n",
    "        return p\n",
    "    p2 = p.replace(OLD_PREFIX, REAL_PREFIX)\n",
    "    if Path(p2).exists():\n",
    "        return p2\n",
    "\n",
    "    # 3) \n",
    "    p3 = p.lstrip(\"/\")\n",
    "    if Path(p3).exists():\n",
    "        return str(Path(p3).resolve())\n",
    "\n",
    "    return p2\n",
    "\n",
    "train_df[\"image_path\"] = train_df[\"image_path\"].apply(fix_image_path)\n",
    "test_df[\"image_path\"]  = test_df[\"image_path\"].apply(fix_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a36f6b3-0b7c-40cf-8451-cb6e6ac74029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fixed-path dfs.\n"
     ]
    }
   ],
   "source": [
    "train_df.to_csv(\"train_df_fixed_paths.csv\", index=False)\n",
    "test_df.to_csv(\"test_df_fixed_paths.csv\", index=False)\n",
    "print(\"Saved fixed-path dfs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "900b20d7-6170-4e37-b8ea-8dd9d7bd37d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"train_df_fixed_paths.csv\")\n",
    "test_df  = pd.read_csv(\"test_df_fixed_paths.csv\")\n",
    "\n",
    "assert \"id\" in train_df.columns and \"id\" in test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd0b545c-94b8-4d49-a32a-25658a1842ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_ROOT = \"features_pgl5\"\n",
    "os.makedirs(FEATURE_ROOT, exist_ok=True)\n",
    "np.save(f\"{FEATURE_ROOT}/train_ids.npy\", train_df[\"id\"].values)\n",
    "np.save(f\"{FEATURE_ROOT}/test_ids.npy\",  test_df[\"id\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8f9ef9-d707-48ab-8d0f-0407a9b17962",
   "metadata": {},
   "source": [
    "Save feature extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3612e83a-582c-4264-8596-693e8ff6b981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processor from: sigmoidclip_finetuned_bce\n",
      "Instantiating base CLIP model 'openai/clip-vit-base-patch32'...\n",
      "Loading finetuned weights from: sigmoidclip_finetuned_bce/model.safetensors\n",
      "SigCLIP finetuned model ready.\n",
      "Loading processor from: clip_finetuned_softmax\n",
      "Instantiating base CLIP model 'openai/clip-vit-base-patch32'...\n",
      "Loading finetuned weights from: clip_finetuned_softmax/model.safetensors\n",
      "SigCLIP finetuned model ready.\n",
      "Base CLIP ready.\n"
     ]
    }
   ],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "class CLIPExtractor(FeatureExtractor):\n",
    "    def __init__(self, model_dir=None, name=\"clip_features\", device=DEVICE):\n",
    "        super().__init__(name=name)\n",
    "        self.device = device\n",
    "\n",
    "        if model_dir is None:\n",
    "            self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "            self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n",
    "            self.model.eval()\n",
    "            print(\"Base CLIP ready.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            print(f\"Loading processor from: {model_dir}\")\n",
    "            self.processor = CLIPProcessor.from_pretrained(model_dir)\n",
    "\n",
    "            print(\"Instantiating base CLIP model 'openai/clip-vit-base-patch32'...\")\n",
    "            base_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "            safetensor_path = os.path.join(model_dir, \"model.safetensors\")\n",
    "            print(f\"Loading finetuned weights from: {safetensor_path}\")\n",
    "            state_dict = load_file(safetensor_path)\n",
    "\n",
    "            missing, unexpected = base_model.load_state_dict(state_dict, strict=False)\n",
    "            if missing: print(\"Missing keys:\", len(missing))\n",
    "            if unexpected: print(\"Unexpected keys:\", len(unexpected))\n",
    "\n",
    "            self.model = base_model.to(self.device)\n",
    "            self.model.eval()\n",
    "            print(\"SigCLIP finetuned model ready.\")\n",
    "        except Exception as e:\n",
    "            print(\"Could not load finetuned model, falling back to base CLIP.\")\n",
    "            print(\"Reason:\", repr(e))\n",
    "            self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "            self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n",
    "            self.model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract(self, image_path, text):\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=img,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "        ).to(self.device)\n",
    "\n",
    "        outputs = self.model(**inputs)\n",
    "        image_embeds = F.normalize(outputs.image_embeds.squeeze(0), dim=-1)\n",
    "        text_embeds  = F.normalize(outputs.text_embeds.squeeze(0),  dim=-1)\n",
    "\n",
    "        combined = (image_embeds + text_embeds) / 2.0                                  \n",
    "        return combined.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "SIGCLIP_FEAT_NAME = \"sigclip_finetuned_image_text_embedding__avgchunk\"\n",
    "CLIP_FT_FEAT_NAME = \"clip_finetuned_image_text_embedding__avgchunk\"\n",
    "CLIP_BASE_FEAT_NAME = \"clip_base_image_text_embedding__avgchunk\"\n",
    "\n",
    "SIGCLIP_FEAT_KEY = \"sigclip_ft\"\n",
    "CLIP_FT_FEAT_KEY = \"clip_ft\"\n",
    "CLIP_BASE_FEAT_KEY = \"clip_base\"\n",
    "\n",
    "sigclip_extractor = CLIPExtractor(\n",
    "    model_dir=CONFIG[\"sigclip_finetuned_drive_dir\"],     \n",
    "    name=SIGCLIP_FEAT_NAME,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "clip_ft_extractor = CLIPExtractor(\n",
    "    model_dir=CONFIG[\"clip_finetuned_drive_dir\"],        \n",
    "    name=CLIP_FT_FEAT_NAME,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "clip_base_extractor = CLIPExtractor(\n",
    "    model_dir=None,                            \n",
    "    name=CLIP_BASE_FEAT_NAME,\n",
    "    device=DEVICE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8d4656a-0e44-4010-86f0-22ad4dcbcde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805d033b8ed64047981d175e47074fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting sigclip_finetuned_image_text_embedding__avgchunk:   0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d790d1cf23134d2fb754e3961e9f36f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting sigclip_finetuned_image_text_embedding__avgchunk:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SigCLIP train embedding shape: (151, 512)\n",
      "SigCLIP test embedding shape : (38, 512)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324ca200e86042fcb5d1330556ceea64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting clip_finetuned_image_text_embedding__avgchunk:   0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8381b80895eb4f87885fc52fc8da5eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting clip_finetuned_image_text_embedding__avgchunk:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP-FT train embedding shape: (151, 512)\n",
      "CLIP-FT test embedding shape : (38, 512)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b03008a3c541c0a5b883a2967b4331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting clip_base_image_text_embedding__avgchunk:   0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3de5401c9cc499a9997977dce54c682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting clip_base_image_text_embedding__avgchunk:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP-BASE train embedding shape: (151, 512)\n",
      "CLIP-BASE test embedding shape : (38, 512)\n"
     ]
    }
   ],
   "source": [
    "def extract_features_chunked(df_in, extractor: CLIPExtractor, num_chunks=3, chunk_size=100, text_col=\"text\", seed=171717): #, text_col=\"text\", seed=171717\n",
    "    df_local = df_in.copy()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    results = []\n",
    "\n",
    "    for _, row in tqdm(df_local.iterrows(), total=len(df_local), desc=f\"Extracting {extractor.name}\"):\n",
    "        text = row.get(text_col, \"\")\n",
    "        image_path = row[\"image_path\"]\n",
    "\n",
    "        if pd.isna(text) or not isinstance(text, str) or len(text.strip()) == 0:\n",
    "            text_chunks = [\"\"]\n",
    "        else:\n",
    "            text = text.strip()\n",
    "            text_chunks = []\n",
    "            for _ in range(num_chunks):\n",
    "                if len(text) <= chunk_size:\n",
    "                    text_chunks.append(text)\n",
    "                else:\n",
    "                    start_idx = rng.integers(0, len(text) - chunk_size + 1)\n",
    "                    text_chunks.append(text[start_idx:start_idx + chunk_size])\n",
    "\n",
    "        chunk_features = [extractor.extract(image_path, chunk) for chunk in text_chunks]\n",
    "        avg_feature = np.mean(chunk_features, axis=0)\n",
    "        results.append(avg_feature)\n",
    "\n",
    "    df_local[extractor.name] = results\n",
    "    return df_local\n",
    "\n",
    "\n",
    "NUM_CHUNKS = 3\n",
    "CHUNK_SIZE = 100\n",
    "\n",
    "train_sig = extract_features_chunked(\n",
    "    train_df, sigclip_extractor,\n",
    "    num_chunks=NUM_CHUNKS, chunk_size=CHUNK_SIZE,\n",
    "    text_col=\"text\",\n",
    "    seed=CONFIG[\"base_seed\"]\n",
    ")\n",
    "test_sig  = extract_features_chunked(\n",
    "    test_df,  sigclip_extractor,\n",
    "    num_chunks=NUM_CHUNKS, chunk_size=CHUNK_SIZE,\n",
    "    text_col=\"text\",\n",
    "    seed=CONFIG[\"base_seed\"]\n",
    ")\n",
    "\n",
    "emb_train_sigclip = np.stack(train_sig[SIGCLIP_FEAT_NAME].values)\n",
    "emb_test_sigclip  = np.stack(test_sig[SIGCLIP_FEAT_NAME].values)\n",
    "\n",
    "print(\"SigCLIP train embedding shape:\", emb_train_sigclip.shape)\n",
    "print(\"SigCLIP test embedding shape :\", emb_test_sigclip.shape)\n",
    "\n",
    "train_cft = extract_features_chunked(train_df, clip_ft_extractor, num_chunks=NUM_CHUNKS, chunk_size=CHUNK_SIZE)\n",
    "test_cft  = extract_features_chunked(test_df,  clip_ft_extractor, num_chunks=NUM_CHUNKS, chunk_size=CHUNK_SIZE)\n",
    "\n",
    "emb_train_clipFT = np.stack(train_cft[CLIP_FT_FEAT_NAME].values)\n",
    "emb_test_clipFT  = np.stack(test_cft[CLIP_FT_FEAT_NAME].values)\n",
    "\n",
    "print(\"CLIP-FT train embedding shape:\", emb_train_clipFT.shape)\n",
    "print(\"CLIP-FT test embedding shape :\", emb_test_clipFT.shape)\n",
    "\n",
    "train_cb = extract_features_chunked(train_df, clip_base_extractor, num_chunks=NUM_CHUNKS, chunk_size=CHUNK_SIZE)\n",
    "test_cb  = extract_features_chunked(test_df,  clip_base_extractor, num_chunks=NUM_CHUNKS, chunk_size=CHUNK_SIZE)\n",
    "\n",
    "emb_train_clipBASE = np.stack(train_cb[CLIP_BASE_FEAT_NAME].values)\n",
    "emb_test_clipBASE  = np.stack(test_cb[CLIP_BASE_FEAT_NAME].values)\n",
    "\n",
    "print(\"CLIP-BASE train embedding shape:\", emb_train_clipBASE.shape)\n",
    "print(\"CLIP-BASE test embedding shape :\", emb_test_clipBASE.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88b2349d-7bb5-4560-b46c-240f87bc4639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca76d03d0faa45279648e170bd659642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206dc691c16c4282b94b266d84a3bfda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5674d529434f6b897c85228fd333cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13fb753296f4486a88146610106ff13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633f35ea62b142a8821ad2a3df17077a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615421d86ce8468d8741bcf2283f523e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c7069f51394df18d77d98130cd5a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24857725ad304ed4bbea20acacba1cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Llama rewrite text:   0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea3a5b82c5f4fea8e3288102ec91244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Llama rewrite text:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  text llama_text\n",
      "0  NaN           \n",
      "1  NaN           \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def _safe_text(x):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    if not isinstance(x, str):\n",
    "        x = str(x)\n",
    "    return x.strip()\n",
    "\n",
    "def load_prompt_llm(model_name):\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        if tok.pad_token is None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "        mdl = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        mdl.eval()\n",
    "        return tok, mdl\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not load Llama prompt model '{model_name}'. Falling back to original text.\\n  Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "LLAMA_TOK, LLAMA_MDL = load_prompt_llm(CONFIG[\"llama_prompt_model\"])\n",
    "\n",
    "@torch.no_grad()\n",
    "def rewrite_text_llama(raw_text: str) -> str:\n",
    "    raw_text = _safe_text(raw_text)\n",
    "    if raw_text == \"\":\n",
    "        return \"\"\n",
    "    if LLAMA_TOK is None or LLAMA_MDL is None:\n",
    "        return raw_text\n",
    "\n",
    "    prompt = (\n",
    "        \"You are helping build a recommender system for folk art panels.\\n\"\n",
    "        \"Rewrite the given text into a short, descriptive list of keywords and entities.\\n\"\n",
    "        \"Keep it factual. Avoid extra sentences.\\n\\n\"\n",
    "        f\"TEXT: {raw_text}\\n\"\n",
    "        \"KEYWORDS:\"\n",
    "    )\n",
    "    inputs = LLAMA_TOK(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(LLAMA_MDL.device)\n",
    "    out = LLAMA_MDL.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=CONFIG[\"llama_prompt_max_new_tokens\"],\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        pad_token_id=LLAMA_TOK.eos_token_id,\n",
    "    )\n",
    "    decoded = LLAMA_TOK.decode(out[0], skip_special_tokens=True)\n",
    "    if \"KEYWORDS:\" in decoded:\n",
    "        rewritten = decoded.split(\"KEYWORDS:\", 1)[-1].strip()\n",
    "        return rewritten if rewritten else raw_text\n",
    "    return raw_text\n",
    "\n",
    "def add_llama_text_column(df_in, src_col=\"text\", new_col=\"llama_text\"):\n",
    "    df = df_in.copy()\n",
    "    rewritten = []\n",
    "    for t in tqdm(df[src_col].tolist(), desc=\"Llama rewrite text\"):\n",
    "        rewritten.append(rewrite_text_llama(t))\n",
    "    df[new_col] = rewritten\n",
    "    return df\n",
    "\n",
    "train_df = add_llama_text_column(train_df, src_col=\"text\", new_col=\"llama_text\")\n",
    "test_df  = add_llama_text_column(test_df,  src_col=\"text\", new_col=\"llama_text\")\n",
    "\n",
    "print(train_df[[\"text\", \"llama_text\"]].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c79cc191-c52f-4900-b761-63d67f781b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de973ffc0c14c7480ab95b4718657c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting sigclip_finetuned_image_text_embedding__avgchunk:   0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc45b5c4c1744046a70da6e156ce9179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting sigclip_finetuned_image_text_embedding__avgchunk:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-SigCLIP train shape: (151, 512)\n",
      "Llama-SigCLIP test  shape: (38, 512)\n"
     ]
    }
   ],
   "source": [
    "LLAMASIG_FEAT_NAME = \"llamasigclip_image_text_embedding__avgchunk\"\n",
    "LLAMASIG_FEAT_KEY  = \"llamasigclip\"\n",
    "\n",
    "train_lls = extract_features_chunked(\n",
    "    train_df, sigclip_extractor,\n",
    "    num_chunks=NUM_CHUNKS, chunk_size=140,\n",
    "    text_col=\"llama_text\",\n",
    "    seed=CONFIG[\"base_seed\"]\n",
    ")\n",
    "test_lls = extract_features_chunked(\n",
    "    test_df, sigclip_extractor,\n",
    "    num_chunks=NUM_CHUNKS, chunk_size=140,\n",
    "    text_col=\"llama_text\",\n",
    "    seed=CONFIG[\"base_seed\"]\n",
    ")\n",
    "\n",
    "\n",
    "emb_train_llamasig = np.stack(train_lls[SIGCLIP_FEAT_NAME].values)\n",
    "emb_test_llamasig  = np.stack(test_lls[SIGCLIP_FEAT_NAME].values)\n",
    "\n",
    "print(\"Llama-SigCLIP train shape:\", emb_train_llamasig.shape)\n",
    "print(\"Llama-SigCLIP test  shape:\", emb_test_llamasig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbff53dc-d772-48ed-8ad4-6dc1f622579e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5EncoderModel, T5ForConditionalGeneration\n",
    "VAE_CONFIG = {\n",
    "    \"model_name\": \"t5-small\",\n",
    "    \"seq_len\": 64,\n",
    "    \"batch_size\": CONFIG[\"batch_size\"],\n",
    "}\n",
    "\n",
    "VAE_CKPT_PATH = os.path.join(\"Pruthvi\", CONFIG[\"vae_ckpt_relpath\"])  \n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(VAE_CONFIG[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ad1b44a-abd8-4dee-9e9f-0f7f417b00a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2797037513e4459821eb80ea0e4c27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded VAE ckpt: Pruthvi/multimodal_vae_400.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2359f033fe4b999d95dbe2a8ae38ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting VAE mu:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cc4e30b3024ceab53a99c8f08069fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting VAE mu:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE mu shapes: (151, 4096) (38, 4096)\n"
     ]
    }
   ],
   "source": [
    "image_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "class ScrollsDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, seq_len):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        image = image_transform(image)\n",
    "\n",
    "        text = row.get(\"text\", \"\")\n",
    "        if not isinstance(text, str) or text.strip() == \"\":\n",
    "            text = tokenizer.pad_token\n",
    "\n",
    "        tokens = self.tokenizer(\n",
    "            text, padding=\"max_length\", truncation=True,\n",
    "            max_length=self.seq_len, return_tensors=\"pt\"\n",
    "        )\n",
    "        return image, tokens.input_ids.squeeze(0), tokens.attention_mask.squeeze(0), row[\"id\"]\n",
    "\n",
    "def product_of_experts(mus, logvars):\n",
    "    TINY = 1e-8\n",
    "    precisions = [1.0 / (torch.exp(lv) + TINY) for lv in logvars]\n",
    "    mu_comb = sum(p * m for p, m in zip(precisions, mus)) / sum(precisions)\n",
    "    logvar_comb = torch.log(1.0 / sum(precisions) + TINY)\n",
    "    return mu_comb, logvar_comb\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(weights=None)\n",
    "        self.cnn = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_mu = nn.Linear(512 * 7 * 7, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512 * 7 * 7, latent_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.dropout(self.fc_mu(x)), self.fc_logvar(x)\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = T5EncoderModel.from_pretrained(model_name)\n",
    "        self.fc_mu = nn.Linear(self.encoder.config.d_model, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.encoder.config.d_model, latent_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        if attention_mask.sum().item() == 0:\n",
    "            b = input_ids.size(0)\n",
    "            mu = torch.zeros(b, self.fc_mu.out_features, device=input_ids.device)\n",
    "            logvar = torch.zeros_like(mu)\n",
    "            return mu, logvar\n",
    "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_repr = out.last_hidden_state[:, 0, :]\n",
    "        return self.dropout(self.fc_mu(cls_repr)), self.fc_logvar(cls_repr)\n",
    "\n",
    "class ImageDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 512 * 7 * 7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (512, 7, 7)),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),  nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),   nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, 1),    nn.Sigmoid()\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.dropout(self.fc(z))\n",
    "        return self.decoder(x)\n",
    "\n",
    "class TextDecoder(nn.Module):\n",
    "    def __init__(self, model_name, latent_dim):\n",
    "        super().__init__()\n",
    "        self.decoder = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.latent_to_prefix = nn.Linear(latent_dim, self.decoder.config.d_model)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, z, input_ids=None, attention_mask=None):\n",
    "        prefix_emb = self.dropout(self.latent_to_prefix(z)).unsqueeze(1)\n",
    "        input_embeds = self.decoder.encoder.embed_tokens(input_ids)\n",
    "        input_embeds = torch.cat([prefix_emb, input_embeds], dim=1)\n",
    "        if attention_mask is not None:\n",
    "            prefix_mask = torch.ones((attention_mask.size(0), 1), device=attention_mask.device)\n",
    "            attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
    "        return self.decoder(inputs_embeds=input_embeds, attention_mask=attention_mask, labels=input_ids)\n",
    "\n",
    "class MultiModalVAE(nn.Module):\n",
    "    def __init__(self, latent_dim, model_name):\n",
    "        super().__init__()\n",
    "        self.image_enc = ImageEncoder(latent_dim)\n",
    "        self.text_enc  = TextEncoder(model_name, latent_dim)\n",
    "        self.image_dec = ImageDecoder(latent_dim)\n",
    "        self.text_dec  = TextDecoder(model_name, latent_dim)\n",
    "\n",
    "if not os.path.exists(VAE_CKPT_PATH):\n",
    "    raise FileNotFoundError(f\"VAE checkpoint not found: {VAE_CKPT_PATH}\")\n",
    "\n",
    "ckpt = torch.load(VAE_CKPT_PATH, map_location=DEVICE)\n",
    "\n",
    "vae_model = MultiModalVAE(\n",
    "    latent_dim=ckpt[\"config\"][\"latent_dim\"],\n",
    "    model_name=ckpt[\"config\"][\"model_name\"],\n",
    ").to(DEVICE)\n",
    "\n",
    "vae_model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "vae_model.eval()\n",
    "print(\"Loaded VAE ckpt:\", VAE_CKPT_PATH)\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_mu_multimodal_vae(df_in, batch_size):\n",
    "    loader = DataLoader(\n",
    "        ScrollsDataset(df_in, tokenizer, VAE_CONFIG[\"seq_len\"]),\n",
    "        batch_size=batch_size, shuffle=False,\n",
    "        num_workers=0, pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    out_rows = []\n",
    "    for img, input_ids, attn_mask, ids in tqdm(loader, desc=\"Extracting VAE mu\"):\n",
    "        img = img.to(DEVICE, non_blocking=True)\n",
    "        input_ids = input_ids.to(DEVICE, non_blocking=True)\n",
    "        attn_mask = attn_mask.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        img_mu, img_logvar = vae_model.image_enc(img)\n",
    "        txt_mu, txt_logvar = vae_model.text_enc(input_ids, attn_mask)\n",
    "        mu, _ = product_of_experts([img_mu, txt_mu], [img_logvar, txt_logvar])\n",
    "\n",
    "        mu = mu.detach().cpu().numpy()\n",
    "        for id_, vec in zip(ids, mu):\n",
    "            out_rows.append({\"id\": id_, \"vae_mu\": vec})\n",
    "\n",
    "    out_df = pd.DataFrame(out_rows)\n",
    "    return df_in.merge(out_df, on=\"id\", how=\"left\")\n",
    "\n",
    "train_df = extract_mu_multimodal_vae(train_df, batch_size=VAE_CONFIG[\"batch_size\"])\n",
    "test_df  = extract_mu_multimodal_vae(test_df,  batch_size=VAE_CONFIG[\"batch_size\"])\n",
    "\n",
    "emb_train_vae = np.stack(train_df[\"vae_mu\"].values).astype(np.float32)\n",
    "emb_test_vae  = np.stack(test_df[\"vae_mu\"].values).astype(np.float32)\n",
    "\n",
    "print(\"VAE mu shapes:\", emb_train_vae.shape, emb_test_vae.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16eb8d03-0e02-4f5f-90f2-52f9985b166f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d054e21d12fa4584b9761995f43ee561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c717492b79f7467199c9c37108f7111f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/219M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] LlamaVAE ckpt not found; using SentenceT5 embeddings as LlamaVAE features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11753b0dbcba4873bd45107909206351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5172e57d723d4372b26859c760b63d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c32b32729294e109d6107f4704ac456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dae56421b0b41ca84190eb0b921ffcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3aea821b8a4df3aea617c60ef71f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting LlamaVAE (text):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289539de3008463b9f83334b74848418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting LlamaVAE (text):   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaVAE shapes: (151, 768) (38, 768)\n"
     ]
    }
   ],
   "source": [
    "LLAMAVAE_CKPT_PATH = os.path.join(\"Pruthvi\", CONFIG[\"llamavae_ckpt_relpath\"])  \n",
    "class SentenceT5VAEEncoder(nn.Module):\n",
    "    def __init__(self, st5_name=\"sentence-transformers/sentence-t5-base\", latent_dim=768):\n",
    "        super().__init__()\n",
    "        self.st5 = T5EncoderModel.from_pretrained(st5_name)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.proj_mu = nn.Linear(self.st5.config.d_model, latent_dim)\n",
    "        self.proj_logvar = nn.Linear(self.st5.config.d_model, latent_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.st5(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        mask = attention_mask.unsqueeze(-1).float()\n",
    "        pooled = (out.last_hidden_state * mask).sum(dim=1) / torch.clamp(mask.sum(dim=1), min=1.0)\n",
    "        mu = self.proj_mu(pooled)\n",
    "        logvar = torch.clamp(self.proj_logvar(pooled), -10, 10)\n",
    "        return mu, logvar\n",
    "\n",
    "def load_llamavae_or_fallback(latent_dim=768):\n",
    "    model = SentenceT5VAEEncoder(latent_dim=latent_dim).to(DEVICE)\n",
    "    if os.path.exists(LLAMAVAE_CKPT_PATH):\n",
    "        try:\n",
    "            ck = torch.load(LLAMAVAE_CKPT_PATH, map_location=DEVICE)\n",
    "            sd = ck[\"model\"] if isinstance(ck, dict) and \"model\" in ck else ck\n",
    "            model.load_state_dict(sd, strict=False)\n",
    "            print(\"Loaded LlamaVAE encoder ckpt:\", LLAMAVAE_CKPT_PATH)\n",
    "            model.eval()\n",
    "            return model, True\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to load LlamaVAE ckpt, falling back to SentenceT5 embeddings.\\n  Error: {e}\")\n",
    "    else:\n",
    "        print(\"[INFO] LlamaVAE ckpt not found; using SentenceT5 embeddings as LlamaVAE features.\")\n",
    "    model.eval()\n",
    "    return model, False\n",
    "\n",
    "LLAMAVAE_ENC, LLAMAVAE_HAS_CKPT = load_llamavae_or_fallback(latent_dim=768)\n",
    "LLAMAVAE_TOK = AutoTokenizer.from_pretrained(\"sentence-transformers/sentence-t5-base\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_llamavae_features_text_only(df_in, batch_size=64, max_len=128, text_col=\"text\"):\n",
    "    texts = []\n",
    "    for t in df_in[text_col].tolist():\n",
    "        t = _safe_text(t)\n",
    "        texts.append(t if t != \"\" else LLAMAVAE_TOK.pad_token)\n",
    "\n",
    "    vecs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting LlamaVAE (text)\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        tok = LLAMAVAE_TOK(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
    "        input_ids = tok[\"input_ids\"].to(DEVICE)\n",
    "        attn = tok[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "        mu, _ = LLAMAVAE_ENC(input_ids, attn)\n",
    "\n",
    "        if not LLAMAVAE_HAS_CKPT:\n",
    "            out = LLAMAVAE_ENC.st5(input_ids=input_ids, attention_mask=attn)\n",
    "            mask = attn.unsqueeze(-1).float()\n",
    "            pooled = (out.last_hidden_state * mask).sum(dim=1) / torch.clamp(mask.sum(dim=1), min=1.0)\n",
    "            vec = pooled\n",
    "        else:\n",
    "            vec = mu\n",
    "\n",
    "        vecs.append(vec.detach().cpu().numpy())\n",
    "\n",
    "    return np.concatenate(vecs, axis=0).astype(np.float32)\n",
    "\n",
    "emb_train_llamavae = extract_llamavae_features_text_only(train_df, batch_size=64, max_len=128, text_col=\"text\")\n",
    "emb_test_llamavae  = extract_llamavae_features_text_only(test_df,  batch_size=64, max_len=128, text_col=\"text\")\n",
    "\n",
    "print(\"LlamaVAE shapes:\", emb_train_llamavae.shape, emb_test_llamavae.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2dadad1-7fc4-4e05-b7da-7f19fece3c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes summary:\n",
      "SigCLIP       : (151, 512) (38, 512)\n",
      "LlamaSigCLIP  : (151, 512) (38, 512)\n",
      "VAE           : (151, 4096) (38, 4096)\n",
      "LlamaVAE      : (151, 768) (38, 768)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes summary:\")\n",
    "print(\"SigCLIP       :\", emb_train_sigclip.shape, emb_test_sigclip.shape)\n",
    "print(\"LlamaSigCLIP  :\", emb_train_llamasig.shape, emb_test_llamasig.shape)\n",
    "print(\"VAE           :\", emb_train_vae.shape, emb_test_vae.shape)\n",
    "print(\"LlamaVAE      :\", emb_train_llamavae.shape, emb_test_llamavae.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a361e4b7-84d1-45c9-93aa-759be1d8e5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] sigclip_ft -> features_pgl5/sigclip_ft | train (151, 512) test (38, 512)\n",
      "[Saved] vae_mu -> features_pgl5/vae_mu | train (151, 4096) test (38, 4096)\n",
      "[Saved] llamasigclip -> features_pgl5/llamasigclip | train (151, 512) test (38, 512)\n",
      "[Saved] llamavae -> features_pgl5/llamavae | train (151, 768) test (38, 768)\n",
      "[Saved] clip_ft -> features_pgl5/clip_ft | train (151, 512) test (38, 512)\n",
      "[Saved] clip_base -> features_pgl5/clip_base | train (151, 512) test (38, 512)\n",
      "✅ All 5 feature sets saved under: features_pgl5\n"
     ]
    }
   ],
   "source": [
    "import os, json, time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "FEATURE_ROOT = \"features_pgl5\"\n",
    "os.makedirs(FEATURE_ROOT, exist_ok=True)\n",
    "\n",
    "np.save(f\"{FEATURE_ROOT}/train_ids.npy\", train_df[\"id\"].values)\n",
    "np.save(f\"{FEATURE_ROOT}/test_ids.npy\",  test_df[\"id\"].values)\n",
    "\n",
    "def save_feature_block(feature_key, feature_name, X_train, X_test, meta_extra=None):\n",
    "    out_dir = f\"{FEATURE_ROOT}/{feature_key}\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    np.save(f\"{out_dir}/X_train.npy\", X_train.astype(np.float32))\n",
    "    np.save(f\"{out_dir}/X_test.npy\",  X_test.astype(np.float32))\n",
    "\n",
    "    meta = {\n",
    "        \"feature_key\": feature_key,\n",
    "        \"feature_name\": feature_name,\n",
    "        \"train_shape\": list(X_train.shape),\n",
    "        \"test_shape\": list(X_test.shape),\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "    if meta_extra:\n",
    "        meta.update(meta_extra)\n",
    "\n",
    "    with open(f\"{out_dir}/meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(f\"[Saved] {feature_key} -> {out_dir} | train {X_train.shape} test {X_test.shape}\")\n",
    "\n",
    "BASE_SEED = int(CONFIG.get(\"base_seed\", 0))\n",
    "\n",
    "SIGCLIP_FEAT_KEY  = \"sigclip_ft\"\n",
    "SIGCLIP_FEAT_NAME = \"sigclip_finetuned_image_text_embedding__avgfusion__avgchunk\"\n",
    "\n",
    "save_feature_block(\n",
    "    SIGCLIP_FEAT_KEY,\n",
    "    SIGCLIP_FEAT_NAME,\n",
    "    emb_train_sigclip,\n",
    "    emb_test_sigclip,\n",
    "    meta_extra={\n",
    "        \"type\": \"clip_like\",\n",
    "        \"fusion\": \"(image_embeds + text_embeds)/2\",\n",
    "        \"num_chunks\": 3,\n",
    "        \"chunk_size\": 100,\n",
    "        \"seed_for_chunk_sampling\": BASE_SEED,\n",
    "        \"model_dir\": str(CONFIG.get(\"sigclip_model_dir\", CONFIG.get(\"sigclip_finetuned_drive_dir\", \"\"))),\n",
    "        \"base_model\": \"openai/clip-vit-base-patch32\",\n",
    "    }\n",
    ")\n",
    "\n",
    "VAE_FEAT_KEY  = \"vae_mu\"\n",
    "VAE_FEAT_NAME = \"multimodal_vae_mu__poe__latent\"\n",
    "\n",
    "save_feature_block(\n",
    "    VAE_FEAT_KEY,\n",
    "    VAE_FEAT_NAME,\n",
    "    emb_train_vae,\n",
    "    emb_test_vae,\n",
    "    meta_extra={\n",
    "        \"type\": \"vae_mu\",\n",
    "        \"poe\": True,\n",
    "        \"seq_len\": int(VAE_CONFIG.get(\"seq_len\", 0)) if \"VAE_CONFIG\" in globals() else None,\n",
    "        \"batch_size\": int(VAE_CONFIG.get(\"batch_size\", 0)) if \"VAE_CONFIG\" in globals() else None,\n",
    "        \"ckpt_relpath\": str(CONFIG.get(\"vae_ckpt_relpath\", \"\")),\n",
    "    }\n",
    ")\n",
    "\n",
    "LLAMASIGCLIP_FEAT_KEY  = \"llamasigclip\"\n",
    "LLAMASIGCLIP_FEAT_NAME = \"llamasigclip_embedding__avgfusion__avgchunk\"\n",
    "\n",
    "save_feature_block(\n",
    "    LLAMASIGCLIP_FEAT_KEY,\n",
    "    LLAMASIGCLIP_FEAT_NAME,\n",
    "    emb_train_llamasig,\n",
    "    emb_test_llamasig,\n",
    "    meta_extra={\n",
    "        \"type\": \"clip_like\",\n",
    "        \"fusion\": \"(image_embeds + text_embeds)/2\",\n",
    "        \"num_chunks\": 3,\n",
    "        \"chunk_size\": 140,\n",
    "        \"seed_for_chunk_sampling\": BASE_SEED,\n",
    "        \"llama_prompt_model\": str(CONFIG.get(\"llama_prompt_model\", \"\")),\n",
    "        \"llama_prompt_max_new_tokens\": int(CONFIG.get(\"llama_prompt_max_new_tokens\", 0)),\n",
    "        \"sigclip_model_dir\": str(CONFIG.get(\"sigclip_model_dir\", CONFIG.get(\"sigclip_finetuned_drive_dir\", \"\"))),\n",
    "        \"base_model\": \"openai/clip-vit-base-patch32\",\n",
    "    }\n",
    ")\n",
    "\n",
    "LLAMAVAE_FEAT_KEY  = \"llamavae\"\n",
    "LLAMAVAE_FEAT_NAME = \"llamavae_text_embedding__mu_or_pooled\"\n",
    "\n",
    "save_feature_block(\n",
    "    LLAMAVAE_FEAT_KEY,\n",
    "    LLAMAVAE_FEAT_NAME,\n",
    "    emb_train_llamavae,\n",
    "    emb_test_llamavae,\n",
    "    meta_extra={\n",
    "        \"type\": \"llamavae_text\",\n",
    "        \"ckpt_relpath\": str(CONFIG.get(\"llamavae_ckpt_relpath\", \"\")),\n",
    "    }\n",
    ")\n",
    "\n",
    "CLIP_FT_FEAT_KEY  = \"clip_ft\"\n",
    "CLIP_FT_FEAT_NAME = \"clip_finetuned_image_text_embedding__avgfusion__avgchunk\"\n",
    "\n",
    "save_feature_block(\n",
    "    CLIP_FT_FEAT_KEY,\n",
    "    CLIP_FT_FEAT_NAME,\n",
    "    emb_train_clipFT,\n",
    "    emb_test_clipFT,\n",
    "    meta_extra={\n",
    "        \"type\": \"clip_like\",\n",
    "        \"fusion\": \"(image_embeds + text_embeds)/2\",\n",
    "        \"num_chunks\": 3,\n",
    "        \"chunk_size\": 100,\n",
    "        \"seed_for_chunk_sampling\": BASE_SEED,\n",
    "        \"model_dir\": str(CONFIG.get(\"clip_model_dir\", CONFIG.get(\"clip_finetuned_drive_dir\", \"\"))),\n",
    "        \"base_model\": \"openai/clip-vit-base-patch32\",\n",
    "    }\n",
    ")\n",
    "\n",
    "CLIP_BASE_FEAT_KEY  = \"clip_base\"\n",
    "CLIP_BASE_FEAT_NAME = \"clip_base_image_text_embedding__avgfusion__avgchunk\"\n",
    "\n",
    "save_feature_block(\n",
    "    CLIP_BASE_FEAT_KEY,\n",
    "    CLIP_BASE_FEAT_NAME,\n",
    "    emb_train_clipBASE,\n",
    "    emb_test_clipBASE,\n",
    "    meta_extra={\n",
    "        \"type\": \"clip_like\",\n",
    "        \"fusion\": \"(image_embeds + text_embeds)/2\",\n",
    "        \"num_chunks\": 3,\n",
    "        \"chunk_size\": 100,\n",
    "        \"seed_for_chunk_sampling\": BASE_SEED,\n",
    "        \"model\": \"openai/clip-vit-base-patch32\",\n",
    "        \"model_dir\": None,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "print(\"All feature sets saved under:\", FEATURE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad655048-21a6-4844-aea3-ef901906b128",
   "metadata": {},
   "source": [
    "# ===== 4) TF-IDF =====\n",
    "TFIDF_FEAT_KEY  = \"tfidf\"\n",
    "TFIDF_FEAT_NAME = f\"tfidf_text_vector__max{int(CONFIG.get('tfidf_max_features', 0) or 0)}\"\n",
    "\n",
    "save_feature_block(\n",
    "    TFIDF_FEAT_KEY,\n",
    "    TFIDF_FEAT_NAME,\n",
    "    emb_train_tfidf,\n",
    "    emb_test_tfidf,\n",
    "    meta_extra={\n",
    "        \"type\": \"tfidf\",\n",
    "        \"tfidf_max_features\": int(CONFIG.get(\"tfidf_max_features\", 0)),\n",
    "    }\n",
    ")\n",
    "\n",
    "# ===== 5) ResNet50 =====\n",
    "RESNET_FEAT_KEY  = \"resnet50\"\n",
    "RESNET_FEAT_NAME = \"resnet50_imagenet_image_embedding__l2norm\"\n",
    "\n",
    "save_feature_block(\n",
    "    RESNET_FEAT_KEY,\n",
    "    RESNET_FEAT_NAME,\n",
    "    emb_train_resnet,\n",
    "    emb_test_resnet,\n",
    "    meta_extra={\n",
    "        \"type\": \"resnet50\",\n",
    "        \"weights\": \"IMAGENET1K_V1\",\n",
    "        \"pooling\": \"avgpool\",\n",
    "        \"l2_normalized\": True,\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
