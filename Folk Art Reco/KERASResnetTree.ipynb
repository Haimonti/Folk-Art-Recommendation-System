{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5269fd86",
      "metadata": {
        "id": "5269fd86"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import os\n",
        "from natsort import natsorted\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import string\n",
        "from numpy.linalg import norm\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import FastText\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import dgl.data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, Dense,Bidirectional\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "import numpy as np\n",
        "from dgl.nn import GraphConv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import os\n",
        "from natsort import natsorted\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import FastText\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import dgl.data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, Dense,Bidirectional\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input\n",
        "from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16,preprocess_input\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl torch_geometric tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g88TYNFQ9PW",
        "outputId": "8613a998-8c2a-4abe-a34b-69cf07366157"
      },
      "id": "7g88TYNFQ9PW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl\n",
            "  Downloading dgl-2.1.0-cp310-cp310-manylinux1_x86_64.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.2.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: torchdata>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (0.7.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2024.2.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata>=0.5.0->dgl) (2.2.1+cu121)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.4.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.13.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.12)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m875.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, torch_geometric, nvidia-cusolver-cu12, dgl\n",
            "Successfully installed dgl-2.1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torch_geometric-2.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78493f0c-b58b-45b4-8979-c48b0adc3428",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78493f0c-b58b-45b4-8979-c48b0adc3428",
        "outputId": "6c83afaf-5e58-4bc6-8fdf-80a5fbd46949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
            "102967424/102967424 [==============================] - 1s 0us/step\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)   (None, 230, 230, 3)          0         ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)         (None, 112, 112, 64)         9472      ['conv1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " conv1_bn (BatchNormalizati  (None, 112, 112, 64)         256       ['conv1_conv[0][0]']          \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)     (None, 112, 112, 64)         0         ['conv1_bn[0][0]']            \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)   (None, 114, 114, 64)         0         ['conv1_relu[0][0]']          \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)   (None, 56, 56, 64)           0         ['pool1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2  (None, 56, 56, 64)           4160      ['pool1_pool[0][0]']          \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2  (None, 56, 56, 64)           36928     ['conv2_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2  (None, 56, 56, 256)          16640     ['pool1_pool[0][0]']          \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNo  (None, 56, 56, 256)          1024      ['conv2_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_3_bn (BatchNo  (None, 56, 56, 256)          1024      ['conv2_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_add (Add)      (None, 56, 56, 256)          0         ['conv2_block1_0_bn[0][0]',   \n",
            "                                                                     'conv2_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block1_out (Activati  (None, 56, 56, 256)          0         ['conv2_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2  (None, 56, 56, 64)           16448     ['conv2_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2  (None, 56, 56, 64)           36928     ['conv2_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_3_bn (BatchNo  (None, 56, 56, 256)          1024      ['conv2_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_add (Add)      (None, 56, 56, 256)          0         ['conv2_block1_out[0][0]',    \n",
            "                                                                     'conv2_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block2_out (Activati  (None, 56, 56, 256)          0         ['conv2_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2  (None, 56, 56, 64)           16448     ['conv2_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2  (None, 56, 56, 64)           36928     ['conv2_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_3_bn (BatchNo  (None, 56, 56, 256)          1024      ['conv2_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_add (Add)      (None, 56, 56, 256)          0         ['conv2_block2_out[0][0]',    \n",
            "                                                                     'conv2_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block3_out (Activati  (None, 56, 56, 256)          0         ['conv2_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2  (None, 28, 28, 128)          32896     ['conv2_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2  (None, 28, 28, 128)          147584    ['conv3_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2  (None, 28, 28, 512)          131584    ['conv2_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_0_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_3_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_add (Add)      (None, 28, 28, 512)          0         ['conv3_block1_0_bn[0][0]',   \n",
            "                                                                     'conv3_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block1_out (Activati  (None, 28, 28, 512)          0         ['conv3_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2  (None, 28, 28, 128)          65664     ['conv3_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2  (None, 28, 28, 128)          147584    ['conv3_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_3_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_add (Add)      (None, 28, 28, 512)          0         ['conv3_block1_out[0][0]',    \n",
            "                                                                     'conv3_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block2_out (Activati  (None, 28, 28, 512)          0         ['conv3_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2  (None, 28, 28, 128)          65664     ['conv3_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2  (None, 28, 28, 128)          147584    ['conv3_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_3_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_add (Add)      (None, 28, 28, 512)          0         ['conv3_block2_out[0][0]',    \n",
            "                                                                     'conv3_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block3_out (Activati  (None, 28, 28, 512)          0         ['conv3_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2  (None, 28, 28, 128)          65664     ['conv3_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2  (None, 28, 28, 128)          147584    ['conv3_block4_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_3_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block4_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_add (Add)      (None, 28, 28, 512)          0         ['conv3_block3_out[0][0]',    \n",
            "                                                                     'conv3_block4_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block4_out (Activati  (None, 28, 28, 512)          0         ['conv3_block4_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2  (None, 14, 14, 256)          131328    ['conv3_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2  (None, 14, 14, 1024)         525312    ['conv3_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_0_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block1_0_bn[0][0]',   \n",
            "                                                                     'conv4_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block1_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block1_out[0][0]',    \n",
            "                                                                     'conv4_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block2_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block2_out[0][0]',    \n",
            "                                                                     'conv4_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block3_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block4_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block4_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block3_out[0][0]',    \n",
            "                                                                     'conv4_block4_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block4_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block4_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block5_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block5_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block5_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block5_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block5_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block5_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block5_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block4_out[0][0]',    \n",
            "                                                                     'conv4_block5_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block5_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block5_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block5_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block6_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block6_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block6_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block6_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block6_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block6_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block6_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block6_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block5_out[0][0]',    \n",
            "                                                                     'conv4_block6_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block6_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block6_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2  (None, 7, 7, 512)            524800    ['conv4_block6_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2  (None, 7, 7, 512)            2359808   ['conv5_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block1_0_conv (Conv2  (None, 7, 7, 2048)           2099200   ['conv4_block6_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_0_bn (BatchNo  (None, 7, 7, 2048)           8192      ['conv5_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_3_bn (BatchNo  (None, 7, 7, 2048)           8192      ['conv5_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_add (Add)      (None, 7, 7, 2048)           0         ['conv5_block1_0_bn[0][0]',   \n",
            "                                                                     'conv5_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block1_out (Activati  (None, 7, 7, 2048)           0         ['conv5_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2  (None, 7, 7, 512)            1049088   ['conv5_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2  (None, 7, 7, 512)            2359808   ['conv5_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block2_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_3_bn (BatchNo  (None, 7, 7, 2048)           8192      ['conv5_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_add (Add)      (None, 7, 7, 2048)           0         ['conv5_block1_out[0][0]',    \n",
            "                                                                     'conv5_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block2_out (Activati  (None, 7, 7, 2048)           0         ['conv5_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2  (None, 7, 7, 512)            1049088   ['conv5_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2  (None, 7, 7, 512)            2359808   ['conv5_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block3_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_3_bn (BatchNo  (None, 7, 7, 2048)           8192      ['conv5_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_add (Add)      (None, 7, 7, 2048)           0         ['conv5_block2_out[0][0]',    \n",
            "                                                                     'conv5_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block3_out (Activati  (None, 7, 7, 2048)           0         ['conv5_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " avg_pool (GlobalAveragePoo  (None, 2048)                 0         ['conv5_block3_out[0][0]']    \n",
            " ling2D)                                                                                          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23587712 (89.98 MB)\n",
            "Trainable params: 23534592 (89.78 MB)\n",
            "Non-trainable params: 53120 (207.50 KB)\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model=ResNet50()\n",
        "model1=Model(inputs=model.inputs,outputs=model.layers[-2].output)\n",
        "print(model1.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5b3e9f7-a59a-43c8-80eb-3f565637ea10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5b3e9f7-a59a-43c8-80eb-3f565637ea10",
        "outputId": "243d6dd5-1143-4af2-d249-776ca1c5582c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d58485c-a94a-4de2-a603-c0179e9aa7fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d58485c-a94a-4de2-a603-c0179e9aa7fa",
        "outputId": "0eb6c647-98ea-4de2-9c91-4dcea6e01a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)   (None, 230, 230, 3)          0         ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)         (None, 112, 112, 64)         9472      ['conv1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " conv1_bn (BatchNormalizati  (None, 112, 112, 64)         256       ['conv1_conv[0][0]']          \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)     (None, 112, 112, 64)         0         ['conv1_bn[0][0]']            \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)   (None, 114, 114, 64)         0         ['conv1_relu[0][0]']          \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)   (None, 56, 56, 64)           0         ['pool1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2  (None, 56, 56, 64)           4160      ['pool1_pool[0][0]']          \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2  (None, 56, 56, 64)           36928     ['conv2_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2  (None, 56, 56, 256)          16640     ['pool1_pool[0][0]']          \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNo  (None, 56, 56, 256)          1024      ['conv2_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_3_bn (BatchNo  (None, 56, 56, 256)          1024      ['conv2_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_add (Add)      (None, 56, 56, 256)          0         ['conv2_block1_0_bn[0][0]',   \n",
            "                                                                     'conv2_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block1_out (Activati  (None, 56, 56, 256)          0         ['conv2_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2  (None, 56, 56, 64)           16448     ['conv2_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2  (None, 56, 56, 64)           36928     ['conv2_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_3_bn (BatchNo  (None, 56, 56, 256)          1024      ['conv2_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_add (Add)      (None, 56, 56, 256)          0         ['conv2_block1_out[0][0]',    \n",
            "                                                                     'conv2_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block2_out (Activati  (None, 56, 56, 256)          0         ['conv2_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2  (None, 56, 56, 64)           16448     ['conv2_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2  (None, 56, 56, 64)           36928     ['conv2_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_3_bn (BatchNo  (None, 56, 56, 256)          1024      ['conv2_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_add (Add)      (None, 56, 56, 256)          0         ['conv2_block2_out[0][0]',    \n",
            "                                                                     'conv2_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv2_block3_out (Activati  (None, 56, 56, 256)          0         ['conv2_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2  (None, 28, 28, 128)          32896     ['conv2_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2  (None, 28, 28, 128)          147584    ['conv3_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2  (None, 28, 28, 512)          131584    ['conv2_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_0_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_3_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_add (Add)      (None, 28, 28, 512)          0         ['conv3_block1_0_bn[0][0]',   \n",
            "                                                                     'conv3_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block1_out (Activati  (None, 28, 28, 512)          0         ['conv3_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2  (None, 28, 28, 128)          65664     ['conv3_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2  (None, 28, 28, 128)          147584    ['conv3_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_3_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_add (Add)      (None, 28, 28, 512)          0         ['conv3_block1_out[0][0]',    \n",
            "                                                                     'conv3_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block2_out (Activati  (None, 28, 28, 512)          0         ['conv3_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2  (None, 28, 28, 128)          65664     ['conv3_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2  (None, 28, 28, 128)          147584    ['conv3_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_3_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_add (Add)      (None, 28, 28, 512)          0         ['conv3_block2_out[0][0]',    \n",
            "                                                                     'conv3_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block3_out (Activati  (None, 28, 28, 512)          0         ['conv3_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2  (None, 28, 28, 128)          65664     ['conv3_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2  (None, 28, 28, 128)          147584    ['conv3_block4_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_3_bn (BatchNo  (None, 28, 28, 512)          2048      ['conv3_block4_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_add (Add)      (None, 28, 28, 512)          0         ['conv3_block3_out[0][0]',    \n",
            "                                                                     'conv3_block4_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv3_block4_out (Activati  (None, 28, 28, 512)          0         ['conv3_block4_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2  (None, 14, 14, 256)          131328    ['conv3_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2  (None, 14, 14, 1024)         525312    ['conv3_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_0_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block1_0_bn[0][0]',   \n",
            "                                                                     'conv4_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block1_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block1_out[0][0]',    \n",
            "                                                                     'conv4_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block2_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block2_out[0][0]',    \n",
            "                                                                     'conv4_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block3_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block3_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block4_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block4_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block3_out[0][0]',    \n",
            "                                                                     'conv4_block4_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block4_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block4_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block4_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block5_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block5_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block5_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block5_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block5_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block5_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block5_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block4_out[0][0]',    \n",
            "                                                                     'conv4_block5_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block5_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block5_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2  (None, 14, 14, 256)          262400    ['conv4_block5_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block6_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block6_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2  (None, 14, 14, 256)          590080    ['conv4_block6_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block6_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block6_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block6_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block6_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_3_bn (BatchNo  (None, 14, 14, 1024)         4096      ['conv4_block6_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_add (Add)      (None, 14, 14, 1024)         0         ['conv4_block5_out[0][0]',    \n",
            "                                                                     'conv4_block6_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv4_block6_out (Activati  (None, 14, 14, 1024)         0         ['conv4_block6_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2  (None, 7, 7, 512)            524800    ['conv4_block6_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2  (None, 7, 7, 512)            2359808   ['conv5_block1_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block1_0_conv (Conv2  (None, 7, 7, 2048)           2099200   ['conv4_block6_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_0_bn (BatchNo  (None, 7, 7, 2048)           8192      ['conv5_block1_0_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_3_bn (BatchNo  (None, 7, 7, 2048)           8192      ['conv5_block1_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_add (Add)      (None, 7, 7, 2048)           0         ['conv5_block1_0_bn[0][0]',   \n",
            "                                                                     'conv5_block1_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block1_out (Activati  (None, 7, 7, 2048)           0         ['conv5_block1_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2  (None, 7, 7, 512)            1049088   ['conv5_block1_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2  (None, 7, 7, 512)            2359808   ['conv5_block2_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block2_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_3_bn (BatchNo  (None, 7, 7, 2048)           8192      ['conv5_block2_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_add (Add)      (None, 7, 7, 2048)           0         ['conv5_block1_out[0][0]',    \n",
            "                                                                     'conv5_block2_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block2_out (Activati  (None, 7, 7, 2048)           0         ['conv5_block2_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2  (None, 7, 7, 512)            1049088   ['conv5_block2_out[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2  (None, 7, 7, 512)            2359808   ['conv5_block3_1_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block3_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_3_bn (BatchNo  (None, 7, 7, 2048)           8192      ['conv5_block3_3_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_add (Add)      (None, 7, 7, 2048)           0         ['conv5_block2_out[0][0]',    \n",
            "                                                                     'conv5_block3_3_bn[0][0]']   \n",
            "                                                                                                  \n",
            " conv5_block3_out (Activati  (None, 7, 7, 2048)           0         ['conv5_block3_add[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " avg_pool (GlobalAveragePoo  (None, 2048)                 0         ['conv5_block3_out[0][0]']    \n",
            " ling2D)                                                                                          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23587712 (89.98 MB)\n",
            "Trainable params: 23534592 (89.78 MB)\n",
            "Non-trainable params: 53120 (207.50 KB)\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(model1.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3abe0404-2e1f-4818-a194-57eccea695ed",
      "metadata": {
        "id": "3abe0404-2e1f-4818-a194-57eccea695ed"
      },
      "outputs": [],
      "source": [
        "column_names = [f\"col_{i}\" for i in range(2048)]\n",
        "Image_dataframe = pd.DataFrame(columns=column_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Replace 'your_uploaded_zip_file.zip' with the actual name of your uploaded zip file\n",
        "zip_file_path = '/content/Image_text_data_Project.zip'\n",
        "extracted_folder_path = '/content/extracted_folder/'\n",
        "\n",
        "# Create the target directory for extraction\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "# Check the contents of the extracted folder\n",
        "extracted_subfolder = os.path.join(extracted_folder_path, 'Image_text_data_Project')\n",
        "extracted_subfolder_contents = os.listdir(extracted_subfolder)\n",
        "print(\"Contents of the extracted subfolder:\", extracted_subfolder_contents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXXGoA8USiVE",
        "outputId": "11563d5d-e2a6-46c3-c9c1-ec115ea8baaa"
      },
      "id": "eXXGoA8USiVE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of the extracted subfolder: ['s6', 's1', 's13', 's11', 's9', 's2', 's10', 's12', 's8', 's3', 's4', 's7', '.DS_Store', 's5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dd14b57-371e-4895-baf4-585c0f4d5989",
      "metadata": {
        "id": "2dd14b57-371e-4895-baf4-585c0f4d5989"
      },
      "outputs": [],
      "source": [
        "def Images_read(root_directory):\n",
        "    file_list = [file for file in os.listdir(root_directory) if not file.startswith('.DS_Store')]\n",
        "    file_list = natsorted(file_list)\n",
        "   # print(file_list)\n",
        "    for subdirectory in file_list:\n",
        "        subdirectory_path = os.path.join(os.path.sep,root_directory, subdirectory, 'img')\n",
        "        file_list_t = [file for file in os.listdir(subdirectory_path) if not file.startswith('.DS_Store')]\n",
        "        file_list_t = natsorted(file_list_t)\n",
        "        B=[]\n",
        "        for filename in file_list_t:\n",
        "            file_path = os.path.join(subdirectory_path, filename)\n",
        "            #print(file_path)\n",
        "            img = cv2.imread(file_path, 1)\n",
        "            R_img = cv2.resize(img, (224, 224))\n",
        "                #R_img.shape\n",
        "            reshaped_img = np.reshape(R_img, (1, 224, 224, 3))\n",
        "            features=model1.predict(reshaped_img,verbose=0)\n",
        "            Image_dataframe.loc[Image_dataframe.shape[0]+1] = features.reshape(-1)\n",
        "    return  Image_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18b28969-b59d-4089-9e66-38886c82d8c3",
      "metadata": {
        "id": "18b28969-b59d-4089-9e66-38886c82d8c3"
      },
      "outputs": [],
      "source": [
        "dataframe=Images_read(extracted_subfolder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37023744-34c6-4246-9df1-001161aed2aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37023744-34c6-4246-9df1-001161aed2aa",
        "outputId": "5a72bb3b-3ee1-4c8c-aca9-08adbcaab70e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(106, 2048)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "dataframe.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7e1cd46-f66c-49d8-8c8a-92bd382150b5",
      "metadata": {
        "id": "a7e1cd46-f66c-49d8-8c8a-92bd382150b5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=100)\n",
        "pca.fit(dataframe)\n",
        "\n",
        "# Get the principal components and explained variance ratio\n",
        "components = pca.components_\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Project the data onto the principal components\n",
        "transformed_data = pd.DataFrame(pca.transform(dataframe))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73ce0479-4669-462f-bc90-1dd79dbb31d6",
      "metadata": {
        "id": "73ce0479-4669-462f-bc90-1dd79dbb31d6"
      },
      "outputs": [],
      "source": [
        "X_train=transformed_data.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3ce829-933d-4e1e-9389-0c895b18f872",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb3ce829-933d-4e1e-9389-0c895b18f872",
        "outputId": "9bc771b4-21b2-4fe5-a424-a78b96d1888a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(106, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "transformed_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de48ad9b-39dd-462b-bb0a-57bedcc0f9d8",
      "metadata": {
        "id": "de48ad9b-39dd-462b-bb0a-57bedcc0f9d8",
        "outputId": "10d5330e-ee45-4bb2-e1d1-fa98fd65c5a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            col_0       col_1       col_2       col_3       col_4       col_5  \\\n",
              "count  106.000000  106.000000  106.000000  106.000000  106.000000  106.000000   \n",
              "mean     0.380856    1.186656    0.072443    0.279810    0.326705    0.152754   \n",
              "std      0.571977    0.815125    0.154008    0.387973    0.336018    0.213446   \n",
              "min      0.000000    0.018293    0.000000    0.000000    0.000000    0.000000   \n",
              "25%      0.039436    0.589946    0.000000    0.020483    0.082157    0.015270   \n",
              "50%      0.141458    1.062546    0.006567    0.138202    0.205544    0.069491   \n",
              "75%      0.477486    1.645298    0.069316    0.352921    0.457125    0.204152   \n",
              "max      3.584100    4.428566    0.789019    2.110673    2.000313    1.142093   \n",
              "\n",
              "            col_6       col_7       col_8       col_9  ...    col_2038  \\\n",
              "count  106.000000  106.000000  106.000000  106.000000  ...  106.000000   \n",
              "mean     0.096202    0.079240    0.537015    0.266192  ...    0.321889   \n",
              "std      0.129368    0.149312    0.754007    0.406062  ...    0.391093   \n",
              "min      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
              "25%      0.000000    0.000000    0.049661    0.025223  ...    0.043960   \n",
              "50%      0.035320    0.018303    0.185069    0.113216  ...    0.178730   \n",
              "75%      0.152588    0.076766    0.745507    0.371170  ...    0.432666   \n",
              "max      0.612798    0.810878    3.882833    2.445075  ...    2.162241   \n",
              "\n",
              "         col_2039    col_2040    col_2041    col_2042    col_2043    col_2044  \\\n",
              "count  106.000000  106.000000  106.000000  106.000000  106.000000  106.000000   \n",
              "mean     0.074889    0.061936    0.165188    0.117762    0.887273    0.319193   \n",
              "std      0.122251    0.117923    0.330511    0.133833    0.708323    0.523770   \n",
              "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              "25%      0.000000    0.000000    0.000000    0.019165    0.309865    0.045982   \n",
              "50%      0.014974    0.011730    0.043068    0.072550    0.855229    0.173719   \n",
              "75%      0.102698    0.071279    0.130867    0.152006    1.263442    0.335740   \n",
              "max      0.756946    0.799137    1.663013    0.672281    3.143565    3.913043   \n",
              "\n",
              "         col_2045    col_2046    col_2047  \n",
              "count  106.000000  106.000000  106.000000  \n",
              "mean     0.198526    0.620034    0.537636  \n",
              "std      0.328426    0.570892    0.640563  \n",
              "min      0.000000    0.000000    0.000000  \n",
              "25%      0.000000    0.153279    0.110131  \n",
              "50%      0.033629    0.512154    0.345757  \n",
              "75%      0.291044    0.890691    0.709160  \n",
              "max      1.721927    2.587005    4.036142  \n",
              "\n",
              "[8 rows x 2048 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88deff5e-c0aa-4420-9761-317e15787c84\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col_0</th>\n",
              "      <th>col_1</th>\n",
              "      <th>col_2</th>\n",
              "      <th>col_3</th>\n",
              "      <th>col_4</th>\n",
              "      <th>col_5</th>\n",
              "      <th>col_6</th>\n",
              "      <th>col_7</th>\n",
              "      <th>col_8</th>\n",
              "      <th>col_9</th>\n",
              "      <th>...</th>\n",
              "      <th>col_2038</th>\n",
              "      <th>col_2039</th>\n",
              "      <th>col_2040</th>\n",
              "      <th>col_2041</th>\n",
              "      <th>col_2042</th>\n",
              "      <th>col_2043</th>\n",
              "      <th>col_2044</th>\n",
              "      <th>col_2045</th>\n",
              "      <th>col_2046</th>\n",
              "      <th>col_2047</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.380856</td>\n",
              "      <td>1.186656</td>\n",
              "      <td>0.072443</td>\n",
              "      <td>0.279810</td>\n",
              "      <td>0.326705</td>\n",
              "      <td>0.152754</td>\n",
              "      <td>0.096202</td>\n",
              "      <td>0.079240</td>\n",
              "      <td>0.537015</td>\n",
              "      <td>0.266192</td>\n",
              "      <td>...</td>\n",
              "      <td>0.321889</td>\n",
              "      <td>0.074889</td>\n",
              "      <td>0.061936</td>\n",
              "      <td>0.165188</td>\n",
              "      <td>0.117762</td>\n",
              "      <td>0.887273</td>\n",
              "      <td>0.319193</td>\n",
              "      <td>0.198526</td>\n",
              "      <td>0.620034</td>\n",
              "      <td>0.537636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.571977</td>\n",
              "      <td>0.815125</td>\n",
              "      <td>0.154008</td>\n",
              "      <td>0.387973</td>\n",
              "      <td>0.336018</td>\n",
              "      <td>0.213446</td>\n",
              "      <td>0.129368</td>\n",
              "      <td>0.149312</td>\n",
              "      <td>0.754007</td>\n",
              "      <td>0.406062</td>\n",
              "      <td>...</td>\n",
              "      <td>0.391093</td>\n",
              "      <td>0.122251</td>\n",
              "      <td>0.117923</td>\n",
              "      <td>0.330511</td>\n",
              "      <td>0.133833</td>\n",
              "      <td>0.708323</td>\n",
              "      <td>0.523770</td>\n",
              "      <td>0.328426</td>\n",
              "      <td>0.570892</td>\n",
              "      <td>0.640563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018293</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.039436</td>\n",
              "      <td>0.589946</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020483</td>\n",
              "      <td>0.082157</td>\n",
              "      <td>0.015270</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.049661</td>\n",
              "      <td>0.025223</td>\n",
              "      <td>...</td>\n",
              "      <td>0.043960</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.019165</td>\n",
              "      <td>0.309865</td>\n",
              "      <td>0.045982</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.153279</td>\n",
              "      <td>0.110131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.141458</td>\n",
              "      <td>1.062546</td>\n",
              "      <td>0.006567</td>\n",
              "      <td>0.138202</td>\n",
              "      <td>0.205544</td>\n",
              "      <td>0.069491</td>\n",
              "      <td>0.035320</td>\n",
              "      <td>0.018303</td>\n",
              "      <td>0.185069</td>\n",
              "      <td>0.113216</td>\n",
              "      <td>...</td>\n",
              "      <td>0.178730</td>\n",
              "      <td>0.014974</td>\n",
              "      <td>0.011730</td>\n",
              "      <td>0.043068</td>\n",
              "      <td>0.072550</td>\n",
              "      <td>0.855229</td>\n",
              "      <td>0.173719</td>\n",
              "      <td>0.033629</td>\n",
              "      <td>0.512154</td>\n",
              "      <td>0.345757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.477486</td>\n",
              "      <td>1.645298</td>\n",
              "      <td>0.069316</td>\n",
              "      <td>0.352921</td>\n",
              "      <td>0.457125</td>\n",
              "      <td>0.204152</td>\n",
              "      <td>0.152588</td>\n",
              "      <td>0.076766</td>\n",
              "      <td>0.745507</td>\n",
              "      <td>0.371170</td>\n",
              "      <td>...</td>\n",
              "      <td>0.432666</td>\n",
              "      <td>0.102698</td>\n",
              "      <td>0.071279</td>\n",
              "      <td>0.130867</td>\n",
              "      <td>0.152006</td>\n",
              "      <td>1.263442</td>\n",
              "      <td>0.335740</td>\n",
              "      <td>0.291044</td>\n",
              "      <td>0.890691</td>\n",
              "      <td>0.709160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.584100</td>\n",
              "      <td>4.428566</td>\n",
              "      <td>0.789019</td>\n",
              "      <td>2.110673</td>\n",
              "      <td>2.000313</td>\n",
              "      <td>1.142093</td>\n",
              "      <td>0.612798</td>\n",
              "      <td>0.810878</td>\n",
              "      <td>3.882833</td>\n",
              "      <td>2.445075</td>\n",
              "      <td>...</td>\n",
              "      <td>2.162241</td>\n",
              "      <td>0.756946</td>\n",
              "      <td>0.799137</td>\n",
              "      <td>1.663013</td>\n",
              "      <td>0.672281</td>\n",
              "      <td>3.143565</td>\n",
              "      <td>3.913043</td>\n",
              "      <td>1.721927</td>\n",
              "      <td>2.587005</td>\n",
              "      <td>4.036142</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 2048 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88deff5e-c0aa-4420-9761-317e15787c84')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-88deff5e-c0aa-4420-9761-317e15787c84 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-88deff5e-c0aa-4420-9761-317e15787c84');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-557cb37b-1a17-4f6a-aca2-bbe24dab5703\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-557cb37b-1a17-4f6a-aca2-bbe24dab5703')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-557cb37b-1a17-4f6a-aca2-bbe24dab5703 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "dataframe.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4aeaec-c6a1-49c3-94f7-de5df8c6b13d",
      "metadata": {
        "id": "fa4aeaec-c6a1-49c3-94f7-de5df8c6b13d"
      },
      "outputs": [],
      "source": [
        "X_train_df=pd.DataFrame(X_train)\n",
        "X_train_df_copy=X_train_df.copy()\n",
        "X_train_df.reset_index(inplace=True)\n",
        "\n",
        "\n",
        "Src_ID=[i for i in range(0,106) for _ in range(0,105-i)]\n",
        "Dst_ID=[i for i in range(0,106) for i in range(1+i,106)]\n",
        "elements_to_repeat=X_train.tolist()\n",
        "repetition_counts=[105-i for i in range(0,105)]\n",
        "\n",
        "Src_feature=[]\n",
        "for element, count in zip(elements_to_repeat, repetition_counts):\n",
        "    Src_feature.extend([element] * count)\n",
        "\n",
        "Dst_feature=[]\n",
        "for i in range(1,106):\n",
        "    for j in range(i,106):\n",
        "        Dst_feature.append(X_train.tolist()[j])\n",
        "\n",
        "Nodes_Data=pd.DataFrame()\n",
        "Nodes_Data['Id']=[i for i in range(0,106)]\n",
        "labels = pd.read_csv(\"TreeLabels.csv\")\n",
        "labels['majority_vote'] = labels.mode(axis=1, numeric_only=True).astype(int)\n",
        "Nodes_Data['features']=X_train.tolist()\n",
        "Nodes_Data['label']=labels['majority_vote']\n",
        "\n",
        "dup=[0 for i in range(0,106)]\n",
        "Edge=pd.DataFrame()\n",
        "Edge['Src Id']=Src_ID\n",
        "Edge['Src_feature']=Src_feature\n",
        "Edge['Dst_feature']=Dst_feature\n",
        "Edge['Dst Id']=Dst_ID\n",
        "\n",
        "\n",
        "Src_Ids=[i for i in range(0,106) for _ in range(0,106)]\n",
        "Dst_Ids = [i % 106 for i in range(106 * 106)]\n",
        "Src_features=[X_train.tolist()[i] for i in range(0,106)  for i in range(0,106)]\n",
        "elements_to_repeat=X_train.tolist()\n",
        "repetition_counts=[106 for i in range(0,106)]\n",
        "Dst_features=[]\n",
        "for element, count in zip(elements_to_repeat, repetition_counts):\n",
        "    Dst_features.extend([element] * count)\n",
        "\n",
        "Edge_Data=pd.DataFrame()\n",
        "Edge_Data['Src Ids']=Src_Ids\n",
        "Edge_Data['Src_features']=Src_features\n",
        "Edge_Data['Dst_features']=Dst_features\n",
        "Edge_Data['Dst Ids']=Dst_Ids\n",
        "\n",
        "edge_weight=[]\n",
        "for i in range(0,5565):\n",
        "    A=np.array(Edge['Src_feature'][i])\n",
        "    B=np.array(Edge['Dst_feature'][i])\n",
        "    Cosine_similarity=np.dot(A,B)/(norm(A)*norm(B))\n",
        "    edge_weight.append(Cosine_similarity)\n",
        "\n",
        "for i in range(len(edge_weight)):\n",
        "    if edge_weight[i]<0:\n",
        "        edge_weight[i]=0\n",
        "\n",
        "Edge['edge weights']=edge_weight\n",
        "\n",
        "\n",
        "edge_weights=[]\n",
        "for i in range(0,11236):\n",
        "    A=np.array(Edge_Data['Src_features'][i])\n",
        "    B=np.array(Edge_Data['Dst_features'][i])\n",
        "    Cosine_similarity=np.dot(A,B)/(norm(A)*norm(B))\n",
        "    edge_weights.append(Cosine_similarity)\n",
        "\n",
        "Edge_Data['edge weights']=edge_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "905150b5-6c90-4799-a6f4-0147c90c5070",
      "metadata": {
        "id": "905150b5-6c90-4799-a6f4-0147c90c5070"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a21cb3a5-1768-4115-adbc-ca9531823828",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a21cb3a5-1768-4115-adbc-ca9531823828",
        "outputId": "caf369cd-c487-4252-c57f-c403648e8ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    5565.000000\n",
            "mean        0.072496\n",
            "std         0.134281\n",
            "min         0.000000\n",
            "25%         0.000000\n",
            "50%         0.000000\n",
            "75%         0.103009\n",
            "max         0.899385\n",
            "Name: edge weights, dtype: float64\n",
            "95th percentile of edge weights column: 0.33766934323606884\n"
          ]
        }
      ],
      "source": [
        "print(Edge['edge weights'].describe())\n",
        "ninetyfive_percentile = Edge['edge weights'].quantile(0.95)\n",
        "print(\"95th percentile of edge weights column:\", ninetyfive_percentile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c11cc07a-abd1-4ae6-a9c9-944acdc330ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "c11cc07a-abd1-4ae6-a9c9-944acdc330ea",
        "outputId": "46bd5dfa-a4c4-44d6-c693-7064f6edea4d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-18-55db6a681009>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-55db6a681009>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    percentiles=[round(0.000000,4),round(0.000000,4),round(0.108516,4),round(,4),round(0.8181034905115703,4)]\u001b[0m\n\u001b[0m                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "percentiles=[round(0.000000,4),round(0.000000,4),round(0.108516,4),round(,4),round(0.8181034905115703,4)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3e0d5d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3e0d5d0",
        "outputId": "e2d2f0a4-9966-4825-8e12-7f944c795ca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph(num_nodes=106, num_edges=658,\n",
            "      ndata_schemes={'feat': Scheme(shape=(100,), dtype=torch.float64), 'label': Scheme(shape=(), dtype=torch.int8), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}\n",
            "      edata_schemes={'weight': Scheme(shape=(), dtype=torch.float64)})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-514107649beb>:28: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "  node_labels = torch.from_numpy(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import torch\n",
        "from dgl.data import DGLDataset\n",
        "edge_weight=[]\n",
        "\n",
        "\n",
        "class KarateClubDataset(DGLDataset):\n",
        "    def __init__(self,threshold):\n",
        "        self.threshold=threshold\n",
        "        super().__init__(name=\"karate_club\")\n",
        "\n",
        "    def process(self):\n",
        "        edge_remove=[]\n",
        "        C=edge_weights\n",
        "        for i in range(0,len(C)):\n",
        "            if C[i]<=self.threshold:\n",
        "                edge_remove.append(i)\n",
        "            else:\n",
        "                edge_weight.append(C[i])\n",
        "\n",
        "        nodes_data = Nodes_Data\n",
        "        edges_data = Edge_Data\n",
        "        features_array = np.array(nodes_data[\"features\"].tolist(), dtype=float)\n",
        "        node_features = torch.from_numpy(features_array)\n",
        "        node_labels = torch.from_numpy(\n",
        "                      nodes_data[\"label\"].astype(\"category\").cat.codes.to_numpy()\n",
        "                       ).clone().detach()  # Make the tensor writable\n",
        "\n",
        "        edge_features = torch.from_numpy(edges_data[\"edge weights\"].to_numpy())\n",
        "        edges_src = torch.from_numpy(edges_data[\"Src Ids\"].to_numpy())\n",
        "        edges_dst = torch.from_numpy(edges_data[\"Dst Ids\"].to_numpy())\n",
        "\n",
        "        self.graph = dgl.graph(\n",
        "            (edges_src, edges_dst), num_nodes=nodes_data.shape[0]\n",
        "\n",
        "        )\n",
        "\n",
        "        self.graph.ndata[\"feat\"] = node_features\n",
        "        self.graph.ndata[\"label\"] = node_labels\n",
        "        self.graph.edata[\"weight\"] = edge_features\n",
        "\n",
        "        self.graph=dgl.remove_edges(self.graph, torch.tensor(edge_remove))\n",
        "        n_nodes = nodes_data.shape[0]\n",
        "        n_train = int(n_nodes * 0.6)\n",
        "        n_val = int(n_nodes * 0.2)\n",
        "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        train_mask[:n_train] = True\n",
        "        val_mask[n_train : n_train + n_val] = True\n",
        "        test_mask[n_train + n_val :] = True\n",
        "        self.graph.ndata[\"train_mask\"] = train_mask\n",
        "        self.graph.ndata[\"val_mask\"] = val_mask\n",
        "        self.graph.ndata[\"test_mask\"] = test_mask\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.graph\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "\n",
        "dataset = KarateClubDataset(0.34)\n",
        "g = dataset[0]\n",
        "\n",
        "print(g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6ca11d1-4e73-4c8c-ab56-5a4aede6063b",
      "metadata": {
        "id": "c6ca11d1-4e73-4c8c-ab56-5a4aede6063b"
      },
      "outputs": [],
      "source": [
        "#new_g = dgl.compact_graphs(g)\n",
        "train_mask = g.ndata[\"train_mask\"]\n",
        "\n",
        "for i in range(106):\n",
        "    train_mask[i] = True\n",
        "\n",
        "indices_to_change = [4, 105, 84, 27, 98, 88, 18, 65, 9, 2, 5, 49, 99, 69, 86, 67, 7, 28, 78, 70, 18, 74]\n",
        "train_mask[indices_to_change] = False\n",
        "g.ndata[\"train_mask\"]=train_mask\n",
        "\n",
        "test_mask = ~train_mask\n",
        "g.ndata[\"test_mask\"]=test_mask\n",
        "\n",
        "Edge_Data_train = Edge_Data[~(Edge_Data['Src Ids'].isin(indices_to_change)) & ~(Edge_Data['Dst Ids'].isin(indices_to_change)) & Edge_Data['edge weights']>0.8258]\n",
        "Edge_Data_train = Edge_Data_train[Edge_Data_train['edge weights']>0.8258]\n",
        "\n",
        "\n",
        "Edge_Data_test = Edge_Data[Edge_Data['Src Ids'].isin(indices_to_change) & Edge_Data['Dst Ids'].isin(indices_to_change) & Edge_Data['edge weights']>0.8258]\n",
        "Edge_Data_test = Edge_Data_test[Edge_Data_test['edge weights']>0.8258]\n",
        "\n",
        "\n",
        "#adj_matrix=g.adj(etype=None)\n",
        "#dense_matrix = adj_matrix.to_dense().numpy()\n",
        "\n",
        "#new_column = np.arange(0,dense_matrix.shape[0])\n",
        "#new_column=np.reshape(new_column,(106,1))\n",
        "#new_column.shape\n",
        "#dense_matrix=np.concatenate((new_column,dense_matrix),axis=1)\n",
        "\n",
        "#new_row = np.arange(0,106)\n",
        "#new_row=np.reshape(new_row,(1,106))\n",
        "#new_row = np.insert(new_row, 0, 0)\n",
        "#new_row=np.reshape(new_row,(1,107))\n",
        "\n",
        "#result_matrix = np.vstack([new_row,dense_matrix])\n",
        "\n",
        "#np.savetxt('dense_matrix_avg.csv',result_matrix , delimiter=',',fmt='%d')\n",
        "#FileLink(r'dense_matrix_avg.csv')\n",
        "\n",
        "sg_train=dgl.node_subgraph(g, train_mask)\n",
        "#sg_adjacency_train=sg_train.adj()\n",
        "#dense_matrix_sg_train = sg_adjacency_train.to_dense().numpy()\n",
        "\n",
        "#np.savetxt('dense_matrix_sg_train.csv',dense_matrix_sg_train, delimiter=',',fmt='%d')\n",
        "#FileLink(r'dense_matrix_sg_train.csv')\n",
        "\n",
        "sg_test = dgl.node_subgraph(g, test_mask)\n",
        "#sg_adjacency_test=sg_test.adj()\n",
        "#dense_matrix_sg_test = sg_adjacency_test.to_dense().numpy()\n",
        "\n",
        "#np.savetxt('dense_matrix_sg_test.csv',dense_matrix_sg_test, delimiter=',',fmt='%d')\n",
        "#FileLink(r'dense_matrix_sg_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81296801",
      "metadata": {
        "id": "81296801"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GraphConv(in_feats, h_feats,norm='both')\n",
        "        self.conv2 = GraphConv(h_feats, num_classes)\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        in_feat = in_feat.float()\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.leaky_relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e5bbcac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e5bbcac",
        "outputId": "e0f1d22f-34c8-4e64-c707-0f2b1e233e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In epoch 0, loss: 1.130, train acc: 0.671 (train 0.671), test acc: 0.762 (best 0.762)\n",
            "In epoch 5, loss: 0.469, train acc: 0.776 (train 0.776), test acc: 0.619 (best 0.762)\n",
            "In epoch 10, loss: 0.421, train acc: 0.765 (train 0.800), test acc: 0.619 (best 0.762)\n",
            "In epoch 15, loss: 0.405, train acc: 0.788 (train 0.800), test acc: 0.714 (best 0.762)\n",
            "In epoch 20, loss: 0.389, train acc: 0.800 (train 0.800), test acc: 0.762 (best 0.762)\n",
            "In epoch 25, loss: 0.378, train acc: 0.765 (train 0.800), test acc: 0.762 (best 0.762)\n",
            "In epoch 30, loss: 0.369, train acc: 0.812 (train 0.812), test acc: 0.619 (best 0.762)\n",
            "In epoch 35, loss: 0.359, train acc: 0.812 (train 0.812), test acc: 0.667 (best 0.762)\n",
            "In epoch 40, loss: 0.351, train acc: 0.824 (train 0.824), test acc: 0.667 (best 0.762)\n",
            "In epoch 45, loss: 0.343, train acc: 0.812 (train 0.824), test acc: 0.667 (best 0.762)\n",
            "In epoch 50, loss: 0.335, train acc: 0.824 (train 0.824), test acc: 0.619 (best 0.762)\n",
            "In epoch 55, loss: 0.326, train acc: 0.847 (train 0.847), test acc: 0.619 (best 0.762)\n",
            "In epoch 60, loss: 0.319, train acc: 0.847 (train 0.847), test acc: 0.619 (best 0.762)\n",
            "In epoch 65, loss: 0.311, train acc: 0.847 (train 0.859), test acc: 0.619 (best 0.762)\n",
            "In epoch 70, loss: 0.303, train acc: 0.859 (train 0.859), test acc: 0.619 (best 0.762)\n",
            "In epoch 75, loss: 0.296, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.762)\n",
            "In epoch 80, loss: 0.288, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.762)\n",
            "In epoch 85, loss: 0.281, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.762)\n",
            "In epoch 90, loss: 0.273, train acc: 0.894 (train 0.894), test acc: 0.619 (best 0.762)\n",
            "In epoch 95, loss: 0.265, train acc: 0.894 (train 0.894), test acc: 0.619 (best 0.762)\n",
            "In epoch 100, loss: 0.257, train acc: 0.906 (train 0.906), test acc: 0.619 (best 0.762)\n",
            "In epoch 105, loss: 0.249, train acc: 0.906 (train 0.906), test acc: 0.619 (best 0.762)\n",
            "In epoch 110, loss: 0.240, train acc: 0.906 (train 0.906), test acc: 0.619 (best 0.762)\n",
            "In epoch 115, loss: 0.232, train acc: 0.906 (train 0.906), test acc: 0.619 (best 0.762)\n",
            "In epoch 120, loss: 0.223, train acc: 0.918 (train 0.918), test acc: 0.619 (best 0.762)\n",
            "In epoch 125, loss: 0.215, train acc: 0.929 (train 0.929), test acc: 0.524 (best 0.762)\n",
            "In epoch 130, loss: 0.207, train acc: 0.929 (train 0.929), test acc: 0.524 (best 0.762)\n",
            "In epoch 135, loss: 0.199, train acc: 0.941 (train 0.941), test acc: 0.524 (best 0.762)\n",
            "In epoch 140, loss: 0.191, train acc: 0.941 (train 0.941), test acc: 0.619 (best 0.762)\n",
            "In epoch 145, loss: 0.184, train acc: 0.941 (train 0.941), test acc: 0.619 (best 0.762)\n",
            "In epoch 150, loss: 0.177, train acc: 0.941 (train 0.941), test acc: 0.619 (best 0.762)\n",
            "In epoch 155, loss: 0.171, train acc: 0.941 (train 0.941), test acc: 0.619 (best 0.762)\n",
            "In epoch 160, loss: 0.165, train acc: 0.941 (train 0.941), test acc: 0.619 (best 0.762)\n",
            "In epoch 165, loss: 0.159, train acc: 0.941 (train 0.941), test acc: 0.619 (best 0.762)\n",
            "In epoch 170, loss: 0.154, train acc: 0.941 (train 0.941), test acc: 0.619 (best 0.762)\n",
            "In epoch 175, loss: 0.149, train acc: 0.941 (train 0.941), test acc: 0.619 (best 0.762)\n",
            "In epoch 180, loss: 0.144, train acc: 0.941 (train 0.941), test acc: 0.619 (best 0.762)\n",
            "In epoch 185, loss: 0.141, train acc: 0.941 (train 0.953), test acc: 0.619 (best 0.762)\n",
            "In epoch 190, loss: 0.136, train acc: 0.941 (train 0.953), test acc: 0.619 (best 0.762)\n",
            "In epoch 195, loss: 0.133, train acc: 0.941 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 200, loss: 0.129, train acc: 0.941 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 205, loss: 0.126, train acc: 0.941 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 210, loss: 0.122, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 215, loss: 0.119, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 220, loss: 0.117, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 225, loss: 0.114, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 230, loss: 0.111, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 235, loss: 0.109, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 240, loss: 0.107, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 245, loss: 0.105, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 250, loss: 0.103, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 255, loss: 0.101, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 260, loss: 0.100, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 265, loss: 0.099, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 270, loss: 0.097, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 275, loss: 0.095, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 280, loss: 0.094, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 285, loss: 0.094, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 290, loss: 0.091, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 295, loss: 0.091, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 300, loss: 0.089, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 305, loss: 0.089, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 310, loss: 0.087, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 315, loss: 0.086, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 320, loss: 0.086, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 325, loss: 0.084, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 330, loss: 0.084, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 335, loss: 0.084, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 340, loss: 0.083, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 345, loss: 0.082, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 350, loss: 0.081, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 355, loss: 0.080, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 360, loss: 0.080, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 365, loss: 0.080, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 370, loss: 0.079, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 375, loss: 0.078, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 380, loss: 0.079, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 385, loss: 0.077, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 390, loss: 0.077, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 395, loss: 0.077, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 400, loss: 0.078, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 405, loss: 0.077, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 410, loss: 0.076, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 415, loss: 0.076, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 420, loss: 0.075, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 425, loss: 0.074, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 430, loss: 0.075, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 435, loss: 0.074, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 440, loss: 0.073, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 445, loss: 0.073, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 450, loss: 0.074, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 455, loss: 0.073, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 460, loss: 0.073, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 465, loss: 0.073, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 470, loss: 0.074, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 475, loss: 0.072, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 480, loss: 0.074, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 485, loss: 0.072, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 490, loss: 0.072, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 495, loss: 0.071, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 500, loss: 0.071, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 505, loss: 0.071, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 510, loss: 0.071, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 515, loss: 0.072, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 520, loss: 0.071, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 525, loss: 0.071, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 530, loss: 0.071, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 535, loss: 0.070, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 540, loss: 0.071, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 545, loss: 0.070, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 550, loss: 0.070, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 555, loss: 0.070, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 560, loss: 0.070, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 565, loss: 0.071, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 570, loss: 0.070, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 575, loss: 0.070, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 580, loss: 0.070, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 585, loss: 0.071, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 590, loss: 0.069, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 595, loss: 0.069, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 600, loss: 0.069, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 605, loss: 0.070, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 610, loss: 0.070, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 615, loss: 0.069, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 620, loss: 0.069, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 625, loss: 0.069, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 630, loss: 0.069, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 635, loss: 0.069, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 640, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 645, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 650, loss: 0.069, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 655, loss: 0.070, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 660, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 665, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 670, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 675, loss: 0.069, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 680, loss: 0.069, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 685, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 690, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 695, loss: 0.069, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 700, loss: 0.069, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 705, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 710, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 715, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 720, loss: 0.069, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 725, loss: 0.070, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 730, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 735, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 740, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 745, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 750, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 755, loss: 0.069, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 760, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 765, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 770, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 775, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 780, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 785, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 790, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 795, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 800, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 805, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 810, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 815, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 820, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 825, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 830, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 835, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 840, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 845, loss: 0.069, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 850, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 855, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 860, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 865, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 870, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 875, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 880, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 885, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 890, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 895, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 900, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 905, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 910, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 915, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 920, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 925, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 930, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 935, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 940, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 945, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 950, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 955, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 960, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 965, loss: 0.069, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 970, loss: 0.068, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 975, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 980, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 985, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 990, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n",
            "In epoch 995, loss: 0.067, train acc: 0.953 (train 0.953), test acc: 0.524 (best 0.762)\n"
          ]
        }
      ],
      "source": [
        "los=[]\n",
        "train_accuracy=[]\n",
        "test_accuracy=[]\n",
        "epoch=[]\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "def train(sg_train,sg_test, model):\n",
        "    model.train()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    best_train_acc = 0\n",
        "    best_test_acc = 0\n",
        "\n",
        "    features_train = sg_train.ndata[\"feat\"]\n",
        "    labels = g.ndata[\"label\"]\n",
        "    train_mask = g.ndata[\"train_mask\"]\n",
        "    test_mask = g.ndata[\"test_mask\"]\n",
        "\n",
        "\n",
        "    for e in range(1000):\n",
        "        # Forward\n",
        "        logits = model(sg_train,features_train)\n",
        "\n",
        "        # Compute prediction\n",
        "        pred = logits.argmax(1)\n",
        "\n",
        "        labels = g.ndata[\"label\"].long()\n",
        "        loss = F.cross_entropy(logits, labels[train_mask])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            logits_test = model(sg_test,sg_test.ndata[\"feat\"])\n",
        "\n",
        "\n",
        "        pred_test = logits_test.argmax(1)\n",
        "\n",
        "        train_acc = (pred == labels[train_mask]).float().mean()\n",
        "        test_acc = (pred_test == labels[test_mask]).float().mean()\n",
        "\n",
        "        if best_train_acc < train_acc:\n",
        "            best_train_acc = train_acc\n",
        "        if best_test_acc < test_acc:\n",
        "            best_test_acc = test_acc\n",
        "\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        if e % 5 == 0:\n",
        "            epoch.append(e)\n",
        "            los.append(round(loss.item(),3))\n",
        "            train_accuracy.append(round(train_acc.item(),3))\n",
        "            test_accuracy.append(round(test_acc.item(),3))\n",
        "            print(\n",
        "                f\"In epoch {e}, loss: {loss:.3f}, train acc: {train_acc:.3f} (train {best_train_acc:.3f}), test acc: {test_acc:.3f} (best {best_test_acc:.3f})\"\n",
        "            )\n",
        "\n",
        "\n",
        "model = GCN(g.ndata[\"feat\"].shape[1],20, 2)\n",
        "train(sg_train,sg_test,model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "993f746b",
      "metadata": {
        "id": "993f746b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cc72f14",
      "metadata": {
        "id": "2cc72f14",
        "outputId": "ce9d1211-8385-4b44-a980-b1a621aa74a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0          1          2          3          4          5   \\\n",
              "0    3.522807   8.523250  -7.518755   7.591661  -1.141810  11.101372   \n",
              "1    4.061774  11.361220  -3.023246  10.427218  -6.771303  11.009508   \n",
              "2   -1.115880   6.413536  -3.349251   3.179761   0.502785   2.702257   \n",
              "3    7.730068   5.584105   3.679399  -4.423949  -7.475021  -1.261305   \n",
              "4   -0.854075   7.123057  -1.147151   5.719614  -7.646954   1.962159   \n",
              "..        ...        ...        ...        ...        ...        ...   \n",
              "101 -0.091258  -8.575962  -9.332034  -1.538814 -10.553558  -6.798434   \n",
              "102 -6.388889  -0.147468 -11.594297   9.057175  -7.241624  10.490988   \n",
              "103 -5.019310 -13.708037  -7.912656   1.497004  -5.449959  -3.449053   \n",
              "104 -5.081897  -2.720155  -8.661973  -3.200719   3.946918   1.777469   \n",
              "105 -7.160864  -2.195789  -7.473003   0.872969  -2.832202   1.721226   \n",
              "\n",
              "            6         7         8         9   ...        90        91  \\\n",
              "0    -2.300289 -2.146377 -4.888121  2.805928  ...  2.276341  0.632720   \n",
              "1     5.233799  2.462731 -2.982295  4.310567  ...  0.050742 -0.845159   \n",
              "2     4.086077 -3.541691 -3.309325 -5.095935  ... -0.632356 -1.037802   \n",
              "3    12.376067 -5.327582 -2.625924 -1.863137  ... -0.402212 -0.006221   \n",
              "4     4.223949 -8.641582 -7.440751 -6.681080  ... -0.373809 -0.163059   \n",
              "..         ...       ...       ...       ...  ...       ...       ...   \n",
              "101   1.734323  0.737949  5.079003  8.497883  ...  0.055452  0.151775   \n",
              "102  -1.807599 -3.506027  4.483942  0.413370  ...  1.159766  0.047579   \n",
              "103  -2.551887 -9.310746 -0.635871  2.416802  ... -0.082917  0.049887   \n",
              "104   4.109616 -9.681529  5.919799 -2.167771  ... -0.102379 -0.022728   \n",
              "105  -2.577652 -6.656373  4.990591  6.231125  ...  0.008236 -0.333928   \n",
              "\n",
              "           92        93        94        95        96        97        98  \\\n",
              "0    0.347799  0.351225  0.380082  0.524647 -0.498945 -0.128223 -0.602982   \n",
              "1    0.569740 -0.105235 -0.337340 -1.213004  0.205053 -3.114140  0.433683   \n",
              "2   -0.828816  0.845760 -0.157617 -0.664811 -0.520409 -0.920657  0.777924   \n",
              "3   -0.230888 -0.985756 -0.614862 -0.123730  0.201314 -0.318117 -0.166227   \n",
              "4   -1.194363 -0.871892  0.422480  0.685808  0.729428 -0.376138  0.312182   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "101  0.009296 -0.791642 -0.552973 -0.313605 -0.456627 -0.379010  1.043307   \n",
              "102 -1.155544 -0.771512  0.690283  0.690738 -0.005875  0.219385 -0.597498   \n",
              "103 -1.095549  0.004212  0.271263 -0.982106  0.215102 -0.925024 -0.164672   \n",
              "104  1.158159  0.304362  0.034789  0.387032  0.004200  0.834406  0.207939   \n",
              "105  1.172649  0.485873 -0.012833 -0.347462  0.510314 -0.282998  0.333233   \n",
              "\n",
              "           99  \n",
              "0   -0.237311  \n",
              "1    1.448499  \n",
              "2   -0.995441  \n",
              "3   -0.748881  \n",
              "4    0.019183  \n",
              "..        ...  \n",
              "101 -0.394521  \n",
              "102 -0.232772  \n",
              "103  0.121966  \n",
              "104 -0.550762  \n",
              "105 -0.529369  \n",
              "\n",
              "[106 rows x 100 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c293383e-5cff-43b6-9f1b-2476865591cc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.522807</td>\n",
              "      <td>8.523250</td>\n",
              "      <td>-7.518755</td>\n",
              "      <td>7.591661</td>\n",
              "      <td>-1.141810</td>\n",
              "      <td>11.101372</td>\n",
              "      <td>-2.300289</td>\n",
              "      <td>-2.146377</td>\n",
              "      <td>-4.888121</td>\n",
              "      <td>2.805928</td>\n",
              "      <td>...</td>\n",
              "      <td>2.276341</td>\n",
              "      <td>0.632720</td>\n",
              "      <td>0.347799</td>\n",
              "      <td>0.351225</td>\n",
              "      <td>0.380082</td>\n",
              "      <td>0.524647</td>\n",
              "      <td>-0.498945</td>\n",
              "      <td>-0.128223</td>\n",
              "      <td>-0.602982</td>\n",
              "      <td>-0.237311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.061774</td>\n",
              "      <td>11.361220</td>\n",
              "      <td>-3.023246</td>\n",
              "      <td>10.427218</td>\n",
              "      <td>-6.771303</td>\n",
              "      <td>11.009508</td>\n",
              "      <td>5.233799</td>\n",
              "      <td>2.462731</td>\n",
              "      <td>-2.982295</td>\n",
              "      <td>4.310567</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050742</td>\n",
              "      <td>-0.845159</td>\n",
              "      <td>0.569740</td>\n",
              "      <td>-0.105235</td>\n",
              "      <td>-0.337340</td>\n",
              "      <td>-1.213004</td>\n",
              "      <td>0.205053</td>\n",
              "      <td>-3.114140</td>\n",
              "      <td>0.433683</td>\n",
              "      <td>1.448499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.115880</td>\n",
              "      <td>6.413536</td>\n",
              "      <td>-3.349251</td>\n",
              "      <td>3.179761</td>\n",
              "      <td>0.502785</td>\n",
              "      <td>2.702257</td>\n",
              "      <td>4.086077</td>\n",
              "      <td>-3.541691</td>\n",
              "      <td>-3.309325</td>\n",
              "      <td>-5.095935</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.632356</td>\n",
              "      <td>-1.037802</td>\n",
              "      <td>-0.828816</td>\n",
              "      <td>0.845760</td>\n",
              "      <td>-0.157617</td>\n",
              "      <td>-0.664811</td>\n",
              "      <td>-0.520409</td>\n",
              "      <td>-0.920657</td>\n",
              "      <td>0.777924</td>\n",
              "      <td>-0.995441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.730068</td>\n",
              "      <td>5.584105</td>\n",
              "      <td>3.679399</td>\n",
              "      <td>-4.423949</td>\n",
              "      <td>-7.475021</td>\n",
              "      <td>-1.261305</td>\n",
              "      <td>12.376067</td>\n",
              "      <td>-5.327582</td>\n",
              "      <td>-2.625924</td>\n",
              "      <td>-1.863137</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.402212</td>\n",
              "      <td>-0.006221</td>\n",
              "      <td>-0.230888</td>\n",
              "      <td>-0.985756</td>\n",
              "      <td>-0.614862</td>\n",
              "      <td>-0.123730</td>\n",
              "      <td>0.201314</td>\n",
              "      <td>-0.318117</td>\n",
              "      <td>-0.166227</td>\n",
              "      <td>-0.748881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.854075</td>\n",
              "      <td>7.123057</td>\n",
              "      <td>-1.147151</td>\n",
              "      <td>5.719614</td>\n",
              "      <td>-7.646954</td>\n",
              "      <td>1.962159</td>\n",
              "      <td>4.223949</td>\n",
              "      <td>-8.641582</td>\n",
              "      <td>-7.440751</td>\n",
              "      <td>-6.681080</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.373809</td>\n",
              "      <td>-0.163059</td>\n",
              "      <td>-1.194363</td>\n",
              "      <td>-0.871892</td>\n",
              "      <td>0.422480</td>\n",
              "      <td>0.685808</td>\n",
              "      <td>0.729428</td>\n",
              "      <td>-0.376138</td>\n",
              "      <td>0.312182</td>\n",
              "      <td>0.019183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>-0.091258</td>\n",
              "      <td>-8.575962</td>\n",
              "      <td>-9.332034</td>\n",
              "      <td>-1.538814</td>\n",
              "      <td>-10.553558</td>\n",
              "      <td>-6.798434</td>\n",
              "      <td>1.734323</td>\n",
              "      <td>0.737949</td>\n",
              "      <td>5.079003</td>\n",
              "      <td>8.497883</td>\n",
              "      <td>...</td>\n",
              "      <td>0.055452</td>\n",
              "      <td>0.151775</td>\n",
              "      <td>0.009296</td>\n",
              "      <td>-0.791642</td>\n",
              "      <td>-0.552973</td>\n",
              "      <td>-0.313605</td>\n",
              "      <td>-0.456627</td>\n",
              "      <td>-0.379010</td>\n",
              "      <td>1.043307</td>\n",
              "      <td>-0.394521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>-6.388889</td>\n",
              "      <td>-0.147468</td>\n",
              "      <td>-11.594297</td>\n",
              "      <td>9.057175</td>\n",
              "      <td>-7.241624</td>\n",
              "      <td>10.490988</td>\n",
              "      <td>-1.807599</td>\n",
              "      <td>-3.506027</td>\n",
              "      <td>4.483942</td>\n",
              "      <td>0.413370</td>\n",
              "      <td>...</td>\n",
              "      <td>1.159766</td>\n",
              "      <td>0.047579</td>\n",
              "      <td>-1.155544</td>\n",
              "      <td>-0.771512</td>\n",
              "      <td>0.690283</td>\n",
              "      <td>0.690738</td>\n",
              "      <td>-0.005875</td>\n",
              "      <td>0.219385</td>\n",
              "      <td>-0.597498</td>\n",
              "      <td>-0.232772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>-5.019310</td>\n",
              "      <td>-13.708037</td>\n",
              "      <td>-7.912656</td>\n",
              "      <td>1.497004</td>\n",
              "      <td>-5.449959</td>\n",
              "      <td>-3.449053</td>\n",
              "      <td>-2.551887</td>\n",
              "      <td>-9.310746</td>\n",
              "      <td>-0.635871</td>\n",
              "      <td>2.416802</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.082917</td>\n",
              "      <td>0.049887</td>\n",
              "      <td>-1.095549</td>\n",
              "      <td>0.004212</td>\n",
              "      <td>0.271263</td>\n",
              "      <td>-0.982106</td>\n",
              "      <td>0.215102</td>\n",
              "      <td>-0.925024</td>\n",
              "      <td>-0.164672</td>\n",
              "      <td>0.121966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>-5.081897</td>\n",
              "      <td>-2.720155</td>\n",
              "      <td>-8.661973</td>\n",
              "      <td>-3.200719</td>\n",
              "      <td>3.946918</td>\n",
              "      <td>1.777469</td>\n",
              "      <td>4.109616</td>\n",
              "      <td>-9.681529</td>\n",
              "      <td>5.919799</td>\n",
              "      <td>-2.167771</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.102379</td>\n",
              "      <td>-0.022728</td>\n",
              "      <td>1.158159</td>\n",
              "      <td>0.304362</td>\n",
              "      <td>0.034789</td>\n",
              "      <td>0.387032</td>\n",
              "      <td>0.004200</td>\n",
              "      <td>0.834406</td>\n",
              "      <td>0.207939</td>\n",
              "      <td>-0.550762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>-7.160864</td>\n",
              "      <td>-2.195789</td>\n",
              "      <td>-7.473003</td>\n",
              "      <td>0.872969</td>\n",
              "      <td>-2.832202</td>\n",
              "      <td>1.721226</td>\n",
              "      <td>-2.577652</td>\n",
              "      <td>-6.656373</td>\n",
              "      <td>4.990591</td>\n",
              "      <td>6.231125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008236</td>\n",
              "      <td>-0.333928</td>\n",
              "      <td>1.172649</td>\n",
              "      <td>0.485873</td>\n",
              "      <td>-0.012833</td>\n",
              "      <td>-0.347462</td>\n",
              "      <td>0.510314</td>\n",
              "      <td>-0.282998</td>\n",
              "      <td>0.333233</td>\n",
              "      <td>-0.529369</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>106 rows × 100 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c293383e-5cff-43b6-9f1b-2476865591cc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c293383e-5cff-43b6-9f1b-2476865591cc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c293383e-5cff-43b6-9f1b-2476865591cc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6e6598cb-83eb-4cd9-8a89-c7cdc369beea\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6e6598cb-83eb-4cd9-8a89-c7cdc369beea')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6e6598cb-83eb-4cd9-8a89-c7cdc369beea button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_ba23c3cf-07ba-4902-90d6-5c04c26049ea\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('X_train_df_copy')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ba23c3cf-07ba-4902-90d6-5c04c26049ea button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('X_train_df_copy');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train_df_copy"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "X_train_df_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "443b582d",
      "metadata": {
        "id": "443b582d",
        "outputId": "f50d24d4-7469-4e99-ca71-0d8531280e4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pandas.core.series.Series</b><br/>def __init__(data=None, index=None, dtype: Dtype | None=None, name=None, copy: bool | None=None, fastpath: bool=False) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pandas/core/series.py</a>One-dimensional ndarray with axis labels (including time series).\n",
              "\n",
              "Labels need not be unique but must be a hashable type. The object\n",
              "supports both integer- and label-based indexing and provides a host of\n",
              "methods for performing operations involving the index. Statistical\n",
              "methods from ndarray have been overridden to automatically exclude\n",
              "missing data (currently represented as NaN).\n",
              "\n",
              "Operations between Series (+, -, /, \\*, \\*\\*) align values based on their\n",
              "associated index values-- they need not be the same length. The result\n",
              "index will be the sorted union of the two indexes.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "data : array-like, Iterable, dict, or scalar value\n",
              "    Contains data stored in Series. If data is a dict, argument order is\n",
              "    maintained.\n",
              "index : array-like or Index (1d)\n",
              "    Values must be hashable and have the same length as `data`.\n",
              "    Non-unique index values are allowed. Will default to\n",
              "    RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n",
              "    and index is None, then the keys in the data are used as the index. If the\n",
              "    index is not None, the resulting Series is reindexed with the index values.\n",
              "dtype : str, numpy.dtype, or ExtensionDtype, optional\n",
              "    Data type for the output Series. If not specified, this will be\n",
              "    inferred from `data`.\n",
              "    See the :ref:`user guide &lt;basics.dtypes&gt;` for more usages.\n",
              "name : Hashable, default None\n",
              "    The name to give to the Series.\n",
              "copy : bool, default False\n",
              "    Copy input data. Only affects Series or 1d ndarray input. See examples.\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Please reference the :ref:`User Guide &lt;basics.series&gt;` for more information.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "Constructing Series from a dictionary with an Index specified\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3}\n",
              "&gt;&gt;&gt; ser = pd.Series(data=d, index=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;])\n",
              "&gt;&gt;&gt; ser\n",
              "a   1\n",
              "b   2\n",
              "c   3\n",
              "dtype: int64\n",
              "\n",
              "The keys of the dictionary match with the Index values, hence the Index\n",
              "values have no effect.\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3}\n",
              "&gt;&gt;&gt; ser = pd.Series(data=d, index=[&#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;])\n",
              "&gt;&gt;&gt; ser\n",
              "x   NaN\n",
              "y   NaN\n",
              "z   NaN\n",
              "dtype: float64\n",
              "\n",
              "Note that the Index is first build with the keys from the dictionary.\n",
              "After this the Series is reindexed with the given Index values, hence we\n",
              "get all NaN as a result.\n",
              "\n",
              "Constructing Series from a list with `copy=False`.\n",
              "\n",
              "&gt;&gt;&gt; r = [1, 2]\n",
              "&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n",
              "&gt;&gt;&gt; ser.iloc[0] = 999\n",
              "&gt;&gt;&gt; r\n",
              "[1, 2]\n",
              "&gt;&gt;&gt; ser\n",
              "0    999\n",
              "1      2\n",
              "dtype: int64\n",
              "\n",
              "Due to input data type the Series has a `copy` of\n",
              "the original data even though `copy=False`, so\n",
              "the data is unchanged.\n",
              "\n",
              "Constructing Series from a 1d ndarray with `copy=False`.\n",
              "\n",
              "&gt;&gt;&gt; r = np.array([1, 2])\n",
              "&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n",
              "&gt;&gt;&gt; ser.iloc[0] = 999\n",
              "&gt;&gt;&gt; r\n",
              "array([999,   2])\n",
              "&gt;&gt;&gt; ser\n",
              "0    999\n",
              "1      2\n",
              "dtype: int64\n",
              "\n",
              "Due to input data type the Series has a `view` on\n",
              "the original data, so\n",
              "the data is changed as well.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 244);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "labels = pd.read_csv(\"TreeLabels.csv\")\n",
        "labels['majority_vote'] = labels.mode(axis=1, numeric_only=True).astype(int)\n",
        "type(labels['majority_vote'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d5eb9c2",
      "metadata": {
        "id": "2d5eb9c2"
      },
      "outputs": [],
      "source": [
        "X_train_df_copy['label']=labels['majority_vote']\n",
        "y=X_train_df_copy['label']\n",
        "X=X_train_df_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "686eb97f",
      "metadata": {
        "id": "686eb97f",
        "outputId": "502eb8f4-bf6b-43db-87ba-b8fca1461e9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0          1          2          3          4          5   \\\n",
              "0    3.522807   8.523250  -7.518755   7.591661  -1.141810  11.101372   \n",
              "1    4.061774  11.361220  -3.023246  10.427218  -6.771303  11.009508   \n",
              "2   -1.115880   6.413536  -3.349251   3.179761   0.502785   2.702257   \n",
              "3    7.730068   5.584105   3.679399  -4.423949  -7.475021  -1.261305   \n",
              "4   -0.854075   7.123057  -1.147151   5.719614  -7.646954   1.962159   \n",
              "..        ...        ...        ...        ...        ...        ...   \n",
              "101 -0.091258  -8.575962  -9.332034  -1.538814 -10.553558  -6.798434   \n",
              "102 -6.388889  -0.147468 -11.594297   9.057175  -7.241624  10.490988   \n",
              "103 -5.019310 -13.708037  -7.912656   1.497004  -5.449959  -3.449053   \n",
              "104 -5.081897  -2.720155  -8.661973  -3.200719   3.946918   1.777469   \n",
              "105 -7.160864  -2.195789  -7.473003   0.872969  -2.832202   1.721226   \n",
              "\n",
              "            6         7         8         9   ...        90        91  \\\n",
              "0    -2.300289 -2.146377 -4.888121  2.805928  ...  2.276341  0.632720   \n",
              "1     5.233799  2.462731 -2.982295  4.310567  ...  0.050742 -0.845159   \n",
              "2     4.086077 -3.541691 -3.309325 -5.095935  ... -0.632356 -1.037802   \n",
              "3    12.376067 -5.327582 -2.625924 -1.863137  ... -0.402212 -0.006221   \n",
              "4     4.223949 -8.641582 -7.440751 -6.681080  ... -0.373809 -0.163059   \n",
              "..         ...       ...       ...       ...  ...       ...       ...   \n",
              "101   1.734323  0.737949  5.079003  8.497883  ...  0.055452  0.151775   \n",
              "102  -1.807599 -3.506027  4.483942  0.413370  ...  1.159766  0.047579   \n",
              "103  -2.551887 -9.310746 -0.635871  2.416802  ... -0.082917  0.049887   \n",
              "104   4.109616 -9.681529  5.919799 -2.167771  ... -0.102379 -0.022728   \n",
              "105  -2.577652 -6.656373  4.990591  6.231125  ...  0.008236 -0.333928   \n",
              "\n",
              "           92        93        94        95        96        97        98  \\\n",
              "0    0.347799  0.351225  0.380082  0.524647 -0.498945 -0.128223 -0.602982   \n",
              "1    0.569740 -0.105235 -0.337340 -1.213004  0.205053 -3.114140  0.433683   \n",
              "2   -0.828816  0.845760 -0.157617 -0.664811 -0.520409 -0.920657  0.777924   \n",
              "3   -0.230888 -0.985756 -0.614862 -0.123730  0.201314 -0.318117 -0.166227   \n",
              "4   -1.194363 -0.871892  0.422480  0.685808  0.729428 -0.376138  0.312182   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "101  0.009296 -0.791642 -0.552973 -0.313605 -0.456627 -0.379010  1.043307   \n",
              "102 -1.155544 -0.771512  0.690283  0.690738 -0.005875  0.219385 -0.597498   \n",
              "103 -1.095549  0.004212  0.271263 -0.982106  0.215102 -0.925024 -0.164672   \n",
              "104  1.158159  0.304362  0.034789  0.387032  0.004200  0.834406  0.207939   \n",
              "105  1.172649  0.485873 -0.012833 -0.347462  0.510314 -0.282998  0.333233   \n",
              "\n",
              "           99  \n",
              "0   -0.237311  \n",
              "1    1.448499  \n",
              "2   -0.995441  \n",
              "3   -0.748881  \n",
              "4    0.019183  \n",
              "..        ...  \n",
              "101 -0.394521  \n",
              "102 -0.232772  \n",
              "103  0.121966  \n",
              "104 -0.550762  \n",
              "105 -0.529369  \n",
              "\n",
              "[106 rows x 100 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f37bc90-6bc4-4ba3-bf6a-4090fccc372e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.522807</td>\n",
              "      <td>8.523250</td>\n",
              "      <td>-7.518755</td>\n",
              "      <td>7.591661</td>\n",
              "      <td>-1.141810</td>\n",
              "      <td>11.101372</td>\n",
              "      <td>-2.300289</td>\n",
              "      <td>-2.146377</td>\n",
              "      <td>-4.888121</td>\n",
              "      <td>2.805928</td>\n",
              "      <td>...</td>\n",
              "      <td>2.276341</td>\n",
              "      <td>0.632720</td>\n",
              "      <td>0.347799</td>\n",
              "      <td>0.351225</td>\n",
              "      <td>0.380082</td>\n",
              "      <td>0.524647</td>\n",
              "      <td>-0.498945</td>\n",
              "      <td>-0.128223</td>\n",
              "      <td>-0.602982</td>\n",
              "      <td>-0.237311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.061774</td>\n",
              "      <td>11.361220</td>\n",
              "      <td>-3.023246</td>\n",
              "      <td>10.427218</td>\n",
              "      <td>-6.771303</td>\n",
              "      <td>11.009508</td>\n",
              "      <td>5.233799</td>\n",
              "      <td>2.462731</td>\n",
              "      <td>-2.982295</td>\n",
              "      <td>4.310567</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050742</td>\n",
              "      <td>-0.845159</td>\n",
              "      <td>0.569740</td>\n",
              "      <td>-0.105235</td>\n",
              "      <td>-0.337340</td>\n",
              "      <td>-1.213004</td>\n",
              "      <td>0.205053</td>\n",
              "      <td>-3.114140</td>\n",
              "      <td>0.433683</td>\n",
              "      <td>1.448499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.115880</td>\n",
              "      <td>6.413536</td>\n",
              "      <td>-3.349251</td>\n",
              "      <td>3.179761</td>\n",
              "      <td>0.502785</td>\n",
              "      <td>2.702257</td>\n",
              "      <td>4.086077</td>\n",
              "      <td>-3.541691</td>\n",
              "      <td>-3.309325</td>\n",
              "      <td>-5.095935</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.632356</td>\n",
              "      <td>-1.037802</td>\n",
              "      <td>-0.828816</td>\n",
              "      <td>0.845760</td>\n",
              "      <td>-0.157617</td>\n",
              "      <td>-0.664811</td>\n",
              "      <td>-0.520409</td>\n",
              "      <td>-0.920657</td>\n",
              "      <td>0.777924</td>\n",
              "      <td>-0.995441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.730068</td>\n",
              "      <td>5.584105</td>\n",
              "      <td>3.679399</td>\n",
              "      <td>-4.423949</td>\n",
              "      <td>-7.475021</td>\n",
              "      <td>-1.261305</td>\n",
              "      <td>12.376067</td>\n",
              "      <td>-5.327582</td>\n",
              "      <td>-2.625924</td>\n",
              "      <td>-1.863137</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.402212</td>\n",
              "      <td>-0.006221</td>\n",
              "      <td>-0.230888</td>\n",
              "      <td>-0.985756</td>\n",
              "      <td>-0.614862</td>\n",
              "      <td>-0.123730</td>\n",
              "      <td>0.201314</td>\n",
              "      <td>-0.318117</td>\n",
              "      <td>-0.166227</td>\n",
              "      <td>-0.748881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.854075</td>\n",
              "      <td>7.123057</td>\n",
              "      <td>-1.147151</td>\n",
              "      <td>5.719614</td>\n",
              "      <td>-7.646954</td>\n",
              "      <td>1.962159</td>\n",
              "      <td>4.223949</td>\n",
              "      <td>-8.641582</td>\n",
              "      <td>-7.440751</td>\n",
              "      <td>-6.681080</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.373809</td>\n",
              "      <td>-0.163059</td>\n",
              "      <td>-1.194363</td>\n",
              "      <td>-0.871892</td>\n",
              "      <td>0.422480</td>\n",
              "      <td>0.685808</td>\n",
              "      <td>0.729428</td>\n",
              "      <td>-0.376138</td>\n",
              "      <td>0.312182</td>\n",
              "      <td>0.019183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>-0.091258</td>\n",
              "      <td>-8.575962</td>\n",
              "      <td>-9.332034</td>\n",
              "      <td>-1.538814</td>\n",
              "      <td>-10.553558</td>\n",
              "      <td>-6.798434</td>\n",
              "      <td>1.734323</td>\n",
              "      <td>0.737949</td>\n",
              "      <td>5.079003</td>\n",
              "      <td>8.497883</td>\n",
              "      <td>...</td>\n",
              "      <td>0.055452</td>\n",
              "      <td>0.151775</td>\n",
              "      <td>0.009296</td>\n",
              "      <td>-0.791642</td>\n",
              "      <td>-0.552973</td>\n",
              "      <td>-0.313605</td>\n",
              "      <td>-0.456627</td>\n",
              "      <td>-0.379010</td>\n",
              "      <td>1.043307</td>\n",
              "      <td>-0.394521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>-6.388889</td>\n",
              "      <td>-0.147468</td>\n",
              "      <td>-11.594297</td>\n",
              "      <td>9.057175</td>\n",
              "      <td>-7.241624</td>\n",
              "      <td>10.490988</td>\n",
              "      <td>-1.807599</td>\n",
              "      <td>-3.506027</td>\n",
              "      <td>4.483942</td>\n",
              "      <td>0.413370</td>\n",
              "      <td>...</td>\n",
              "      <td>1.159766</td>\n",
              "      <td>0.047579</td>\n",
              "      <td>-1.155544</td>\n",
              "      <td>-0.771512</td>\n",
              "      <td>0.690283</td>\n",
              "      <td>0.690738</td>\n",
              "      <td>-0.005875</td>\n",
              "      <td>0.219385</td>\n",
              "      <td>-0.597498</td>\n",
              "      <td>-0.232772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>-5.019310</td>\n",
              "      <td>-13.708037</td>\n",
              "      <td>-7.912656</td>\n",
              "      <td>1.497004</td>\n",
              "      <td>-5.449959</td>\n",
              "      <td>-3.449053</td>\n",
              "      <td>-2.551887</td>\n",
              "      <td>-9.310746</td>\n",
              "      <td>-0.635871</td>\n",
              "      <td>2.416802</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.082917</td>\n",
              "      <td>0.049887</td>\n",
              "      <td>-1.095549</td>\n",
              "      <td>0.004212</td>\n",
              "      <td>0.271263</td>\n",
              "      <td>-0.982106</td>\n",
              "      <td>0.215102</td>\n",
              "      <td>-0.925024</td>\n",
              "      <td>-0.164672</td>\n",
              "      <td>0.121966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>-5.081897</td>\n",
              "      <td>-2.720155</td>\n",
              "      <td>-8.661973</td>\n",
              "      <td>-3.200719</td>\n",
              "      <td>3.946918</td>\n",
              "      <td>1.777469</td>\n",
              "      <td>4.109616</td>\n",
              "      <td>-9.681529</td>\n",
              "      <td>5.919799</td>\n",
              "      <td>-2.167771</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.102379</td>\n",
              "      <td>-0.022728</td>\n",
              "      <td>1.158159</td>\n",
              "      <td>0.304362</td>\n",
              "      <td>0.034789</td>\n",
              "      <td>0.387032</td>\n",
              "      <td>0.004200</td>\n",
              "      <td>0.834406</td>\n",
              "      <td>0.207939</td>\n",
              "      <td>-0.550762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>-7.160864</td>\n",
              "      <td>-2.195789</td>\n",
              "      <td>-7.473003</td>\n",
              "      <td>0.872969</td>\n",
              "      <td>-2.832202</td>\n",
              "      <td>1.721226</td>\n",
              "      <td>-2.577652</td>\n",
              "      <td>-6.656373</td>\n",
              "      <td>4.990591</td>\n",
              "      <td>6.231125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008236</td>\n",
              "      <td>-0.333928</td>\n",
              "      <td>1.172649</td>\n",
              "      <td>0.485873</td>\n",
              "      <td>-0.012833</td>\n",
              "      <td>-0.347462</td>\n",
              "      <td>0.510314</td>\n",
              "      <td>-0.282998</td>\n",
              "      <td>0.333233</td>\n",
              "      <td>-0.529369</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>106 rows × 100 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f37bc90-6bc4-4ba3-bf6a-4090fccc372e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0f37bc90-6bc4-4ba3-bf6a-4090fccc372e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0f37bc90-6bc4-4ba3-bf6a-4090fccc372e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-416d152d-22a6-4a2d-b8fc-f84f795f50ea\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-416d152d-22a6-4a2d-b8fc-f84f795f50ea')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-416d152d-22a6-4a2d-b8fc-f84f795f50ea button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_15df94a6-9f1f-4cbf-8de2-4aa55365085a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('X_train_df_copy')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_15df94a6-9f1f-4cbf-8de2-4aa55365085a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('X_train_df_copy');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train_df_copy"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "X.drop(columns=['label'],axis=1,inplace=True)\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f14fe96d-da32-46e4-af9a-adb84db689a7",
      "metadata": {
        "id": "f14fe96d-da32-46e4-af9a-adb84db689a7"
      },
      "outputs": [],
      "source": [
        "indices_to_change = [4, 105, 84, 27, 98, 88, 18, 65, 9, 2, 5, 49, 99, 69, 86, 67, 7, 28, 78, 70, 18, 74]\n",
        "\n",
        "# Create a boolean mask to identify indices for the test set\n",
        "test_mask = np.zeros(len(X), dtype=bool)\n",
        "test_mask[indices_to_change] = True\n",
        "\n",
        "# Split the dataset based on the boolean mask\n",
        "X_train = X[~test_mask]\n",
        "X_test = X[test_mask]\n",
        "y_train = y[~test_mask]\n",
        "y_test = y[test_mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "232245f6",
      "metadata": {
        "id": "232245f6",
        "outputId": "771209c0-820a-4728-cf81-d3f3df83064a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "12 fits failed out of a total of 72.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "12 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1216, in fit\n",
            "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
            "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
            "    raise ValueError(\n",
            "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.3998358  0.72906404 0.44581281 0.60960591        nan 0.43390805\n",
            " 0.46962233 0.47003284 0.43390805 0.43390805        nan 0.43390805\n",
            " 0.46962233 0.47003284 0.43390805 0.43390805        nan 0.43390805\n",
            " 0.46962233 0.47003284 0.43390805 0.43390805        nan 0.43390805]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'C': 0.1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "Best Score: 0.729064039408867\n"
          ]
        }
      ],
      "source": [
        "param_grid = {'penalty':['l1','l2',None],'C': [0.1,2,2.5,3],\n",
        "                   'solver':['liblinear','saga'],'max_iter':[500]}\n",
        "LR=LogisticRegression(random_state=0)\n",
        "grid_search_LR = GridSearchCV(estimator=LR,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=3)\n",
        "\n",
        "grid_search_LR.fit(X_train, y_train)\n",
        "print(\"Best parameters found: \", grid_search_LR.best_params_)\n",
        "print(\"Best Score: {}\".format(grid_search_LR.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cc09049",
      "metadata": {
        "id": "3cc09049",
        "outputId": "1cbaca75-4fc9-4e04-dd16-05e382454cc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'C': 0.1, 'l1_ratio': 0.5, 'max_iter': 500, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
            "Best Score: 0.6943521594684385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "param_grid = {'penalty':['elasticnet',None],'C': [0.1,2,2.5,3],\n",
        "                   'solver':['saga'],'max_iter':[500],'l1_ratio':[0.5,0.2,0.7]}\n",
        "LR=LogisticRegression(random_state=0)\n",
        "grid_search_LR = GridSearchCV(estimator=LR,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=2)\n",
        "grid_search_LR.fit(X_train, y_train)\n",
        "print(\"Best parameters found: \", grid_search_LR.best_params_)\n",
        "print(\"Best Score: {}\".format(grid_search_LR.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d4dfadd",
      "metadata": {
        "id": "7d4dfadd",
        "outputId": "692ba6ec-c088-434f-964f-42bca76a0745",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/optimize/_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
            "  warnings.warn(\"Line Search failed\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'C': 2, 'max_iter': 500, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Best Score: 0.7532840722495896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "param_grid = {'penalty':['l2',None],'C': [0.1,2,2.5,3],\n",
        "                   'solver':['lbfgs','newton-cg','sag'],'max_iter':[500]}\n",
        "LR=LogisticRegression(random_state=0)\n",
        "grid_search_LR = GridSearchCV(estimator=LR,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=3\n",
        "                          )\n",
        "\n",
        "grid_search_LR.fit(X_train, y_train)\n",
        "print(\"Best parameters found: \", grid_search_LR.best_params_)\n",
        "print(\"Best Score: {}\".format(grid_search_LR.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36fa37d7",
      "metadata": {
        "id": "36fa37d7",
        "outputId": "f84cb859-ce4a-44bf-f41b-2a070920783c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=2, max_iter=1000)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=2, max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=2, max_iter=1000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "LR=LogisticRegression(C=2,penalty='l2',solver='lbfgs',max_iter=1000)\n",
        "LR.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17158811",
      "metadata": {
        "id": "17158811",
        "outputId": "27e4839a-417c-478b-8e84-3dc0da6b2048",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_accuracy 1.0\n",
            "test_accuracy 0.8095238095238095\n"
          ]
        }
      ],
      "source": [
        "LR_pred = LR.predict(X_train)\n",
        "acc_train=accuracy_score(y_train,LR_pred)\n",
        "print('train_accuracy',acc_train)\n",
        "LR_pred = LR.predict(X_test)\n",
        "acc=accuracy_score(y_test,LR_pred)\n",
        "print('test_accuracy',acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7b8a2f2-3db9-4cd9-9af5-06720d76eccf",
      "metadata": {
        "id": "b7b8a2f2-3db9-4cd9-9af5-06720d76eccf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ba961e4-336d-41c3-8509-7e816bf5f69e",
      "metadata": {
        "id": "9ba961e4-336d-41c3-8509-7e816bf5f69e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e2df025-4cd8-4584-b163-3e1722f1ef04",
      "metadata": {
        "id": "4e2df025-4cd8-4584-b163-3e1722f1ef04"
      },
      "outputs": [],
      "source": [
        "X_train_values = X_train.values\n",
        "X_test_values = X_test.values\n",
        "#X_valid_values = X_valid.values\n",
        "\n",
        "# Reshape input data to 3D [samples, timesteps, features] so it'll fit the LSTM layer\n",
        "# Use a timestep of 1.\n",
        "X_train_reshaped = np.reshape(X_train_values, (X_train_values.shape[0], 1, X_train_values.shape[1]))\n",
        "X_test_reshaped = np.reshape(X_test_values, (X_test_values.shape[0], 1, X_test_values.shape[1]))\n",
        "#X_valid_reshaped = np.reshape(X_valid_values, (X_valid_values.shape[0], 1, X_valid_values.shape[1]))\n",
        "\n",
        "# LSTM model\n",
        "lstm = Sequential()\n",
        "lstm.add(LSTM(100,return_sequences=True,activation='selu',input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
        "lstm.add(Dropout(0.7))\n",
        "\n",
        "lstm.add(LSTM(50,activation='selu',return_sequences=False,kernel_regularizer=l2(0.01)))\n",
        "lstm.add(Dropout(0.7))\n",
        "lstm.add(Dense(1,activation='sigmoid'))  # Prediction of the next closing value\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "lstm.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "lstm.fit(X_train_reshaped, y_train, epochs=1000, batch_size=32, verbose=1)\n",
        "\n",
        "# Predicting and inverse transforming the predictions\n",
        "y_pred_train_lstm = lstm.predict(X_train_reshaped)\n",
        "y_pred_test_lstm = lstm.predict(X_test_reshaped)\n",
        "\n",
        "#y_pred = scaler.inverse_transform(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37f7966e-0c00-4050-9c4b-c955c14561b6",
      "metadata": {
        "id": "37f7966e-0c00-4050-9c4b-c955c14561b6",
        "outputId": "74aee31b-44c8-4e75-87e4-ca368992758b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train accuracy 1.0\n",
            "test accuracy 0.7619047619047619\n"
          ]
        }
      ],
      "source": [
        "y_pred_train_lstm = np.squeeze(y_pred_train_lstm)\n",
        "y_pred_test_lstm = np.squeeze(y_pred_test_lstm)\n",
        "y_pred_train_lstm = (y_pred_train_lstm > 0.5).astype(int)\n",
        "acc_train=accuracy_score(y_train,y_pred_train_lstm)\n",
        "print('train accuracy',acc_train)\n",
        "y_pred_test_lstm = (y_pred_test_lstm > 0.5).astype(int)\n",
        "acc_test=accuracy_score(y_test,y_pred_test_lstm)\n",
        "print('test accuracy',acc_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59673cad-837a-484c-9ccf-a7fda0957464",
      "metadata": {
        "id": "59673cad-837a-484c-9ccf-a7fda0957464"
      },
      "outputs": [],
      "source": [
        "bilstm = Sequential()\n",
        "bilstm.add(Bidirectional(LSTM(100, return_sequences=True,kernel_regularizer=l2(0.01)), input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
        "#bilstm.add(Dropout(0.7))\n",
        "bilstm.add(Bidirectional(LSTM(50, return_sequences=False,kernel_regularizer=l2(0.01))))\n",
        "#bilstm.add(Dropout(0.7))\n",
        "bilstm.add(Dense(1, activation='sigmoid',kernel_regularizer=l2(0.01)))  # Binary classification output\n",
        "\n",
        "# Compile the model\n",
        "bilstm.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "bilstm.fit(X_train_reshaped, y_train, epochs=500, batch_size=32, verbose=1)\n",
        "\n",
        "# Predicting\n",
        "y_pred_train_bilstm = bilstm.predict(X_train_reshaped)\n",
        "y_pred_test_bilstm = bilstm.predict(X_test_reshaped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76247ddc",
      "metadata": {
        "id": "76247ddc"
      },
      "outputs": [],
      "source": [
        "y_pred_train_bilstm = np.squeeze(y_pred_train_bilstm)\n",
        "y_pred_test_bilstm = np.squeeze(y_pred_test_bilstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd68f6a0",
      "metadata": {
        "id": "fd68f6a0",
        "outputId": "442457e0-1fc1-412d-8262-71c0fd89abe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train accuracy 1.0\n",
            "test accuracy 0.8095238095238095\n"
          ]
        }
      ],
      "source": [
        "y_pred_train_bilstm = (y_pred_train_bilstm > 0.5).astype(int)\n",
        "acc_train=accuracy_score(y_train,y_pred_train_bilstm)\n",
        "print('train accuracy',acc_train)\n",
        "y_pred_test_bilstm = (y_pred_test_bilstm > 0.5).astype(int)\n",
        "acc_test=accuracy_score(y_test,y_pred_test_bilstm)\n",
        "print('test accuracy',acc_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e26bd467-e763-40f3-a335-b5a7602e379e",
      "metadata": {
        "id": "e26bd467-e763-40f3-a335-b5a7602e379e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}