{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.4.1\n",
        "!pip install dgl==0.9.0\n",
        "!pip install torch_geometric\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nSL7s_iV28E",
        "outputId": "f16a107d-2eeb-4cec-d359-da3136f35429"
      },
      "id": "8nSL7s_iV28E",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n",
            "  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.1)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.1)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.1)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.1)\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1) (12.6.77)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.1) (1.3.0)\n",
            "Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.5.0.50\n",
            "    Uninstalling nvidia-cudnn-cu12-9.5.0.50:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.5.0.50\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0+cu121\n",
            "    Uninstalling torch-2.5.0+cu121:\n",
            "      Successfully uninstalled torch-2.5.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.4.1 which is incompatible.\n",
            "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.1 triton-3.0.0\n",
            "Collecting dgl==0.9.0\n",
            "  Downloading dgl-0.9.0-cp310-cp310-manylinux1_x86_64.whl.metadata (557 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9.0) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9.0) (3.4.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl==0.9.0) (4.66.5)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9.0) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==0.9.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==0.9.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==0.9.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==0.9.0) (2024.8.30)\n",
            "Downloading dgl-0.9.0-cp310-cp310-manylinux1_x86_64.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dgl\n",
            "Successfully installed dgl-0.9.0\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.10.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->torch_geometric) (0.2.0)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5269fd86",
      "metadata": {
        "id": "5269fd86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ac1d3d-82bf-43a5-837f-827d7b2cdff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:104: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=th.float16)\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:128: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, dZ):\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:177: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=th.float16)\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:207: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, *dZ):\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:287: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=th.float16)\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:304: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, dZ):\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:352: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=th.float16)\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:371: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, *dZ):\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:431: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=th.float16)\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:467: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, grad_out):\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:498: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=th.float16)\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:535: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, *grad_out):\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:566: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=th.float16)\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:575: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, dy):\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:595: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=th.float16)\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:603: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, dy):\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:666: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=th.float16)\n",
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/sparse.py:692: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=th.float16)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import os\n",
        "from natsort import natsorted\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import string\n",
        "from numpy.linalg import norm\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import FastText\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import dgl.data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, Dense,Bidirectional\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "import numpy as np\n",
        "from dgl.nn import GraphConv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import os\n",
        "from natsort import natsorted\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import FastText\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import dgl.data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, Dense,Bidirectional\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input\n",
        "from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16,preprocess_input\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl torch_geometric tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g88TYNFQ9PW",
        "outputId": "2c8eb913-3ffe-41b4-d782-23b7b0fffa80",
        "collapsed": true
      },
      "id": "7g88TYNFQ9PW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl\n",
            "  Downloading dgl-2.1.0-cp310-cp310-manylinux1_x86_64.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: torchdata>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (0.7.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2024.2.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata>=0.5.0->dgl) (2.2.1+cu121)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.5.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.12)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, torch_geometric, nvidia-cusolver-cu12, dgl\n",
            "Successfully installed dgl-2.1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torch_geometric-2.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "78493f0c-b58b-45b4-8979-c48b0adc3428",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "78493f0c-b58b-45b4-8979-c48b0adc3428",
        "outputId": "ecdce8ec-b475-41f6-f3b5-2ccefe9b7094",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m33188688/33188688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ zero_padding2d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m230\u001b[0m, \u001b[38;5;34m230\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mZeroPadding2D\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1_conv (\u001b[38;5;33mConv2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m9,408\u001b[0m │ zero_padding2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1_bn                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │            \u001b[38;5;34m256\u001b[0m │ conv1_conv[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1_relu (\u001b[38;5;33mActivation\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ zero_padding2d_1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m114\u001b[0m, \u001b[38;5;34m114\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv1_relu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mZeroPadding2D\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ zero_padding2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │            \u001b[38;5;34m256\u001b[0m │ pool1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ conv2_block1_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │          \u001b[38;5;34m8,192\u001b[0m │ conv2_block1_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv2_block1_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block1_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv2_block1_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ pool1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv2_block1_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │            \u001b[38;5;34m384\u001b[0m │ conv2_block1_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ conv2_block2_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m12,288\u001b[0m │ conv2_block2_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv2_block2_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block2_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv2_block2_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block1_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv2_block2_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv2_block2_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block3_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m16,384\u001b[0m │ conv2_block3_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv2_block3_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block3_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv2_block3_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m160\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block2_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv2_block3_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m160\u001b[0m)    │            \u001b[38;5;34m640\u001b[0m │ conv2_block3_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m160\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block4_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m20,480\u001b[0m │ conv2_block4_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv2_block4_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block4_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv2_block4_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m192\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block3_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv2_block4_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m192\u001b[0m)    │            \u001b[38;5;34m768\u001b[0m │ conv2_block4_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m192\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block5_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m24,576\u001b[0m │ conv2_block5_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv2_block5_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block5_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv2_block5_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m224\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block4_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv2_block5_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m224\u001b[0m)    │            \u001b[38;5;34m896\u001b[0m │ conv2_block5_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m224\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block6_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m28,672\u001b[0m │ conv2_block6_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv2_block6_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block6_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv2_block6_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block5_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv2_block6_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool2_bn                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m1,024\u001b[0m │ conv2_block6_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool2_relu (\u001b[38;5;33mActivation\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ pool2_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool2_conv (\u001b[38;5;33mConv2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m32,768\u001b[0m │ pool2_relu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool2_pool                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ pool2_conv[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)        │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ pool2_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block1_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m16,384\u001b[0m │ conv3_block1_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block1_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block1_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block1_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m160\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ pool2_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block1_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m160\u001b[0m)    │            \u001b[38;5;34m640\u001b[0m │ conv3_block1_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m160\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block2_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m20,480\u001b[0m │ conv3_block2_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block2_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block2_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block2_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m192\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block1_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block2_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m192\u001b[0m)    │            \u001b[38;5;34m768\u001b[0m │ conv3_block2_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m192\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block3_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m24,576\u001b[0m │ conv3_block3_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block3_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block3_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block3_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m224\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block2_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block3_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m224\u001b[0m)    │            \u001b[38;5;34m896\u001b[0m │ conv3_block3_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m224\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block4_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m28,672\u001b[0m │ conv3_block4_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block4_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block4_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block4_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block3_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block4_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m1,024\u001b[0m │ conv3_block4_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block5_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m32,768\u001b[0m │ conv3_block5_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block5_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block5_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block5_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m288\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block4_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block5_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m288\u001b[0m)    │          \u001b[38;5;34m1,152\u001b[0m │ conv3_block5_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m288\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block6_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block6_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block6_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block6_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block6_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m320\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block5_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block6_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m320\u001b[0m)    │          \u001b[38;5;34m1,280\u001b[0m │ conv3_block6_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m320\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block7_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m40,960\u001b[0m │ conv3_block7_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block7_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block7_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block7_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m352\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block6_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block7_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m352\u001b[0m)    │          \u001b[38;5;34m1,408\u001b[0m │ conv3_block7_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m352\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block8_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m45,056\u001b[0m │ conv3_block8_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block8_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block8_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block8_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m384\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block7_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block8_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m384\u001b[0m)    │          \u001b[38;5;34m1,536\u001b[0m │ conv3_block8_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m384\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block9_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m49,152\u001b[0m │ conv3_block9_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block9_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block9_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block9_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m416\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block8_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block9_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m416\u001b[0m)    │          \u001b[38;5;34m1,664\u001b[0m │ conv3_block9_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m416\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block10_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m53,248\u001b[0m │ conv3_block10_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block10_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block10_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block10_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m448\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block9_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block10_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m448\u001b[0m)    │          \u001b[38;5;34m1,792\u001b[0m │ conv3_block10_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m448\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block11_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m57,344\u001b[0m │ conv3_block11_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block11_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block11_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block11_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m480\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block10_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block11_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m480\u001b[0m)    │          \u001b[38;5;34m1,920\u001b[0m │ conv3_block11_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m480\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block12_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m61,440\u001b[0m │ conv3_block12_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block12_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block12_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block12_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block11_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block12_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool3_bn                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │          \u001b[38;5;34m2,048\u001b[0m │ conv3_block12_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool3_relu (\u001b[38;5;33mActivation\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ pool3_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool3_conv (\u001b[38;5;33mConv2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m131,072\u001b[0m │ pool3_relu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool3_pool                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ pool3_conv[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)        │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m1,024\u001b[0m │ pool3_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block1_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m32,768\u001b[0m │ conv4_block1_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block1_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block1_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block1_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m288\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ pool3_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block1_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m288\u001b[0m)    │          \u001b[38;5;34m1,152\u001b[0m │ conv4_block1_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m288\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block2_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block2_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block2_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block2_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block2_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m320\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block1_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block2_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m320\u001b[0m)    │          \u001b[38;5;34m1,280\u001b[0m │ conv4_block2_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m320\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block3_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m40,960\u001b[0m │ conv4_block3_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block3_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block3_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block3_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m352\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block2_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block3_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m352\u001b[0m)    │          \u001b[38;5;34m1,408\u001b[0m │ conv4_block3_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m352\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block4_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m45,056\u001b[0m │ conv4_block4_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block4_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block4_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block4_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m384\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block3_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block4_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m384\u001b[0m)    │          \u001b[38;5;34m1,536\u001b[0m │ conv4_block4_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m384\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block5_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m49,152\u001b[0m │ conv4_block5_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block5_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block5_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block5_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m416\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block4_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block5_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m416\u001b[0m)    │          \u001b[38;5;34m1,664\u001b[0m │ conv4_block5_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m416\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block6_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m53,248\u001b[0m │ conv4_block6_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block6_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block6_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block6_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m448\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block5_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block6_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m448\u001b[0m)    │          \u001b[38;5;34m1,792\u001b[0m │ conv4_block6_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m448\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block7_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m57,344\u001b[0m │ conv4_block7_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block7_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block7_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block7_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m480\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block6_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block7_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m480\u001b[0m)    │          \u001b[38;5;34m1,920\u001b[0m │ conv4_block7_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m480\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block8_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m61,440\u001b[0m │ conv4_block8_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block8_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block8_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block8_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block7_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block8_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │          \u001b[38;5;34m2,048\u001b[0m │ conv4_block8_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block9_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m65,536\u001b[0m │ conv4_block9_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block9_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block9_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block9_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m544\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block8_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block9_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m544\u001b[0m)    │          \u001b[38;5;34m2,176\u001b[0m │ conv4_block9_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m544\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block10_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m69,632\u001b[0m │ conv4_block10_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block10_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block10_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block10_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m576\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block9_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block10_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m576\u001b[0m)    │          \u001b[38;5;34m2,304\u001b[0m │ conv4_block10_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m576\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block11_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m73,728\u001b[0m │ conv4_block11_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block11_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block11_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block11_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m608\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block10_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block11_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m608\u001b[0m)    │          \u001b[38;5;34m2,432\u001b[0m │ conv4_block11_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m608\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block12_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m77,824\u001b[0m │ conv4_block12_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block12_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block12_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block12_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m640\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block11_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block12_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m640\u001b[0m)    │          \u001b[38;5;34m2,560\u001b[0m │ conv4_block12_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m640\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block13_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m81,920\u001b[0m │ conv4_block13_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block13_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block13_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block13_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m672\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block12_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block13_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m672\u001b[0m)    │          \u001b[38;5;34m2,688\u001b[0m │ conv4_block13_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m672\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block14_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m86,016\u001b[0m │ conv4_block14_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block14_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block14_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block14_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m704\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block13_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block14_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m704\u001b[0m)    │          \u001b[38;5;34m2,816\u001b[0m │ conv4_block14_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m704\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block15_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m90,112\u001b[0m │ conv4_block15_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block15_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block15_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block15_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m736\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block14_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block15_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m736\u001b[0m)    │          \u001b[38;5;34m2,944\u001b[0m │ conv4_block15_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m736\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block16_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m94,208\u001b[0m │ conv4_block16_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block16_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block16_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block16_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block15_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block16_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)    │          \u001b[38;5;34m3,072\u001b[0m │ conv4_block16_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block17_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m98,304\u001b[0m │ conv4_block17_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block17_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block17_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block17_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m800\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block16_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block17_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m800\u001b[0m)    │          \u001b[38;5;34m3,200\u001b[0m │ conv4_block17_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m800\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block18_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m102,400\u001b[0m │ conv4_block18_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block18_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block18_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block18_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m832\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block17_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block18_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m832\u001b[0m)    │          \u001b[38;5;34m3,328\u001b[0m │ conv4_block18_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m832\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block19_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m106,496\u001b[0m │ conv4_block19_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block19_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block19_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block19_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m864\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block18_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block19_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m864\u001b[0m)    │          \u001b[38;5;34m3,456\u001b[0m │ conv4_block19_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m864\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block20_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m110,592\u001b[0m │ conv4_block20_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block20_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block20_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block20_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m896\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block19_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block20_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m896\u001b[0m)    │          \u001b[38;5;34m3,584\u001b[0m │ conv4_block20_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m896\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block21_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m114,688\u001b[0m │ conv4_block21_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block21_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block21_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block21_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m928\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block20_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block21_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m928\u001b[0m)    │          \u001b[38;5;34m3,712\u001b[0m │ conv4_block21_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m928\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block22_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m118,784\u001b[0m │ conv4_block22_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block22_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block22_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block22_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m960\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block21_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block22_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m960\u001b[0m)    │          \u001b[38;5;34m3,840\u001b[0m │ conv4_block22_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m960\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block23_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m122,880\u001b[0m │ conv4_block23_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block23_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block23_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block23_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m992\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block22_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block23_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m992\u001b[0m)    │          \u001b[38;5;34m3,968\u001b[0m │ conv4_block23_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m992\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block24_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m126,976\u001b[0m │ conv4_block24_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block24_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block24_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block24_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m1024\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv4_block23_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block24_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool4_bn                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m1024\u001b[0m)   │          \u001b[38;5;34m4,096\u001b[0m │ conv4_block24_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool4_relu (\u001b[38;5;33mActivation\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m1024\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ pool4_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool4_conv (\u001b[38;5;33mConv2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │        \u001b[38;5;34m524,288\u001b[0m │ pool4_relu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool4_pool                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ pool4_conv[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)        │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │          \u001b[38;5;34m2,048\u001b[0m │ pool4_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block1_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m65,536\u001b[0m │ conv5_block1_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block1_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block1_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block1_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m544\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ pool4_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block1_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m544\u001b[0m)      │          \u001b[38;5;34m2,176\u001b[0m │ conv5_block1_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m544\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block2_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m69,632\u001b[0m │ conv5_block2_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block2_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block2_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block2_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m576\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block1_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block2_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m576\u001b[0m)      │          \u001b[38;5;34m2,304\u001b[0m │ conv5_block2_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m576\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block3_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m73,728\u001b[0m │ conv5_block3_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block3_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block3_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block3_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m608\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block2_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block3_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m608\u001b[0m)      │          \u001b[38;5;34m2,432\u001b[0m │ conv5_block3_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m608\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block4_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m77,824\u001b[0m │ conv5_block4_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block4_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block4_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block4_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m640\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block3_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block4_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m640\u001b[0m)      │          \u001b[38;5;34m2,560\u001b[0m │ conv5_block4_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m640\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block5_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m81,920\u001b[0m │ conv5_block5_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block5_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block5_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block5_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m672\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block4_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block5_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m672\u001b[0m)      │          \u001b[38;5;34m2,688\u001b[0m │ conv5_block5_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m672\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block6_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m86,016\u001b[0m │ conv5_block6_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block6_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block6_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block6_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m704\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block5_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block6_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m704\u001b[0m)      │          \u001b[38;5;34m2,816\u001b[0m │ conv5_block6_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m704\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block7_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m90,112\u001b[0m │ conv5_block7_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block7_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block7_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block7_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m736\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block6_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block7_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m736\u001b[0m)      │          \u001b[38;5;34m2,944\u001b[0m │ conv5_block7_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m736\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block8_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m94,208\u001b[0m │ conv5_block8_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block8_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block8_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block8_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m768\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block7_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block8_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m768\u001b[0m)      │          \u001b[38;5;34m3,072\u001b[0m │ conv5_block8_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m768\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block9_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m98,304\u001b[0m │ conv5_block9_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block9_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block9_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block9_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m800\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block8_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block9_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m800\u001b[0m)      │          \u001b[38;5;34m3,200\u001b[0m │ conv5_block9_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m800\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block10_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m102,400\u001b[0m │ conv5_block10_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block10_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block10_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block10_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m832\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block9_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block10_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m832\u001b[0m)      │          \u001b[38;5;34m3,328\u001b[0m │ conv5_block10_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m832\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block11_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m106,496\u001b[0m │ conv5_block11_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block11_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block11_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block11_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m864\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block10_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block11_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m864\u001b[0m)      │          \u001b[38;5;34m3,456\u001b[0m │ conv5_block11_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m864\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block12_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m110,592\u001b[0m │ conv5_block12_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block12_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block12_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block12_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m896\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block11_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block12_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m896\u001b[0m)      │          \u001b[38;5;34m3,584\u001b[0m │ conv5_block12_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m896\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block13_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m114,688\u001b[0m │ conv5_block13_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block13_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block13_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block13_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m928\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block12_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block13_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m928\u001b[0m)      │          \u001b[38;5;34m3,712\u001b[0m │ conv5_block13_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m928\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block14_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m118,784\u001b[0m │ conv5_block14_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block14_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block14_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block14_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m960\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block13_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block14_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m960\u001b[0m)      │          \u001b[38;5;34m3,840\u001b[0m │ conv5_block14_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m960\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block15_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m122,880\u001b[0m │ conv5_block15_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block15_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block15_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block15_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m992\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block14_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block15_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m992\u001b[0m)      │          \u001b[38;5;34m3,968\u001b[0m │ conv5_block15_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m992\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block16_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m126,976\u001b[0m │ conv5_block16_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block16_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block16_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block16_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ conv5_block15_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block16_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bn (\u001b[38;5;33mBatchNormalization\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │          \u001b[38;5;34m4,096\u001b[0m │ conv5_block16_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ relu (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ avg_pool                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ relu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ zero_padding2d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">230</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">230</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ZeroPadding2D</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1_conv (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">9,408</span> │ zero_padding2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1_bn                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1_relu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ zero_padding2d_1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">114</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">114</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ZeroPadding2D</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ zero_padding2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ pool1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block1_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │ conv2_block1_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2_block1_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block1_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv2_block1_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv2_block1_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │ conv2_block1_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block2_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">12,288</span> │ conv2_block2_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2_block2_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block2_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv2_block2_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block1_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv2_block2_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2_block2_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block3_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span> │ conv2_block3_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2_block3_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block3_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv2_block3_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block2_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv2_block3_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ conv2_block3_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block4_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">20,480</span> │ conv2_block4_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2_block4_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block4_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv2_block4_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block3_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv2_block4_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ conv2_block4_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block5_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">24,576</span> │ conv2_block5_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2_block5_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block5_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv2_block5_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block4_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv2_block5_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ conv2_block5_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block6_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">28,672</span> │ conv2_block6_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2_block6_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block6_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv2_block6_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block5_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv2_block6_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool2_bn                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv2_block6_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool2_relu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool2_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool2_conv (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span> │ pool2_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool2_pool                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)        │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ pool2_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block1_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span> │ conv3_block1_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block1_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block1_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block1_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool2_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block1_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ conv3_block1_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block2_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">20,480</span> │ conv3_block2_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block2_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block2_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block2_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block1_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block2_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ conv3_block2_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block3_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">24,576</span> │ conv3_block3_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block3_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block3_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block3_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block2_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block3_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ conv3_block3_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block4_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">28,672</span> │ conv3_block4_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block4_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block4_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block4_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block3_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block4_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv3_block4_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block5_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span> │ conv3_block5_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block5_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block5_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block5_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block4_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block5_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> │ conv3_block5_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block6_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block6_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block6_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block6_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block6_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block5_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block6_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> │ conv3_block6_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block7_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">40,960</span> │ conv3_block7_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block7_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block7_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block7_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">352</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block6_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block7_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">352</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> │ conv3_block7_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">352</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block8_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">45,056</span> │ conv3_block8_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block8_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block8_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block8_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block7_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block8_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ conv3_block8_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block9_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,152</span> │ conv3_block9_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block9_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block9_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block9_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block8_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block9_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span> │ conv3_block9_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block10_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">53,248</span> │ conv3_block10_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block10_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block10_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block10_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block9_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block10_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │ conv3_block10_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block11_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">57,344</span> │ conv3_block11_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block11_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block11_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block11_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block10_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block11_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920</span> │ conv3_block11_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block12_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">61,440</span> │ conv3_block12_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block12_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block12_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block12_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block11_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block12_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool3_bn                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv3_block12_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool3_relu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool3_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool3_conv (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">131,072</span> │ pool3_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool3_pool                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool3_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)        │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ pool3_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block1_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span> │ conv4_block1_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block1_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block1_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block1_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool3_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block1_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> │ conv4_block1_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block2_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block2_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block2_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block2_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block2_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block1_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block2_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> │ conv4_block2_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block3_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">40,960</span> │ conv4_block3_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block3_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block3_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block3_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">352</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block2_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block3_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">352</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> │ conv4_block3_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">352</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block4_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">45,056</span> │ conv4_block4_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block4_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block4_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block4_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block3_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block4_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ conv4_block4_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block5_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,152</span> │ conv4_block5_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block5_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block5_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block5_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block4_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block5_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span> │ conv4_block5_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block6_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">53,248</span> │ conv4_block6_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block6_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block6_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block6_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block5_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block6_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │ conv4_block6_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block7_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">57,344</span> │ conv4_block7_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block7_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block7_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block7_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block6_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block7_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920</span> │ conv4_block7_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block8_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">61,440</span> │ conv4_block8_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block8_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block8_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block8_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block7_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block8_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv4_block8_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block9_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,536</span> │ conv4_block9_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block9_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block9_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block9_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block8_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block9_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,176</span> │ conv4_block9_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block10_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">69,632</span> │ conv4_block10_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block10_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block10_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block10_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block9_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block10_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,304</span> │ conv4_block10_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block11_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">73,728</span> │ conv4_block11_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block11_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block11_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block11_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">608</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block10_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block11_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">608</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │ conv4_block11_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">608</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block12_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">77,824</span> │ conv4_block12_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block12_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block12_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block12_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block11_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block12_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span> │ conv4_block12_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block13_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">81,920</span> │ conv4_block13_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block13_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block13_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block13_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">672</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block12_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block13_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">672</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,688</span> │ conv4_block13_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">672</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block14_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">86,016</span> │ conv4_block14_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block14_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block14_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block14_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block13_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block14_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,816</span> │ conv4_block14_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block15_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">90,112</span> │ conv4_block15_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block15_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block15_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block15_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">736</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block14_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block15_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">736</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,944</span> │ conv4_block15_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">736</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block16_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">94,208</span> │ conv4_block16_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block16_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block16_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block16_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block15_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block16_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> │ conv4_block16_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block17_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">98,304</span> │ conv4_block17_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block17_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block17_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block17_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block16_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block17_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span> │ conv4_block17_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block18_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">102,400</span> │ conv4_block18_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block18_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block18_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block18_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block17_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block18_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,328</span> │ conv4_block18_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block19_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">106,496</span> │ conv4_block19_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block19_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block19_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block19_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">864</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block18_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block19_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">864</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,456</span> │ conv4_block19_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">864</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block20_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">110,592</span> │ conv4_block20_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block20_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block20_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block20_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block19_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block20_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,584</span> │ conv4_block20_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block21_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">114,688</span> │ conv4_block21_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block21_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block21_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block21_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">928</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block20_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block21_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">928</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,712</span> │ conv4_block21_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">928</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block22_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">118,784</span> │ conv4_block22_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block22_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block22_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block22_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">960</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block21_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block22_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">960</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840</span> │ conv4_block22_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">960</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block23_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">122,880</span> │ conv4_block23_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block23_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block23_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block23_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">992</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block22_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block23_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">992</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,968</span> │ conv4_block23_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">992</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block24_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">126,976</span> │ conv4_block24_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block24_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block24_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block24_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block23_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block24_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool4_bn                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ conv4_block24_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool4_relu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool4_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool4_conv (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288</span> │ pool4_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool4_pool                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool4_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)        │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ pool4_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block1_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,536</span> │ conv5_block1_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block1_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block1_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block1_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool4_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block1_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,176</span> │ conv5_block1_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block2_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">69,632</span> │ conv5_block2_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block2_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block2_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block2_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block1_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block2_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,304</span> │ conv5_block2_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block3_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">73,728</span> │ conv5_block3_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block3_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block3_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block3_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">608</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block2_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block3_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">608</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │ conv5_block3_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">608</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block4_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">77,824</span> │ conv5_block4_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block4_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block4_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block4_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block3_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block4_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span> │ conv5_block4_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block5_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">81,920</span> │ conv5_block5_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block5_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block5_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block5_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">672</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block4_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block5_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">672</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,688</span> │ conv5_block5_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">672</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block6_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">86,016</span> │ conv5_block6_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block6_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block6_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block6_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block5_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block6_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,816</span> │ conv5_block6_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block7_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">90,112</span> │ conv5_block7_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block7_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block7_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block7_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">736</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block6_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block7_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">736</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,944</span> │ conv5_block7_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">736</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block8_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">94,208</span> │ conv5_block8_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block8_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block8_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block8_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block7_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block8_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> │ conv5_block8_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block9_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">98,304</span> │ conv5_block9_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block9_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block9_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block9_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block8_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block9_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span> │ conv5_block9_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block10_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">102,400</span> │ conv5_block10_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block10_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block10_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block10_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block9_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block10_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,328</span> │ conv5_block10_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block11_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">106,496</span> │ conv5_block11_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block11_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block11_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block11_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">864</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block10_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block11_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">864</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,456</span> │ conv5_block11_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">864</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block12_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">110,592</span> │ conv5_block12_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block12_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block12_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block12_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block11_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block12_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,584</span> │ conv5_block12_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block13_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">114,688</span> │ conv5_block13_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block13_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block13_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block13_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">928</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block12_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block13_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">928</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,712</span> │ conv5_block13_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">928</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block14_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">118,784</span> │ conv5_block14_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block14_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block14_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block14_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">960</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block13_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block14_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">960</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840</span> │ conv5_block14_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">960</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block15_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">122,880</span> │ conv5_block15_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block15_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block15_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block15_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">992</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block14_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block15_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">992</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,968</span> │ conv5_block15_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">992</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block16_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">126,976</span> │ conv5_block16_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block16_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block16_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block16_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block15_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block16_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ conv5_block16_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ relu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ avg_pool                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,037,504\u001b[0m (26.85 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,037,504</span> (26.85 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,953,856\u001b[0m (26.53 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,953,856</span> (26.53 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m83,648\u001b[0m (326.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">83,648</span> (326.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "model= DenseNet121()\n",
        "model1=Model(inputs=model.inputs,outputs=model.layers[-2].output)\n",
        "print(model1.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a5b3e9f7-a59a-43c8-80eb-3f565637ea10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5b3e9f7-a59a-43c8-80eb-3f565637ea10",
        "outputId": "4ceb9e2b-0eb3-4de3-e105-54828d0fe4bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1d58485c-a94a-4de2-a603-c0179e9aa7fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1d58485c-a94a-4de2-a603-c0179e9aa7fa",
        "outputId": "ebc85676-da36-445b-9d18-4d90d6450a0d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ zero_padding2d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m230\u001b[0m, \u001b[38;5;34m230\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mZeroPadding2D\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1_conv (\u001b[38;5;33mConv2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m9,408\u001b[0m │ zero_padding2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1_bn                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │            \u001b[38;5;34m256\u001b[0m │ conv1_conv[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1_relu (\u001b[38;5;33mActivation\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ zero_padding2d_1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m114\u001b[0m, \u001b[38;5;34m114\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv1_relu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mZeroPadding2D\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ zero_padding2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │            \u001b[38;5;34m256\u001b[0m │ pool1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ conv2_block1_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │          \u001b[38;5;34m8,192\u001b[0m │ conv2_block1_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv2_block1_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block1_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv2_block1_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ pool1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv2_block1_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │            \u001b[38;5;34m384\u001b[0m │ conv2_block1_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ conv2_block2_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m12,288\u001b[0m │ conv2_block2_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv2_block2_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block2_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv2_block2_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block1_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv2_block2_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv2_block2_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block3_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m16,384\u001b[0m │ conv2_block3_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv2_block3_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block3_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv2_block3_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m160\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block2_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv2_block3_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m160\u001b[0m)    │            \u001b[38;5;34m640\u001b[0m │ conv2_block3_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m160\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block4_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m20,480\u001b[0m │ conv2_block4_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv2_block4_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block4_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv2_block4_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m192\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block3_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv2_block4_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m192\u001b[0m)    │            \u001b[38;5;34m768\u001b[0m │ conv2_block4_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m192\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block5_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m24,576\u001b[0m │ conv2_block5_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv2_block5_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block5_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv2_block5_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m224\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block4_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv2_block5_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m224\u001b[0m)    │            \u001b[38;5;34m896\u001b[0m │ conv2_block5_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m224\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block6_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m28,672\u001b[0m │ conv2_block6_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv2_block6_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block6_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv2_block6_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2_block5_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv2_block6_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool2_bn                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m1,024\u001b[0m │ conv2_block6_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool2_relu (\u001b[38;5;33mActivation\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ pool2_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool2_conv (\u001b[38;5;33mConv2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m32,768\u001b[0m │ pool2_relu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool2_pool                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ pool2_conv[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)        │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ pool2_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block1_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m16,384\u001b[0m │ conv3_block1_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block1_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block1_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block1_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m160\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ pool2_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block1_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m160\u001b[0m)    │            \u001b[38;5;34m640\u001b[0m │ conv3_block1_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m160\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block2_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m20,480\u001b[0m │ conv3_block2_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block2_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block2_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block2_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m192\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block1_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block2_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m192\u001b[0m)    │            \u001b[38;5;34m768\u001b[0m │ conv3_block2_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m192\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block3_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m24,576\u001b[0m │ conv3_block3_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block3_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block3_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block3_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m224\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block2_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block3_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m224\u001b[0m)    │            \u001b[38;5;34m896\u001b[0m │ conv3_block3_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m224\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block4_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m28,672\u001b[0m │ conv3_block4_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block4_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block4_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block4_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block3_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block4_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m1,024\u001b[0m │ conv3_block4_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block5_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m32,768\u001b[0m │ conv3_block5_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block5_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block5_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block5_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m288\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block4_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block5_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m288\u001b[0m)    │          \u001b[38;5;34m1,152\u001b[0m │ conv3_block5_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m288\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block6_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block6_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block6_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block6_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block6_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m320\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block5_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block6_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m320\u001b[0m)    │          \u001b[38;5;34m1,280\u001b[0m │ conv3_block6_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m320\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block7_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m40,960\u001b[0m │ conv3_block7_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block7_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block7_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block7_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m352\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block6_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block7_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m352\u001b[0m)    │          \u001b[38;5;34m1,408\u001b[0m │ conv3_block7_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m352\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block8_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m45,056\u001b[0m │ conv3_block8_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block8_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block8_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block8_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m384\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block7_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block8_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m384\u001b[0m)    │          \u001b[38;5;34m1,536\u001b[0m │ conv3_block8_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m384\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block9_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m49,152\u001b[0m │ conv3_block9_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block9_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block9_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block9_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m416\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block8_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block9_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m416\u001b[0m)    │          \u001b[38;5;34m1,664\u001b[0m │ conv3_block9_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m416\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block10_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m53,248\u001b[0m │ conv3_block10_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block10_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block10_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block10_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m448\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block9_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block10_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m448\u001b[0m)    │          \u001b[38;5;34m1,792\u001b[0m │ conv3_block10_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m448\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block11_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m57,344\u001b[0m │ conv3_block11_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block11_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block11_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block11_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m480\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block10_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block11_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m480\u001b[0m)    │          \u001b[38;5;34m1,920\u001b[0m │ conv3_block11_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m480\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block12_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m61,440\u001b[0m │ conv3_block12_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv3_block12_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block12_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv3_block12_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv3_block11_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv3_block12_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool3_bn                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │          \u001b[38;5;34m2,048\u001b[0m │ conv3_block12_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool3_relu (\u001b[38;5;33mActivation\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ pool3_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool3_conv (\u001b[38;5;33mConv2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m131,072\u001b[0m │ pool3_relu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool3_pool                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ pool3_conv[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)        │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m1,024\u001b[0m │ pool3_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block1_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m32,768\u001b[0m │ conv4_block1_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block1_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block1_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block1_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m288\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ pool3_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block1_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m288\u001b[0m)    │          \u001b[38;5;34m1,152\u001b[0m │ conv4_block1_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m288\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block2_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block2_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block2_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block2_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block2_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m320\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block1_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block2_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m320\u001b[0m)    │          \u001b[38;5;34m1,280\u001b[0m │ conv4_block2_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m320\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block3_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m40,960\u001b[0m │ conv4_block3_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block3_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block3_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block3_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m352\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block2_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block3_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m352\u001b[0m)    │          \u001b[38;5;34m1,408\u001b[0m │ conv4_block3_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m352\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block4_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m45,056\u001b[0m │ conv4_block4_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block4_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block4_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block4_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m384\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block3_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block4_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m384\u001b[0m)    │          \u001b[38;5;34m1,536\u001b[0m │ conv4_block4_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m384\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block5_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m49,152\u001b[0m │ conv4_block5_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block5_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block5_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block5_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m416\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block4_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block5_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m416\u001b[0m)    │          \u001b[38;5;34m1,664\u001b[0m │ conv4_block5_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m416\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block6_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m53,248\u001b[0m │ conv4_block6_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block6_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block6_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block6_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m448\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block5_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block6_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m448\u001b[0m)    │          \u001b[38;5;34m1,792\u001b[0m │ conv4_block6_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m448\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block7_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m57,344\u001b[0m │ conv4_block7_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block7_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block7_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block7_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m480\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block6_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block7_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m480\u001b[0m)    │          \u001b[38;5;34m1,920\u001b[0m │ conv4_block7_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m480\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block8_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m61,440\u001b[0m │ conv4_block8_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block8_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block8_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block8_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block7_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block8_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │          \u001b[38;5;34m2,048\u001b[0m │ conv4_block8_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block9_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m65,536\u001b[0m │ conv4_block9_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block9_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block9_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block9_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m544\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block8_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block9_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m544\u001b[0m)    │          \u001b[38;5;34m2,176\u001b[0m │ conv4_block9_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m544\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block10_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m69,632\u001b[0m │ conv4_block10_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block10_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block10_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block10_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m576\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block9_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block10_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m576\u001b[0m)    │          \u001b[38;5;34m2,304\u001b[0m │ conv4_block10_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m576\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block11_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m73,728\u001b[0m │ conv4_block11_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block11_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block11_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block11_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m608\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block10_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block11_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m608\u001b[0m)    │          \u001b[38;5;34m2,432\u001b[0m │ conv4_block11_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m608\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block12_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m77,824\u001b[0m │ conv4_block12_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block12_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block12_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block12_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m640\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block11_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block12_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m640\u001b[0m)    │          \u001b[38;5;34m2,560\u001b[0m │ conv4_block12_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m640\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block13_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m81,920\u001b[0m │ conv4_block13_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block13_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block13_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block13_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m672\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block12_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block13_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m672\u001b[0m)    │          \u001b[38;5;34m2,688\u001b[0m │ conv4_block13_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m672\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block14_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m86,016\u001b[0m │ conv4_block14_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block14_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block14_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block14_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m704\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block13_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block14_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m704\u001b[0m)    │          \u001b[38;5;34m2,816\u001b[0m │ conv4_block14_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m704\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block15_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m90,112\u001b[0m │ conv4_block15_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block15_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block15_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block15_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m736\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block14_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block15_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m736\u001b[0m)    │          \u001b[38;5;34m2,944\u001b[0m │ conv4_block15_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m736\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block16_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m94,208\u001b[0m │ conv4_block16_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block16_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block16_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block16_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block15_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block16_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)    │          \u001b[38;5;34m3,072\u001b[0m │ conv4_block16_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m768\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block17_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m98,304\u001b[0m │ conv4_block17_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block17_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block17_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block17_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m800\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block16_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block17_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m800\u001b[0m)    │          \u001b[38;5;34m3,200\u001b[0m │ conv4_block17_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m800\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block18_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m102,400\u001b[0m │ conv4_block18_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block18_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block18_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block18_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m832\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block17_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block18_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m832\u001b[0m)    │          \u001b[38;5;34m3,328\u001b[0m │ conv4_block18_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m832\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block19_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m106,496\u001b[0m │ conv4_block19_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block19_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block19_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block19_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m864\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block18_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block19_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m864\u001b[0m)    │          \u001b[38;5;34m3,456\u001b[0m │ conv4_block19_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m864\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block20_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m110,592\u001b[0m │ conv4_block20_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block20_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block20_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block20_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m896\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block19_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block20_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m896\u001b[0m)    │          \u001b[38;5;34m3,584\u001b[0m │ conv4_block20_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m896\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block21_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m114,688\u001b[0m │ conv4_block21_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block21_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block21_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block21_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m928\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block20_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block21_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m928\u001b[0m)    │          \u001b[38;5;34m3,712\u001b[0m │ conv4_block21_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m928\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block22_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m118,784\u001b[0m │ conv4_block22_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block22_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block22_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block22_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m960\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block21_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block22_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m960\u001b[0m)    │          \u001b[38;5;34m3,840\u001b[0m │ conv4_block22_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m960\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block23_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m122,880\u001b[0m │ conv4_block23_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block23_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block23_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block23_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m992\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block22_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block23_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m992\u001b[0m)    │          \u001b[38;5;34m3,968\u001b[0m │ conv4_block23_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m992\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block24_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m126,976\u001b[0m │ conv4_block24_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │            \u001b[38;5;34m512\u001b[0m │ conv4_block24_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv4_block24_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m36,864\u001b[0m │ conv4_block24_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m1024\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv4_block23_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv4_block24_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool4_bn                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m1024\u001b[0m)   │          \u001b[38;5;34m4,096\u001b[0m │ conv4_block24_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool4_relu (\u001b[38;5;33mActivation\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m1024\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ pool4_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool4_conv (\u001b[38;5;33mConv2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │        \u001b[38;5;34m524,288\u001b[0m │ pool4_relu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool4_pool                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ pool4_conv[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)        │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │          \u001b[38;5;34m2,048\u001b[0m │ pool4_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block1_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m65,536\u001b[0m │ conv5_block1_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block1_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block1_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block1_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m544\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ pool4_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block1_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m544\u001b[0m)      │          \u001b[38;5;34m2,176\u001b[0m │ conv5_block1_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m544\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block2_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m69,632\u001b[0m │ conv5_block2_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block2_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block2_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block2_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m576\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block1_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block2_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m576\u001b[0m)      │          \u001b[38;5;34m2,304\u001b[0m │ conv5_block2_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m576\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block3_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m73,728\u001b[0m │ conv5_block3_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block3_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block3_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block3_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m608\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block2_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block3_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m608\u001b[0m)      │          \u001b[38;5;34m2,432\u001b[0m │ conv5_block3_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m608\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block4_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m77,824\u001b[0m │ conv5_block4_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block4_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block4_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block4_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m640\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block3_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block4_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m640\u001b[0m)      │          \u001b[38;5;34m2,560\u001b[0m │ conv5_block4_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m640\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block5_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m81,920\u001b[0m │ conv5_block5_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block5_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block5_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block5_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m672\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block4_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block5_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m672\u001b[0m)      │          \u001b[38;5;34m2,688\u001b[0m │ conv5_block5_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m672\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block6_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m86,016\u001b[0m │ conv5_block6_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block6_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block6_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block6_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m704\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block5_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block6_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m704\u001b[0m)      │          \u001b[38;5;34m2,816\u001b[0m │ conv5_block6_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m704\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block7_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m90,112\u001b[0m │ conv5_block7_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block7_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block7_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block7_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m736\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block6_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block7_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m736\u001b[0m)      │          \u001b[38;5;34m2,944\u001b[0m │ conv5_block7_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m736\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block8_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m94,208\u001b[0m │ conv5_block8_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block8_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block8_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block8_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m768\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block7_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block8_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_0_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m768\u001b[0m)      │          \u001b[38;5;34m3,072\u001b[0m │ conv5_block8_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_0_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m768\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block9_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_1_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m98,304\u001b[0m │ conv5_block9_0_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_1_bn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block9_1_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_1_relu       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block9_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_2_conv       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block9_1_relu[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_concat       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m800\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block8_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block9_2_conv[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m800\u001b[0m)      │          \u001b[38;5;34m3,200\u001b[0m │ conv5_block9_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m800\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block10_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m102,400\u001b[0m │ conv5_block10_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block10_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block10_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block10_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m832\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block9_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block10_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m832\u001b[0m)      │          \u001b[38;5;34m3,328\u001b[0m │ conv5_block10_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m832\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block11_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m106,496\u001b[0m │ conv5_block11_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block11_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block11_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block11_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m864\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block10_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block11_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m864\u001b[0m)      │          \u001b[38;5;34m3,456\u001b[0m │ conv5_block11_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m864\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block12_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m110,592\u001b[0m │ conv5_block12_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block12_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block12_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block12_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m896\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block11_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block12_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m896\u001b[0m)      │          \u001b[38;5;34m3,584\u001b[0m │ conv5_block12_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m896\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block13_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m114,688\u001b[0m │ conv5_block13_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block13_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block13_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block13_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m928\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block12_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block13_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m928\u001b[0m)      │          \u001b[38;5;34m3,712\u001b[0m │ conv5_block13_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m928\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block14_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m118,784\u001b[0m │ conv5_block14_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block14_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block14_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block14_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m960\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block13_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block14_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m960\u001b[0m)      │          \u001b[38;5;34m3,840\u001b[0m │ conv5_block14_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m960\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block15_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m122,880\u001b[0m │ conv5_block15_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block15_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block15_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block15_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m992\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block14_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block15_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_0_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m992\u001b[0m)      │          \u001b[38;5;34m3,968\u001b[0m │ conv5_block15_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_0_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m992\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block16_0_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_1_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m126,976\u001b[0m │ conv5_block16_0_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_1_bn        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │            \u001b[38;5;34m512\u001b[0m │ conv5_block16_1_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_1_relu      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ conv5_block16_1_bn[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_2_conv      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m36,864\u001b[0m │ conv5_block16_1_relu[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2D\u001b[0m)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ conv5_block15_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv5_block16_2_conv[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bn (\u001b[38;5;33mBatchNormalization\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │          \u001b[38;5;34m4,096\u001b[0m │ conv5_block16_concat[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ relu (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ avg_pool                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ relu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ zero_padding2d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">230</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">230</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ZeroPadding2D</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1_conv (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">9,408</span> │ zero_padding2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1_bn                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1_relu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ zero_padding2d_1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">114</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">114</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ZeroPadding2D</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ zero_padding2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ pool1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block1_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │ conv2_block1_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2_block1_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block1_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv2_block1_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block1_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv2_block1_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │ conv2_block1_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block2_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">12,288</span> │ conv2_block2_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2_block2_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block2_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv2_block2_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block2_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block1_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv2_block2_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2_block2_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block3_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span> │ conv2_block3_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2_block3_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block3_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv2_block3_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block3_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block2_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv2_block3_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ conv2_block3_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block4_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">20,480</span> │ conv2_block4_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2_block4_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block4_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv2_block4_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block4_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block3_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv2_block4_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ conv2_block4_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block5_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">24,576</span> │ conv2_block5_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2_block5_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block5_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv2_block5_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block5_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block4_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv2_block5_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ conv2_block5_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block6_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">28,672</span> │ conv2_block6_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2_block6_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block6_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv2_block6_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2_block6_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2_block5_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv2_block6_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool2_bn                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv2_block6_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool2_relu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool2_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool2_conv (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span> │ pool2_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool2_pool                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)        │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ pool2_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block1_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span> │ conv3_block1_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block1_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block1_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block1_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block1_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool2_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block1_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ conv3_block1_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block2_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">20,480</span> │ conv3_block2_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block2_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block2_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block2_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block2_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block1_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block2_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │ conv3_block2_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block3_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">24,576</span> │ conv3_block3_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block3_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block3_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block3_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block3_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block2_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block3_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ conv3_block3_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block4_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">28,672</span> │ conv3_block4_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block4_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block4_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block4_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block4_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block3_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block4_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv3_block4_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block5_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span> │ conv3_block5_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block5_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block5_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block5_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block5_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block4_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block5_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> │ conv3_block5_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block6_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block6_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block6_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block6_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block6_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block6_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block5_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block6_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> │ conv3_block6_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block7_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">40,960</span> │ conv3_block7_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block7_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block7_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block7_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block7_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">352</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block6_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block7_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">352</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> │ conv3_block7_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">352</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block8_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">45,056</span> │ conv3_block8_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block8_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block8_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block8_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block8_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block7_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block8_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ conv3_block8_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block9_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,152</span> │ conv3_block9_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block9_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block9_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block9_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block9_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block8_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block9_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span> │ conv3_block9_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block10_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">53,248</span> │ conv3_block10_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block10_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block10_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block10_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block10_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block9_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block10_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │ conv3_block10_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block11_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">57,344</span> │ conv3_block11_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block11_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block11_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block11_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block11_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block10_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block11_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920</span> │ conv3_block11_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block12_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">61,440</span> │ conv3_block12_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv3_block12_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block12_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv3_block12_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv3_block12_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3_block11_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv3_block12_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool3_bn                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv3_block12_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool3_relu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool3_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool3_conv (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">131,072</span> │ pool3_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool3_pool                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool3_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)        │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ pool3_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block1_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span> │ conv4_block1_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block1_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block1_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block1_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block1_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool3_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block1_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> │ conv4_block1_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block2_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block2_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block2_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block2_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block2_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block2_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block1_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block2_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> │ conv4_block2_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block3_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">40,960</span> │ conv4_block3_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block3_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block3_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block3_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block3_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">352</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block2_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block3_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">352</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> │ conv4_block3_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">352</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block4_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">45,056</span> │ conv4_block4_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block4_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block4_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block4_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block4_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block3_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block4_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ conv4_block4_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block5_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,152</span> │ conv4_block5_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block5_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block5_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block5_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block5_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block4_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block5_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span> │ conv4_block5_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block6_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">53,248</span> │ conv4_block6_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block6_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block6_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block6_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block6_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block5_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block6_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │ conv4_block6_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block7_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">57,344</span> │ conv4_block7_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block7_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block7_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block7_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block7_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block6_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block7_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920</span> │ conv4_block7_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block8_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">61,440</span> │ conv4_block8_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block8_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block8_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block8_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block8_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block7_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block8_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ conv4_block8_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block9_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,536</span> │ conv4_block9_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block9_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block9_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block9_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block9_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block8_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block9_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,176</span> │ conv4_block9_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block10_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">69,632</span> │ conv4_block10_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block10_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block10_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block10_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block10_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block9_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block10_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,304</span> │ conv4_block10_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block11_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">73,728</span> │ conv4_block11_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block11_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block11_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block11_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block11_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">608</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block10_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block11_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">608</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │ conv4_block11_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">608</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block12_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">77,824</span> │ conv4_block12_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block12_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block12_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block12_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block12_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block11_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block12_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span> │ conv4_block12_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block13_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">81,920</span> │ conv4_block13_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block13_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block13_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block13_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block13_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">672</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block12_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block13_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">672</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,688</span> │ conv4_block13_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">672</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block14_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">86,016</span> │ conv4_block14_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block14_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block14_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block14_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block14_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block13_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block14_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,816</span> │ conv4_block14_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block15_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">90,112</span> │ conv4_block15_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block15_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block15_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block15_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block15_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">736</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block14_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block15_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">736</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,944</span> │ conv4_block15_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">736</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block16_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">94,208</span> │ conv4_block16_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block16_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block16_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block16_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block16_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block15_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block16_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> │ conv4_block16_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block17_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">98,304</span> │ conv4_block17_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block17_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block17_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block17_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block17_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block16_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block17_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span> │ conv4_block17_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block18_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">102,400</span> │ conv4_block18_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block18_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block18_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block18_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block18_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block17_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block18_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,328</span> │ conv4_block18_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block19_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">106,496</span> │ conv4_block19_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block19_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block19_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block19_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block19_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">864</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block18_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block19_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">864</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,456</span> │ conv4_block19_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">864</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block20_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">110,592</span> │ conv4_block20_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block20_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block20_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block20_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block20_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block19_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block20_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,584</span> │ conv4_block20_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block21_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">114,688</span> │ conv4_block21_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block21_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block21_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block21_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block21_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">928</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block20_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block21_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">928</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,712</span> │ conv4_block21_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">928</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block22_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">118,784</span> │ conv4_block22_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block22_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block22_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block22_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block22_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">960</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block21_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block22_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">960</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840</span> │ conv4_block22_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">960</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block23_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">122,880</span> │ conv4_block23_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block23_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block23_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block23_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block23_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">992</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block22_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block23_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">992</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,968</span> │ conv4_block23_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">992</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block24_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">126,976</span> │ conv4_block24_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv4_block24_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block24_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv4_block24_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv4_block24_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv4_block23_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv4_block24_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool4_bn                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ conv4_block24_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool4_relu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool4_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool4_conv (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288</span> │ pool4_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ pool4_pool                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool4_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)        │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ pool4_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block1_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,536</span> │ conv5_block1_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block1_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block1_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block1_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block1_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool4_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block1_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,176</span> │ conv5_block1_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block2_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">69,632</span> │ conv5_block2_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block2_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block2_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block2_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block2_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block1_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block2_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,304</span> │ conv5_block2_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block3_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">73,728</span> │ conv5_block3_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block3_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block3_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block3_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block3_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">608</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block2_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block3_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">608</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │ conv5_block3_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">608</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block4_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">77,824</span> │ conv5_block4_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block4_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block4_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block4_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block4_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block3_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block4_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span> │ conv5_block4_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block5_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">81,920</span> │ conv5_block5_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block5_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block5_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block5_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block5_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">672</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block4_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block5_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">672</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,688</span> │ conv5_block5_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">672</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block6_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">86,016</span> │ conv5_block6_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block6_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block6_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block6_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block6_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block5_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block6_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,816</span> │ conv5_block6_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block7_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">90,112</span> │ conv5_block7_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block7_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block7_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block7_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block7_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">736</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block6_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block7_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">736</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,944</span> │ conv5_block7_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">736</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block8_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">94,208</span> │ conv5_block8_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block8_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block8_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block8_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block8_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block7_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block8_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_0_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> │ conv5_block8_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_0_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block9_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_1_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">98,304</span> │ conv5_block9_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_1_bn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block9_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_1_relu       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block9_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_2_conv       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block9_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block9_concat       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block8_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block9_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span> │ conv5_block9_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block10_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">102,400</span> │ conv5_block10_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block10_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block10_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block10_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block10_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block9_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block10_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,328</span> │ conv5_block10_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block11_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">106,496</span> │ conv5_block11_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block11_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block11_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block11_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block11_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">864</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block10_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block11_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">864</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,456</span> │ conv5_block11_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">864</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block12_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">110,592</span> │ conv5_block12_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block12_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block12_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block12_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block12_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block11_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block12_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,584</span> │ conv5_block12_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block13_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">114,688</span> │ conv5_block13_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block13_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block13_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block13_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block13_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">928</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block12_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block13_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">928</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,712</span> │ conv5_block13_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">928</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block14_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">118,784</span> │ conv5_block14_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block14_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block14_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block14_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block14_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">960</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block13_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block14_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">960</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840</span> │ conv5_block14_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">960</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block15_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">122,880</span> │ conv5_block15_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block15_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block15_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block15_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block15_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">992</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block14_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block15_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_0_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">992</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,968</span> │ conv5_block15_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_0_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">992</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block16_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_1_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">126,976</span> │ conv5_block16_0_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_1_bn        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv5_block16_1_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_1_relu      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block16_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_2_conv      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │ conv5_block16_1_relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv5_block16_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv5_block15_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv5_block16_2_conv[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ conv5_block16_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ relu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ avg_pool                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ relu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,037,504\u001b[0m (26.85 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,037,504</span> (26.85 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,953,856\u001b[0m (26.53 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,953,856</span> (26.53 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m83,648\u001b[0m (326.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">83,648</span> (326.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "print(model1.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3abe0404-2e1f-4818-a194-57eccea695ed",
      "metadata": {
        "id": "3abe0404-2e1f-4818-a194-57eccea695ed"
      },
      "outputs": [],
      "source": [
        "column_names = [f\"col_{i}\" for i in range(1024)]\n",
        "Image_dataframe = pd.DataFrame(columns=column_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Image_dataframe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "tTj-UNE34MvQ",
        "outputId": "9a15a52e-1256-4abe-ff54-3e9024f65767"
      },
      "id": "tTj-UNE34MvQ",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [col_0, col_1, col_2, col_3, col_4, col_5, col_6, col_7, col_8, col_9, col_10, col_11, col_12, col_13, col_14, col_15, col_16, col_17, col_18, col_19, col_20, col_21, col_22, col_23, col_24, col_25, col_26, col_27, col_28, col_29, col_30, col_31, col_32, col_33, col_34, col_35, col_36, col_37, col_38, col_39, col_40, col_41, col_42, col_43, col_44, col_45, col_46, col_47, col_48, col_49, col_50, col_51, col_52, col_53, col_54, col_55, col_56, col_57, col_58, col_59, col_60, col_61, col_62, col_63, col_64, col_65, col_66, col_67, col_68, col_69, col_70, col_71, col_72, col_73, col_74, col_75, col_76, col_77, col_78, col_79, col_80, col_81, col_82, col_83, col_84, col_85, col_86, col_87, col_88, col_89, col_90, col_91, col_92, col_93, col_94, col_95, col_96, col_97, col_98, col_99, ...]\n",
              "Index: []\n",
              "\n",
              "[0 rows x 1024 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb4aedbf-dfaf-4fd1-aa84-ebecd296468a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col_0</th>\n",
              "      <th>col_1</th>\n",
              "      <th>col_2</th>\n",
              "      <th>col_3</th>\n",
              "      <th>col_4</th>\n",
              "      <th>col_5</th>\n",
              "      <th>col_6</th>\n",
              "      <th>col_7</th>\n",
              "      <th>col_8</th>\n",
              "      <th>col_9</th>\n",
              "      <th>...</th>\n",
              "      <th>col_1014</th>\n",
              "      <th>col_1015</th>\n",
              "      <th>col_1016</th>\n",
              "      <th>col_1017</th>\n",
              "      <th>col_1018</th>\n",
              "      <th>col_1019</th>\n",
              "      <th>col_1020</th>\n",
              "      <th>col_1021</th>\n",
              "      <th>col_1022</th>\n",
              "      <th>col_1023</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>0 rows × 1024 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb4aedbf-dfaf-4fd1-aa84-ebecd296468a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eb4aedbf-dfaf-4fd1-aa84-ebecd296468a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eb4aedbf-dfaf-4fd1-aa84-ebecd296468a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_8f1e614d-7204-4a26-b23f-f328edd23936\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('Image_dataframe')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8f1e614d-7204-4a26-b23f-f328edd23936 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('Image_dataframe');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "Image_dataframe"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Replace 'your_uploaded_zip_file.zip' with the actual name of your uploaded zip file\n",
        "zip_file_path = '/Image_data.zip'\n",
        "extracted_folder_path = '/extracted_folder'\n",
        "\n",
        "# Create the target directory for extraction\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "# Check the contents of the extracted folder\n",
        "extracted_subfolder = os.path.join(extracted_folder_path, 'Image_data')\n",
        "extracted_subfolder_contents = os.listdir(extracted_subfolder)\n",
        "print(\"Contents of the extracted subfolder:\", extracted_subfolder_contents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXXGoA8USiVE",
        "outputId": "246ed103-65c9-4b3f-c6a1-693e0388b3bf"
      },
      "id": "eXXGoA8USiVE",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of the extracted subfolder: ['s12', 's3', '.DS_Store', 's13', 's7', 's4', 's11', 's9', 's6', 's5', 's8', 's10', 's1', 's2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2dd14b57-371e-4895-baf4-585c0f4d5989",
      "metadata": {
        "id": "2dd14b57-371e-4895-baf4-585c0f4d5989"
      },
      "outputs": [],
      "source": [
        "def Images_read(root_directory):\n",
        "    file_list = [file for file in os.listdir(root_directory) if not file.startswith('.DS_Store')]\n",
        "    file_list = natsorted(file_list)\n",
        "   # print(file_list)\n",
        "    for subdirectory in file_list:\n",
        "        subdirectory_path = os.path.join(os.path.sep,root_directory, subdirectory, 'img')\n",
        "        file_list_t = [file for file in os.listdir(subdirectory_path) if not file.startswith('.DS_Store')]\n",
        "        file_list_t = natsorted(file_list_t)\n",
        "        B=[]\n",
        "        for filename in file_list_t:\n",
        "            file_path = os.path.join(subdirectory_path, filename)\n",
        "            #print(file_path)\n",
        "            img = cv2.imread(file_path, 1)\n",
        "            R_img = cv2.resize(img, (224, 224))\n",
        "                #R_img.shape\n",
        "            reshaped_img = np.reshape(R_img, (1, 224, 224, 3))\n",
        "            features=model1.predict(reshaped_img,verbose=0)\n",
        "            Image_dataframe.loc[Image_dataframe.shape[0]+1] = features.reshape(-1)\n",
        "    return  Image_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "18b28969-b59d-4089-9e66-38886c82d8c3",
      "metadata": {
        "id": "18b28969-b59d-4089-9e66-38886c82d8c3"
      },
      "outputs": [],
      "source": [
        "dataframe=Images_read(extracted_subfolder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "37023744-34c6-4246-9df1-001161aed2aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37023744-34c6-4246-9df1-001161aed2aa",
        "outputId": "f01b109c-276d-4032-ad52-a29f06e3a291"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(106, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "dataframe.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "3TtumGTl8ZxQ",
        "outputId": "77a84ea3-660f-4f25-d9e1-05a137f7c417"
      },
      "id": "3TtumGTl8ZxQ",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            col_0       col_1       col_2       col_3       col_4       col_5  \\\n",
              "count  106.000000  106.000000  106.000000  106.000000  106.000000  106.000000   \n",
              "mean     0.000123    0.001596    0.013115    0.028490    0.008604    1.067306   \n",
              "std      0.000171    0.003280    0.014912    0.011983    0.020568    1.473539   \n",
              "min      0.000000    0.000000    0.000000    0.006891    0.000000    0.000000   \n",
              "25%      0.000008    0.000000    0.001953    0.018879    0.000000    0.068232   \n",
              "50%      0.000065    0.000439    0.007047    0.027899    0.000000    0.426330   \n",
              "75%      0.000157    0.001517    0.018837    0.036742    0.004838    1.536959   \n",
              "max      0.001028    0.019375    0.058558    0.064134    0.119419    6.501442   \n",
              "\n",
              "            col_6       col_7       col_8       col_9  ...    col_1014  \\\n",
              "count  106.000000  106.000000  106.000000  106.000000  ...  106.000000   \n",
              "mean     0.003759    0.009692    0.820852    0.000341  ...    2.574852   \n",
              "std      0.001750    0.005831    0.880453    0.000449  ...    2.034847   \n",
              "min      0.000709    0.001137    0.000000    0.000000  ...    0.088338   \n",
              "25%      0.002526    0.005572    0.104984    0.000060  ...    1.263401   \n",
              "50%      0.003601    0.008399    0.482937    0.000162  ...    2.257724   \n",
              "75%      0.004792    0.012330    1.273323    0.000462  ...    3.237286   \n",
              "max      0.009723    0.031457    4.076707    0.002417  ...   14.442288   \n",
              "\n",
              "         col_1015    col_1016    col_1017    col_1018    col_1019    col_1020  \\\n",
              "count  106.000000  106.000000  106.000000  106.000000  106.000000  106.000000   \n",
              "mean     1.793822    1.863258    0.057073    1.563864    4.832652    0.472520   \n",
              "std      2.359615    1.540987    0.158559    1.730997    4.040923    0.708981   \n",
              "min      0.000000    0.000000    0.000000    0.000000    0.062468    0.000000   \n",
              "25%      0.238907    0.665238    0.000000    0.333118    1.718838    0.016078   \n",
              "50%      0.724648    1.526831    0.000387    0.868006    3.584293    0.149909   \n",
              "75%      2.402003    2.774607    0.047685    2.047931    6.655711    0.551298   \n",
              "max     12.272979    7.946446    1.157101    7.420815   18.426815    4.624882   \n",
              "\n",
              "         col_1021    col_1022    col_1023  \n",
              "count  106.000000  106.000000  106.000000  \n",
              "mean     7.973285    9.658977    2.003007  \n",
              "std      6.645405    6.318370    3.265117  \n",
              "min      0.127351    1.237949    0.000000  \n",
              "25%      2.953169    5.186445    0.185229  \n",
              "50%      6.123485    7.510555    0.755417  \n",
              "75%      9.690436   13.127022    2.107613  \n",
              "max     30.552481   28.587507   15.474661  \n",
              "\n",
              "[8 rows x 1024 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4553693e-9f1e-4e1b-8c0d-827e1b847bbf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col_0</th>\n",
              "      <th>col_1</th>\n",
              "      <th>col_2</th>\n",
              "      <th>col_3</th>\n",
              "      <th>col_4</th>\n",
              "      <th>col_5</th>\n",
              "      <th>col_6</th>\n",
              "      <th>col_7</th>\n",
              "      <th>col_8</th>\n",
              "      <th>col_9</th>\n",
              "      <th>...</th>\n",
              "      <th>col_1014</th>\n",
              "      <th>col_1015</th>\n",
              "      <th>col_1016</th>\n",
              "      <th>col_1017</th>\n",
              "      <th>col_1018</th>\n",
              "      <th>col_1019</th>\n",
              "      <th>col_1020</th>\n",
              "      <th>col_1021</th>\n",
              "      <th>col_1022</th>\n",
              "      <th>col_1023</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.000123</td>\n",
              "      <td>0.001596</td>\n",
              "      <td>0.013115</td>\n",
              "      <td>0.028490</td>\n",
              "      <td>0.008604</td>\n",
              "      <td>1.067306</td>\n",
              "      <td>0.003759</td>\n",
              "      <td>0.009692</td>\n",
              "      <td>0.820852</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>...</td>\n",
              "      <td>2.574852</td>\n",
              "      <td>1.793822</td>\n",
              "      <td>1.863258</td>\n",
              "      <td>0.057073</td>\n",
              "      <td>1.563864</td>\n",
              "      <td>4.832652</td>\n",
              "      <td>0.472520</td>\n",
              "      <td>7.973285</td>\n",
              "      <td>9.658977</td>\n",
              "      <td>2.003007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.000171</td>\n",
              "      <td>0.003280</td>\n",
              "      <td>0.014912</td>\n",
              "      <td>0.011983</td>\n",
              "      <td>0.020568</td>\n",
              "      <td>1.473539</td>\n",
              "      <td>0.001750</td>\n",
              "      <td>0.005831</td>\n",
              "      <td>0.880453</td>\n",
              "      <td>0.000449</td>\n",
              "      <td>...</td>\n",
              "      <td>2.034847</td>\n",
              "      <td>2.359615</td>\n",
              "      <td>1.540987</td>\n",
              "      <td>0.158559</td>\n",
              "      <td>1.730997</td>\n",
              "      <td>4.040923</td>\n",
              "      <td>0.708981</td>\n",
              "      <td>6.645405</td>\n",
              "      <td>6.318370</td>\n",
              "      <td>3.265117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006891</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000709</td>\n",
              "      <td>0.001137</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.088338</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062468</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.127351</td>\n",
              "      <td>1.237949</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.018879</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.068232</td>\n",
              "      <td>0.002526</td>\n",
              "      <td>0.005572</td>\n",
              "      <td>0.104984</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>...</td>\n",
              "      <td>1.263401</td>\n",
              "      <td>0.238907</td>\n",
              "      <td>0.665238</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333118</td>\n",
              "      <td>1.718838</td>\n",
              "      <td>0.016078</td>\n",
              "      <td>2.953169</td>\n",
              "      <td>5.186445</td>\n",
              "      <td>0.185229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000439</td>\n",
              "      <td>0.007047</td>\n",
              "      <td>0.027899</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.426330</td>\n",
              "      <td>0.003601</td>\n",
              "      <td>0.008399</td>\n",
              "      <td>0.482937</td>\n",
              "      <td>0.000162</td>\n",
              "      <td>...</td>\n",
              "      <td>2.257724</td>\n",
              "      <td>0.724648</td>\n",
              "      <td>1.526831</td>\n",
              "      <td>0.000387</td>\n",
              "      <td>0.868006</td>\n",
              "      <td>3.584293</td>\n",
              "      <td>0.149909</td>\n",
              "      <td>6.123485</td>\n",
              "      <td>7.510555</td>\n",
              "      <td>0.755417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.018837</td>\n",
              "      <td>0.036742</td>\n",
              "      <td>0.004838</td>\n",
              "      <td>1.536959</td>\n",
              "      <td>0.004792</td>\n",
              "      <td>0.012330</td>\n",
              "      <td>1.273323</td>\n",
              "      <td>0.000462</td>\n",
              "      <td>...</td>\n",
              "      <td>3.237286</td>\n",
              "      <td>2.402003</td>\n",
              "      <td>2.774607</td>\n",
              "      <td>0.047685</td>\n",
              "      <td>2.047931</td>\n",
              "      <td>6.655711</td>\n",
              "      <td>0.551298</td>\n",
              "      <td>9.690436</td>\n",
              "      <td>13.127022</td>\n",
              "      <td>2.107613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.001028</td>\n",
              "      <td>0.019375</td>\n",
              "      <td>0.058558</td>\n",
              "      <td>0.064134</td>\n",
              "      <td>0.119419</td>\n",
              "      <td>6.501442</td>\n",
              "      <td>0.009723</td>\n",
              "      <td>0.031457</td>\n",
              "      <td>4.076707</td>\n",
              "      <td>0.002417</td>\n",
              "      <td>...</td>\n",
              "      <td>14.442288</td>\n",
              "      <td>12.272979</td>\n",
              "      <td>7.946446</td>\n",
              "      <td>1.157101</td>\n",
              "      <td>7.420815</td>\n",
              "      <td>18.426815</td>\n",
              "      <td>4.624882</td>\n",
              "      <td>30.552481</td>\n",
              "      <td>28.587507</td>\n",
              "      <td>15.474661</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 1024 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4553693e-9f1e-4e1b-8c0d-827e1b847bbf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4553693e-9f1e-4e1b-8c0d-827e1b847bbf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4553693e-9f1e-4e1b-8c0d-827e1b847bbf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0c88e2bd-c770-4167-afb8-917a7ad78f66\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0c88e2bd-c770-4167-afb8-917a7ad78f66')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0c88e2bd-c770-4167-afb8-917a7ad78f66 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "di6OLUlQoOEr",
        "outputId": "b00c6af6-90e0-430e-b290-7facc8b6f517"
      },
      "id": "di6OLUlQoOEr",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            col_0     col_1     col_2     col_3     col_4     col_5     col_6  \\\n",
              "1    9.396902e-07  0.000333  0.006347  0.036504  0.000000  0.412569  0.005403   \n",
              "2    2.362571e-04  0.000000  0.000411  0.037396  0.053374  0.083630  0.001613   \n",
              "3    2.583982e-06  0.000495  0.005132  0.027404  0.005338  0.650659  0.002697   \n",
              "4    1.044141e-04  0.000000  0.006923  0.019871  0.051321  0.126890  0.002017   \n",
              "5    0.000000e+00  0.000499  0.001477  0.027336  0.006523  0.065832  0.004406   \n",
              "..            ...       ...       ...       ...       ...       ...       ...   \n",
              "102  2.858673e-04  0.000422  0.006531  0.022602  0.000000  1.175198  0.003814   \n",
              "103  2.909551e-05  0.000292  0.009650  0.025030  0.000000  0.874825  0.002984   \n",
              "104  7.770518e-05  0.001500  0.009023  0.016013  0.000000  6.501442  0.003794   \n",
              "105  2.152358e-04  0.001478  0.008229  0.029233  0.000000  0.310566  0.005019   \n",
              "106  1.462159e-04  0.000094  0.003205  0.018711  0.000000  0.075432  0.001312   \n",
              "\n",
              "        col_7     col_8     col_9  ...  col_1014   col_1015  col_1016  \\\n",
              "1    0.011643  0.689527  0.000072  ...  2.211190   1.830731  1.599084   \n",
              "2    0.012419  0.011209  0.000125  ...  4.289502  12.272979  1.061795   \n",
              "3    0.011865  0.469903  0.000580  ...  2.990084   1.965211  1.895124   \n",
              "4    0.004525  0.251325  0.000200  ...  0.840801   4.740310  1.045594   \n",
              "5    0.007106  0.000000  0.000153  ...  0.696185   5.060624  1.150050   \n",
              "..        ...       ...       ...  ...       ...        ...       ...   \n",
              "102  0.017166  0.217029  0.000102  ...  4.431988   0.719263  2.798100   \n",
              "103  0.026656  0.718126  0.000280  ...  2.590968   0.200574  3.127841   \n",
              "104  0.026377  0.268255  0.000352  ...  1.249410   0.140803  2.604059   \n",
              "105  0.010961  1.799433  0.000420  ...  4.212001   0.797767  4.924192   \n",
              "106  0.021138  0.885663  0.000009  ...  4.719266   0.158773  2.016609   \n",
              "\n",
              "     col_1017  col_1018   col_1019  col_1020  col_1021   col_1022  col_1023  \n",
              "1    0.000000  0.170072   4.629833  0.204918  2.117601  22.509760  0.000000  \n",
              "2    0.000000  5.703880  15.103234  0.127094  8.411349   8.961848  5.570603  \n",
              "3    0.000000  0.701533   5.503802  0.266022  1.436058   8.441154  0.866877  \n",
              "4    0.011223  4.561114  11.867761  0.000000  5.264866   9.659755  8.124499  \n",
              "5    0.000000  1.931380   5.611865  0.000000  2.602277   9.053319  5.621300  \n",
              "..        ...       ...        ...       ...       ...        ...       ...  \n",
              "102  0.000000  0.271902   2.649144  1.664901  6.068490  11.417628  0.963899  \n",
              "103  0.000202  0.453639   1.604295  0.160515  2.136951  17.379366  0.417633  \n",
              "104  0.000000  0.010276   0.509129  0.010101  7.449607  25.247751  0.126923  \n",
              "105  0.000000  0.723061   3.003130  1.330651  3.110856  13.298103  2.201562  \n",
              "106  0.000000  2.106893   4.555364  0.494679  3.708731  10.488916  0.566087  \n",
              "\n",
              "[106 rows x 1024 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-33c7959f-2d9e-4abd-8c73-584980881ef1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col_0</th>\n",
              "      <th>col_1</th>\n",
              "      <th>col_2</th>\n",
              "      <th>col_3</th>\n",
              "      <th>col_4</th>\n",
              "      <th>col_5</th>\n",
              "      <th>col_6</th>\n",
              "      <th>col_7</th>\n",
              "      <th>col_8</th>\n",
              "      <th>col_9</th>\n",
              "      <th>...</th>\n",
              "      <th>col_1014</th>\n",
              "      <th>col_1015</th>\n",
              "      <th>col_1016</th>\n",
              "      <th>col_1017</th>\n",
              "      <th>col_1018</th>\n",
              "      <th>col_1019</th>\n",
              "      <th>col_1020</th>\n",
              "      <th>col_1021</th>\n",
              "      <th>col_1022</th>\n",
              "      <th>col_1023</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.396902e-07</td>\n",
              "      <td>0.000333</td>\n",
              "      <td>0.006347</td>\n",
              "      <td>0.036504</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.412569</td>\n",
              "      <td>0.005403</td>\n",
              "      <td>0.011643</td>\n",
              "      <td>0.689527</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>...</td>\n",
              "      <td>2.211190</td>\n",
              "      <td>1.830731</td>\n",
              "      <td>1.599084</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.170072</td>\n",
              "      <td>4.629833</td>\n",
              "      <td>0.204918</td>\n",
              "      <td>2.117601</td>\n",
              "      <td>22.509760</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.362571e-04</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000411</td>\n",
              "      <td>0.037396</td>\n",
              "      <td>0.053374</td>\n",
              "      <td>0.083630</td>\n",
              "      <td>0.001613</td>\n",
              "      <td>0.012419</td>\n",
              "      <td>0.011209</td>\n",
              "      <td>0.000125</td>\n",
              "      <td>...</td>\n",
              "      <td>4.289502</td>\n",
              "      <td>12.272979</td>\n",
              "      <td>1.061795</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.703880</td>\n",
              "      <td>15.103234</td>\n",
              "      <td>0.127094</td>\n",
              "      <td>8.411349</td>\n",
              "      <td>8.961848</td>\n",
              "      <td>5.570603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.583982e-06</td>\n",
              "      <td>0.000495</td>\n",
              "      <td>0.005132</td>\n",
              "      <td>0.027404</td>\n",
              "      <td>0.005338</td>\n",
              "      <td>0.650659</td>\n",
              "      <td>0.002697</td>\n",
              "      <td>0.011865</td>\n",
              "      <td>0.469903</td>\n",
              "      <td>0.000580</td>\n",
              "      <td>...</td>\n",
              "      <td>2.990084</td>\n",
              "      <td>1.965211</td>\n",
              "      <td>1.895124</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.701533</td>\n",
              "      <td>5.503802</td>\n",
              "      <td>0.266022</td>\n",
              "      <td>1.436058</td>\n",
              "      <td>8.441154</td>\n",
              "      <td>0.866877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.044141e-04</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006923</td>\n",
              "      <td>0.019871</td>\n",
              "      <td>0.051321</td>\n",
              "      <td>0.126890</td>\n",
              "      <td>0.002017</td>\n",
              "      <td>0.004525</td>\n",
              "      <td>0.251325</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>...</td>\n",
              "      <td>0.840801</td>\n",
              "      <td>4.740310</td>\n",
              "      <td>1.045594</td>\n",
              "      <td>0.011223</td>\n",
              "      <td>4.561114</td>\n",
              "      <td>11.867761</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.264866</td>\n",
              "      <td>9.659755</td>\n",
              "      <td>8.124499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000499</td>\n",
              "      <td>0.001477</td>\n",
              "      <td>0.027336</td>\n",
              "      <td>0.006523</td>\n",
              "      <td>0.065832</td>\n",
              "      <td>0.004406</td>\n",
              "      <td>0.007106</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>...</td>\n",
              "      <td>0.696185</td>\n",
              "      <td>5.060624</td>\n",
              "      <td>1.150050</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.931380</td>\n",
              "      <td>5.611865</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.602277</td>\n",
              "      <td>9.053319</td>\n",
              "      <td>5.621300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>2.858673e-04</td>\n",
              "      <td>0.000422</td>\n",
              "      <td>0.006531</td>\n",
              "      <td>0.022602</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.175198</td>\n",
              "      <td>0.003814</td>\n",
              "      <td>0.017166</td>\n",
              "      <td>0.217029</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>...</td>\n",
              "      <td>4.431988</td>\n",
              "      <td>0.719263</td>\n",
              "      <td>2.798100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.271902</td>\n",
              "      <td>2.649144</td>\n",
              "      <td>1.664901</td>\n",
              "      <td>6.068490</td>\n",
              "      <td>11.417628</td>\n",
              "      <td>0.963899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>2.909551e-05</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.009650</td>\n",
              "      <td>0.025030</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.874825</td>\n",
              "      <td>0.002984</td>\n",
              "      <td>0.026656</td>\n",
              "      <td>0.718126</td>\n",
              "      <td>0.000280</td>\n",
              "      <td>...</td>\n",
              "      <td>2.590968</td>\n",
              "      <td>0.200574</td>\n",
              "      <td>3.127841</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.453639</td>\n",
              "      <td>1.604295</td>\n",
              "      <td>0.160515</td>\n",
              "      <td>2.136951</td>\n",
              "      <td>17.379366</td>\n",
              "      <td>0.417633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>7.770518e-05</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.009023</td>\n",
              "      <td>0.016013</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.501442</td>\n",
              "      <td>0.003794</td>\n",
              "      <td>0.026377</td>\n",
              "      <td>0.268255</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>...</td>\n",
              "      <td>1.249410</td>\n",
              "      <td>0.140803</td>\n",
              "      <td>2.604059</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010276</td>\n",
              "      <td>0.509129</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>7.449607</td>\n",
              "      <td>25.247751</td>\n",
              "      <td>0.126923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>2.152358e-04</td>\n",
              "      <td>0.001478</td>\n",
              "      <td>0.008229</td>\n",
              "      <td>0.029233</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.310566</td>\n",
              "      <td>0.005019</td>\n",
              "      <td>0.010961</td>\n",
              "      <td>1.799433</td>\n",
              "      <td>0.000420</td>\n",
              "      <td>...</td>\n",
              "      <td>4.212001</td>\n",
              "      <td>0.797767</td>\n",
              "      <td>4.924192</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.723061</td>\n",
              "      <td>3.003130</td>\n",
              "      <td>1.330651</td>\n",
              "      <td>3.110856</td>\n",
              "      <td>13.298103</td>\n",
              "      <td>2.201562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>1.462159e-04</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.003205</td>\n",
              "      <td>0.018711</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.075432</td>\n",
              "      <td>0.001312</td>\n",
              "      <td>0.021138</td>\n",
              "      <td>0.885663</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>...</td>\n",
              "      <td>4.719266</td>\n",
              "      <td>0.158773</td>\n",
              "      <td>2.016609</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.106893</td>\n",
              "      <td>4.555364</td>\n",
              "      <td>0.494679</td>\n",
              "      <td>3.708731</td>\n",
              "      <td>10.488916</td>\n",
              "      <td>0.566087</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>106 rows × 1024 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33c7959f-2d9e-4abd-8c73-584980881ef1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-33c7959f-2d9e-4abd-8c73-584980881ef1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-33c7959f-2d9e-4abd-8c73-584980881ef1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9a49cb94-0825-4c30-a43b-6349ad891445\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9a49cb94-0825-4c30-a43b-6349ad891445')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9a49cb94-0825-4c30-a43b-6349ad891445 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_2806fe00-d1b3-4a65-9589-2ec6b04b41a4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('Image_dataframe')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2806fe00-d1b3-4a65-9589-2ec6b04b41a4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('Image_dataframe');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "Image_dataframe"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "print(scaler.fit(dataframe))\n",
        "dataframe=scaler.transform(dataframe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te_hf-u8Ma0F",
        "outputId": "9967ec7f-52bc-4d5e-c933-5a7803fc33a6"
      },
      "id": "te_hf-u8Ma0F",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MinMaxScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTPnsguqqD8q",
        "outputId": "dc07ada9-094f-46c9-c85c-fcc6dcb7c7e8"
      },
      "id": "nTPnsguqqD8q",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00091414, 0.01716133, 0.10838489, ..., 0.06541469, 0.7777753 ,\n",
              "        0.        ],\n",
              "       [0.22983432, 0.        , 0.0070224 , ..., 0.2722749 , 0.28241405,\n",
              "        0.35998222],\n",
              "       [0.00251374, 0.02554736, 0.08763265, ..., 0.04301401, 0.26337555,\n",
              "        0.05601912],\n",
              "       ...,\n",
              "       [0.07559273, 0.07739867, 0.1540815 , ..., 0.24066477, 0.8778863 ,\n",
              "        0.008202  ],\n",
              "       [0.20938455, 0.07627997, 0.1405232 , ..., 0.09806056, 0.4409634 ,\n",
              "        0.14226884],\n",
              "       [0.14224097, 0.0048458 , 0.0547402 , ..., 0.11771126, 0.33824924,\n",
              "        0.03658157]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(dataframe).describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "cR-f3ghirDJK",
        "outputId": "53a4f52e-b3ec-4770-b571-42f9f270319b"
      },
      "id": "cR-f3ghirDJK",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0           1           2           3           4           5     \\\n",
              "count  106.000000  106.000000  106.000000  106.000000  106.000000  106.000000   \n",
              "mean     0.119986    0.082368    0.223971    0.377328    0.072051    0.164164   \n",
              "std      0.166267    0.169271    0.254652    0.209341    0.172236    0.226648   \n",
              "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              "25%      0.007480    0.000000    0.033354    0.209426    0.000000    0.010495   \n",
              "50%      0.063207    0.022675    0.120347    0.366997    0.000000    0.065575   \n",
              "75%      0.152269    0.078314    0.321681    0.521490    0.040514    0.236403   \n",
              "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
              "\n",
              "             6           7           8           9     ...        1014  \\\n",
              "count  106.000000  106.000000  106.000000  106.000000  ...  106.000000   \n",
              "mean     0.338313    0.282156    0.201352    0.141045  ...    0.173229   \n",
              "std      0.194116    0.192313    0.215972    0.185671  ...    0.141762   \n",
              "min      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
              "25%      0.201570    0.146269    0.025752    0.024873  ...    0.081863   \n",
              "50%      0.320854    0.239526    0.118462    0.066954  ...    0.151135   \n",
              "75%      0.452947    0.369175    0.312341    0.191228  ...    0.219379   \n",
              "max      1.000000    1.000000    1.000000    1.000000  ...    1.000000   \n",
              "\n",
              "             1015        1016        1017        1018        1019        1020  \\\n",
              "count  106.000000  106.000000  106.000000  106.000000  106.000000  106.000000   \n",
              "mean     0.146160    0.234477    0.049324    0.210740    0.259752    0.102169   \n",
              "std      0.192261    0.193922    0.137031    0.233262    0.220042    0.153297   \n",
              "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
              "25%      0.019466    0.083715    0.000000    0.044890    0.090195    0.003476   \n",
              "50%      0.059044    0.192140    0.000335    0.116969    0.191775    0.032414   \n",
              "75%      0.195715    0.349163    0.041211    0.275971    0.359024    0.119203   \n",
              "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
              "\n",
              "             1021        1022        1023  \n",
              "count  106.000000  106.000000  106.000000  \n",
              "mean     0.257877    0.307904    0.129438  \n",
              "std      0.218418    0.231023    0.210998  \n",
              "min      0.000000    0.000000    0.000000  \n",
              "25%      0.092878    0.144371    0.011970  \n",
              "50%      0.197078    0.229349    0.048816  \n",
              "75%      0.314315    0.434708    0.136198  \n",
              "max      1.000000    1.000000    1.000000  \n",
              "\n",
              "[8 rows x 1024 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88cf8bc0-23e4-49bc-a965-c7840067d76c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>1014</th>\n",
              "      <th>1015</th>\n",
              "      <th>1016</th>\n",
              "      <th>1017</th>\n",
              "      <th>1018</th>\n",
              "      <th>1019</th>\n",
              "      <th>1020</th>\n",
              "      <th>1021</th>\n",
              "      <th>1022</th>\n",
              "      <th>1023</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "      <td>106.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.119986</td>\n",
              "      <td>0.082368</td>\n",
              "      <td>0.223971</td>\n",
              "      <td>0.377328</td>\n",
              "      <td>0.072051</td>\n",
              "      <td>0.164164</td>\n",
              "      <td>0.338313</td>\n",
              "      <td>0.282156</td>\n",
              "      <td>0.201352</td>\n",
              "      <td>0.141045</td>\n",
              "      <td>...</td>\n",
              "      <td>0.173229</td>\n",
              "      <td>0.146160</td>\n",
              "      <td>0.234477</td>\n",
              "      <td>0.049324</td>\n",
              "      <td>0.210740</td>\n",
              "      <td>0.259752</td>\n",
              "      <td>0.102169</td>\n",
              "      <td>0.257877</td>\n",
              "      <td>0.307904</td>\n",
              "      <td>0.129438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.166267</td>\n",
              "      <td>0.169271</td>\n",
              "      <td>0.254652</td>\n",
              "      <td>0.209341</td>\n",
              "      <td>0.172236</td>\n",
              "      <td>0.226648</td>\n",
              "      <td>0.194116</td>\n",
              "      <td>0.192313</td>\n",
              "      <td>0.215972</td>\n",
              "      <td>0.185671</td>\n",
              "      <td>...</td>\n",
              "      <td>0.141762</td>\n",
              "      <td>0.192261</td>\n",
              "      <td>0.193922</td>\n",
              "      <td>0.137031</td>\n",
              "      <td>0.233262</td>\n",
              "      <td>0.220042</td>\n",
              "      <td>0.153297</td>\n",
              "      <td>0.218418</td>\n",
              "      <td>0.231023</td>\n",
              "      <td>0.210998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.007480</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033354</td>\n",
              "      <td>0.209426</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010495</td>\n",
              "      <td>0.201570</td>\n",
              "      <td>0.146269</td>\n",
              "      <td>0.025752</td>\n",
              "      <td>0.024873</td>\n",
              "      <td>...</td>\n",
              "      <td>0.081863</td>\n",
              "      <td>0.019466</td>\n",
              "      <td>0.083715</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.044890</td>\n",
              "      <td>0.090195</td>\n",
              "      <td>0.003476</td>\n",
              "      <td>0.092878</td>\n",
              "      <td>0.144371</td>\n",
              "      <td>0.011970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.063207</td>\n",
              "      <td>0.022675</td>\n",
              "      <td>0.120347</td>\n",
              "      <td>0.366997</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.065575</td>\n",
              "      <td>0.320854</td>\n",
              "      <td>0.239526</td>\n",
              "      <td>0.118462</td>\n",
              "      <td>0.066954</td>\n",
              "      <td>...</td>\n",
              "      <td>0.151135</td>\n",
              "      <td>0.059044</td>\n",
              "      <td>0.192140</td>\n",
              "      <td>0.000335</td>\n",
              "      <td>0.116969</td>\n",
              "      <td>0.191775</td>\n",
              "      <td>0.032414</td>\n",
              "      <td>0.197078</td>\n",
              "      <td>0.229349</td>\n",
              "      <td>0.048816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.152269</td>\n",
              "      <td>0.078314</td>\n",
              "      <td>0.321681</td>\n",
              "      <td>0.521490</td>\n",
              "      <td>0.040514</td>\n",
              "      <td>0.236403</td>\n",
              "      <td>0.452947</td>\n",
              "      <td>0.369175</td>\n",
              "      <td>0.312341</td>\n",
              "      <td>0.191228</td>\n",
              "      <td>...</td>\n",
              "      <td>0.219379</td>\n",
              "      <td>0.195715</td>\n",
              "      <td>0.349163</td>\n",
              "      <td>0.041211</td>\n",
              "      <td>0.275971</td>\n",
              "      <td>0.359024</td>\n",
              "      <td>0.119203</td>\n",
              "      <td>0.314315</td>\n",
              "      <td>0.434708</td>\n",
              "      <td>0.136198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 1024 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88cf8bc0-23e4-49bc-a965-c7840067d76c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-88cf8bc0-23e4-49bc-a965-c7840067d76c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-88cf8bc0-23e4-49bc-a965-c7840067d76c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ebb75eac-234d-4c0d-930f-f2e5fbeffb04\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ebb75eac-234d-4c0d-930f-f2e5fbeffb04')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ebb75eac-234d-4c0d-930f-f2e5fbeffb04 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a7e1cd46-f66c-49d8-8c8a-92bd382150b5",
      "metadata": {
        "id": "a7e1cd46-f66c-49d8-8c8a-92bd382150b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "e07471de-b970-4cbd-c0d9-f3a3a66c7664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.21045953 0.10083306 0.0723205  0.06009856 0.05921614 0.04222376\n",
            " 0.03097494 0.02443718 0.01945475 0.01797237 0.01754793 0.01436432\n",
            " 0.01355635 0.01220271 0.01177963 0.01104891 0.01029541 0.00951038\n",
            " 0.00899909 0.00854145 0.00813899 0.0079101  0.00747407 0.00701922\n",
            " 0.00678895 0.00637169 0.00616506 0.00582813 0.00575607 0.0055878\n",
            " 0.00543403 0.00537778 0.00520366 0.00490138 0.00486817 0.00476778\n",
            " 0.00449616 0.00431276 0.00424041 0.00413802 0.0040531  0.00389291\n",
            " 0.00380932 0.00374016 0.00365275 0.00347932 0.00346366 0.00334598\n",
            " 0.00327666 0.00318352 0.00308776 0.00303284 0.00295662 0.00288075\n",
            " 0.00280785 0.00269355 0.00262216 0.00260654 0.00254474 0.00252484\n",
            " 0.00237049 0.00234814 0.00228008 0.00225985 0.00222833 0.00220081\n",
            " 0.00213391 0.00206095 0.00201463 0.00195667 0.0019086  0.00188997\n",
            " 0.00180782 0.00174807 0.00173155 0.00167939 0.00167055 0.00164839\n",
            " 0.00158558 0.00151939 0.00148712 0.00144812 0.00141413 0.00137616\n",
            " 0.00134119 0.00130738 0.00126626 0.00124352 0.00119601 0.00117331\n",
            " 0.00116493 0.00110772 0.0010895  0.00107012]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=94)\n",
        "pca.fit(dataframe)\n",
        "\n",
        "# Get the principal components and explained variance ratio\n",
        "components = pca.components_\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "print(explained_variance_ratio)\n",
        "# Project the data onto the principal components\n",
        "transformed_data = pd.DataFrame(pca.transform(dataframe))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cumulative_variance_ratio = explained_variance_ratio.cumsum()\n",
        "\n",
        "# Find the number of components that cover 89% of the variance\n",
        "n_components_95_variance = (cumulative_variance_ratio < 0.99).sum() + 1"
      ],
      "metadata": {
        "id": "sYGm67SV5RDQ"
      },
      "id": "sYGm67SV5RDQ",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_components_95_variance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7oNcezD5Vkz",
        "outputId": "8f03c86d-8d3e-4ecc-fafc-f01c20f3b52d"
      },
      "id": "b7oNcezD5Vkz",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "94"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_data.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "s-KC-PEcrQ6A",
        "outputId": "893d6aa5-00d4-41e4-eca2-b0b6735511d0"
      },
      "id": "s-KC-PEcrQ6A",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 0             1             2             3             4   \\\n",
              "count  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02   \n",
              "mean  -3.058955e-07 -1.439508e-07 -4.498464e-08  1.180847e-08  2.215493e-07   \n",
              "std    2.926343e+00  2.025548e+00  1.715425e+00  1.563771e+00  1.552248e+00   \n",
              "min   -4.014367e+00 -3.093603e+00 -4.107776e+00 -3.346595e+00 -3.367878e+00   \n",
              "25%   -1.723334e+00 -1.306381e+00 -1.211402e+00 -1.074956e+00 -1.143476e+00   \n",
              "50%   -5.665178e-01 -2.861803e-01  2.567888e-01 -3.522820e-01 -3.474005e-01   \n",
              "75%    4.666237e-01  7.602456e-01  1.180015e+00  1.120254e+00  7.643524e-01   \n",
              "max    9.025208e+00  6.278797e+00  4.384637e+00  3.212867e+00  4.699007e+00   \n",
              "\n",
              "                 5             6             7             8             9   \\\n",
              "count  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02   \n",
              "mean   3.936156e-08  2.642848e-08 -2.204247e-07 -2.654094e-07  2.069293e-07   \n",
              "std    1.310749e+00  1.122654e+00  9.971640e-01  8.897214e-01  8.551528e-01   \n",
              "min   -4.044148e+00 -2.566602e+00 -2.671555e+00 -2.667445e+00 -2.021720e+00   \n",
              "25%   -7.126259e-01 -8.341737e-01 -7.620839e-01 -5.876057e-01 -4.443687e-01   \n",
              "50%   -3.080714e-02 -1.369742e-02 -2.285057e-02 -2.291334e-02 -4.000890e-02   \n",
              "75%    7.919617e-01  5.364352e-01  5.710053e-01  3.321130e-01  3.686893e-01   \n",
              "max    3.282794e+00  2.740792e+00  2.124160e+00  2.745443e+00  2.651770e+00   \n",
              "\n",
              "       ...            84            85            86            87  \\\n",
              "count  ...  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02   \n",
              "mean   ... -9.144534e-08  3.275444e-08  7.703619e-08 -9.137504e-09   \n",
              "std    ...  2.336073e-01  2.306442e-01  2.269877e-01  2.249403e-01   \n",
              "min    ... -6.761023e-01 -9.508160e-01 -5.759639e-01 -6.592889e-01   \n",
              "25%    ... -9.926789e-02 -1.191009e-01 -1.550610e-01 -1.102020e-01   \n",
              "50%    ...  2.746347e-03  7.630020e-03 -2.648043e-02  9.784035e-03   \n",
              "75%    ...  1.560046e-01  1.341316e-01  1.485926e-01  1.060697e-01   \n",
              "max    ...  5.676768e-01  7.479881e-01  7.837525e-01  5.886874e-01   \n",
              "\n",
              "                 88            89            90            91            92  \\\n",
              "count  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02   \n",
              "mean  -4.695271e-08  1.715039e-08 -4.779618e-09  2.890263e-07 -8.490851e-08   \n",
              "std    2.206017e-01  2.184982e-01  2.177163e-01  2.123025e-01  2.105501e-01   \n",
              "min   -7.345520e-01 -6.942117e-01 -5.217301e-01 -6.634886e-01 -6.408812e-01   \n",
              "25%   -1.187994e-01 -8.552986e-02 -1.393867e-01 -1.171828e-01 -7.715099e-02   \n",
              "50%   -1.497912e-02 -1.217006e-02  7.174924e-03 -6.122857e-03 -1.886236e-03   \n",
              "75%    1.048423e-01  1.038653e-01  1.289528e-01  1.011102e-01  1.092009e-01   \n",
              "max    6.539001e-01  7.707458e-01  6.555741e-01  7.675595e-01  5.632073e-01   \n",
              "\n",
              "                 93  \n",
              "count  1.060000e+02  \n",
              "mean   4.918789e-07  \n",
              "std    2.086684e-01  \n",
              "min   -6.918372e-01  \n",
              "25%   -1.297556e-01  \n",
              "50%   -2.155318e-02  \n",
              "75%    1.204874e-01  \n",
              "max    5.784544e-01  \n",
              "\n",
              "[8 rows x 94 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-47dc8cb1-c5cf-40a9-a01d-5f0c77ed92b2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-3.058955e-07</td>\n",
              "      <td>-1.439508e-07</td>\n",
              "      <td>-4.498464e-08</td>\n",
              "      <td>1.180847e-08</td>\n",
              "      <td>2.215493e-07</td>\n",
              "      <td>3.936156e-08</td>\n",
              "      <td>2.642848e-08</td>\n",
              "      <td>-2.204247e-07</td>\n",
              "      <td>-2.654094e-07</td>\n",
              "      <td>2.069293e-07</td>\n",
              "      <td>...</td>\n",
              "      <td>-9.144534e-08</td>\n",
              "      <td>3.275444e-08</td>\n",
              "      <td>7.703619e-08</td>\n",
              "      <td>-9.137504e-09</td>\n",
              "      <td>-4.695271e-08</td>\n",
              "      <td>1.715039e-08</td>\n",
              "      <td>-4.779618e-09</td>\n",
              "      <td>2.890263e-07</td>\n",
              "      <td>-8.490851e-08</td>\n",
              "      <td>4.918789e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.926343e+00</td>\n",
              "      <td>2.025548e+00</td>\n",
              "      <td>1.715425e+00</td>\n",
              "      <td>1.563771e+00</td>\n",
              "      <td>1.552248e+00</td>\n",
              "      <td>1.310749e+00</td>\n",
              "      <td>1.122654e+00</td>\n",
              "      <td>9.971640e-01</td>\n",
              "      <td>8.897214e-01</td>\n",
              "      <td>8.551528e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>2.336073e-01</td>\n",
              "      <td>2.306442e-01</td>\n",
              "      <td>2.269877e-01</td>\n",
              "      <td>2.249403e-01</td>\n",
              "      <td>2.206017e-01</td>\n",
              "      <td>2.184982e-01</td>\n",
              "      <td>2.177163e-01</td>\n",
              "      <td>2.123025e-01</td>\n",
              "      <td>2.105501e-01</td>\n",
              "      <td>2.086684e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-4.014367e+00</td>\n",
              "      <td>-3.093603e+00</td>\n",
              "      <td>-4.107776e+00</td>\n",
              "      <td>-3.346595e+00</td>\n",
              "      <td>-3.367878e+00</td>\n",
              "      <td>-4.044148e+00</td>\n",
              "      <td>-2.566602e+00</td>\n",
              "      <td>-2.671555e+00</td>\n",
              "      <td>-2.667445e+00</td>\n",
              "      <td>-2.021720e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>-6.761023e-01</td>\n",
              "      <td>-9.508160e-01</td>\n",
              "      <td>-5.759639e-01</td>\n",
              "      <td>-6.592889e-01</td>\n",
              "      <td>-7.345520e-01</td>\n",
              "      <td>-6.942117e-01</td>\n",
              "      <td>-5.217301e-01</td>\n",
              "      <td>-6.634886e-01</td>\n",
              "      <td>-6.408812e-01</td>\n",
              "      <td>-6.918372e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-1.723334e+00</td>\n",
              "      <td>-1.306381e+00</td>\n",
              "      <td>-1.211402e+00</td>\n",
              "      <td>-1.074956e+00</td>\n",
              "      <td>-1.143476e+00</td>\n",
              "      <td>-7.126259e-01</td>\n",
              "      <td>-8.341737e-01</td>\n",
              "      <td>-7.620839e-01</td>\n",
              "      <td>-5.876057e-01</td>\n",
              "      <td>-4.443687e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>-9.926789e-02</td>\n",
              "      <td>-1.191009e-01</td>\n",
              "      <td>-1.550610e-01</td>\n",
              "      <td>-1.102020e-01</td>\n",
              "      <td>-1.187994e-01</td>\n",
              "      <td>-8.552986e-02</td>\n",
              "      <td>-1.393867e-01</td>\n",
              "      <td>-1.171828e-01</td>\n",
              "      <td>-7.715099e-02</td>\n",
              "      <td>-1.297556e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-5.665178e-01</td>\n",
              "      <td>-2.861803e-01</td>\n",
              "      <td>2.567888e-01</td>\n",
              "      <td>-3.522820e-01</td>\n",
              "      <td>-3.474005e-01</td>\n",
              "      <td>-3.080714e-02</td>\n",
              "      <td>-1.369742e-02</td>\n",
              "      <td>-2.285057e-02</td>\n",
              "      <td>-2.291334e-02</td>\n",
              "      <td>-4.000890e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>2.746347e-03</td>\n",
              "      <td>7.630020e-03</td>\n",
              "      <td>-2.648043e-02</td>\n",
              "      <td>9.784035e-03</td>\n",
              "      <td>-1.497912e-02</td>\n",
              "      <td>-1.217006e-02</td>\n",
              "      <td>7.174924e-03</td>\n",
              "      <td>-6.122857e-03</td>\n",
              "      <td>-1.886236e-03</td>\n",
              "      <td>-2.155318e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>4.666237e-01</td>\n",
              "      <td>7.602456e-01</td>\n",
              "      <td>1.180015e+00</td>\n",
              "      <td>1.120254e+00</td>\n",
              "      <td>7.643524e-01</td>\n",
              "      <td>7.919617e-01</td>\n",
              "      <td>5.364352e-01</td>\n",
              "      <td>5.710053e-01</td>\n",
              "      <td>3.321130e-01</td>\n",
              "      <td>3.686893e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>1.560046e-01</td>\n",
              "      <td>1.341316e-01</td>\n",
              "      <td>1.485926e-01</td>\n",
              "      <td>1.060697e-01</td>\n",
              "      <td>1.048423e-01</td>\n",
              "      <td>1.038653e-01</td>\n",
              "      <td>1.289528e-01</td>\n",
              "      <td>1.011102e-01</td>\n",
              "      <td>1.092009e-01</td>\n",
              "      <td>1.204874e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.025208e+00</td>\n",
              "      <td>6.278797e+00</td>\n",
              "      <td>4.384637e+00</td>\n",
              "      <td>3.212867e+00</td>\n",
              "      <td>4.699007e+00</td>\n",
              "      <td>3.282794e+00</td>\n",
              "      <td>2.740792e+00</td>\n",
              "      <td>2.124160e+00</td>\n",
              "      <td>2.745443e+00</td>\n",
              "      <td>2.651770e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>5.676768e-01</td>\n",
              "      <td>7.479881e-01</td>\n",
              "      <td>7.837525e-01</td>\n",
              "      <td>5.886874e-01</td>\n",
              "      <td>6.539001e-01</td>\n",
              "      <td>7.707458e-01</td>\n",
              "      <td>6.555741e-01</td>\n",
              "      <td>7.675595e-01</td>\n",
              "      <td>5.632073e-01</td>\n",
              "      <td>5.784544e-01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 94 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47dc8cb1-c5cf-40a9-a01d-5f0c77ed92b2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-47dc8cb1-c5cf-40a9-a01d-5f0c77ed92b2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-47dc8cb1-c5cf-40a9-a01d-5f0c77ed92b2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-38d782b7-e250-4e75-8be6-ea606d5c3f82\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-38d782b7-e250-4e75-8be6-ea606d5c3f82')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-38d782b7-e250-4e75-8be6-ea606d5c3f82 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_data= pd.DataFrame(transformed_data)\n",
        "transformed_data.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "e9b-VpTQwQm0",
        "outputId": "e601028e-f184-4284-d820-cd797da8279e"
      },
      "id": "e9b-VpTQwQm0",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 0             1             2             3             4   \\\n",
              "count  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02   \n",
              "mean  -3.058955e-07 -1.439508e-07 -4.498464e-08  1.180847e-08  2.215493e-07   \n",
              "std    2.926343e+00  2.025548e+00  1.715425e+00  1.563771e+00  1.552248e+00   \n",
              "min   -4.014367e+00 -3.093603e+00 -4.107776e+00 -3.346595e+00 -3.367878e+00   \n",
              "25%   -1.723334e+00 -1.306381e+00 -1.211402e+00 -1.074956e+00 -1.143476e+00   \n",
              "50%   -5.665178e-01 -2.861803e-01  2.567888e-01 -3.522820e-01 -3.474005e-01   \n",
              "75%    4.666237e-01  7.602456e-01  1.180015e+00  1.120254e+00  7.643524e-01   \n",
              "max    9.025208e+00  6.278797e+00  4.384637e+00  3.212867e+00  4.699007e+00   \n",
              "\n",
              "                 5             6             7             8             9   \\\n",
              "count  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02   \n",
              "mean   3.936156e-08  2.642848e-08 -2.204247e-07 -2.654094e-07  2.069293e-07   \n",
              "std    1.310749e+00  1.122654e+00  9.971640e-01  8.897214e-01  8.551528e-01   \n",
              "min   -4.044148e+00 -2.566602e+00 -2.671555e+00 -2.667445e+00 -2.021720e+00   \n",
              "25%   -7.126259e-01 -8.341737e-01 -7.620839e-01 -5.876057e-01 -4.443687e-01   \n",
              "50%   -3.080714e-02 -1.369742e-02 -2.285057e-02 -2.291334e-02 -4.000890e-02   \n",
              "75%    7.919617e-01  5.364352e-01  5.710053e-01  3.321130e-01  3.686893e-01   \n",
              "max    3.282794e+00  2.740792e+00  2.124160e+00  2.745443e+00  2.651770e+00   \n",
              "\n",
              "       ...            84            85            86            87  \\\n",
              "count  ...  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02   \n",
              "mean   ... -9.144534e-08  3.275444e-08  7.703619e-08 -9.137504e-09   \n",
              "std    ...  2.336073e-01  2.306442e-01  2.269877e-01  2.249403e-01   \n",
              "min    ... -6.761023e-01 -9.508160e-01 -5.759639e-01 -6.592889e-01   \n",
              "25%    ... -9.926789e-02 -1.191009e-01 -1.550610e-01 -1.102020e-01   \n",
              "50%    ...  2.746347e-03  7.630020e-03 -2.648043e-02  9.784035e-03   \n",
              "75%    ...  1.560046e-01  1.341316e-01  1.485926e-01  1.060697e-01   \n",
              "max    ...  5.676768e-01  7.479881e-01  7.837525e-01  5.886874e-01   \n",
              "\n",
              "                 88            89            90            91            92  \\\n",
              "count  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02   \n",
              "mean  -4.695271e-08  1.715039e-08 -4.779618e-09  2.890263e-07 -8.490851e-08   \n",
              "std    2.206017e-01  2.184982e-01  2.177163e-01  2.123025e-01  2.105501e-01   \n",
              "min   -7.345520e-01 -6.942117e-01 -5.217301e-01 -6.634886e-01 -6.408812e-01   \n",
              "25%   -1.187994e-01 -8.552986e-02 -1.393867e-01 -1.171828e-01 -7.715099e-02   \n",
              "50%   -1.497912e-02 -1.217006e-02  7.174924e-03 -6.122857e-03 -1.886236e-03   \n",
              "75%    1.048423e-01  1.038653e-01  1.289528e-01  1.011102e-01  1.092009e-01   \n",
              "max    6.539001e-01  7.707458e-01  6.555741e-01  7.675595e-01  5.632073e-01   \n",
              "\n",
              "                 93  \n",
              "count  1.060000e+02  \n",
              "mean   4.918789e-07  \n",
              "std    2.086684e-01  \n",
              "min   -6.918372e-01  \n",
              "25%   -1.297556e-01  \n",
              "50%   -2.155318e-02  \n",
              "75%    1.204874e-01  \n",
              "max    5.784544e-01  \n",
              "\n",
              "[8 rows x 94 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f08c626d-0d67-4285-8305-a08c5c28e7d1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-3.058955e-07</td>\n",
              "      <td>-1.439508e-07</td>\n",
              "      <td>-4.498464e-08</td>\n",
              "      <td>1.180847e-08</td>\n",
              "      <td>2.215493e-07</td>\n",
              "      <td>3.936156e-08</td>\n",
              "      <td>2.642848e-08</td>\n",
              "      <td>-2.204247e-07</td>\n",
              "      <td>-2.654094e-07</td>\n",
              "      <td>2.069293e-07</td>\n",
              "      <td>...</td>\n",
              "      <td>-9.144534e-08</td>\n",
              "      <td>3.275444e-08</td>\n",
              "      <td>7.703619e-08</td>\n",
              "      <td>-9.137504e-09</td>\n",
              "      <td>-4.695271e-08</td>\n",
              "      <td>1.715039e-08</td>\n",
              "      <td>-4.779618e-09</td>\n",
              "      <td>2.890263e-07</td>\n",
              "      <td>-8.490851e-08</td>\n",
              "      <td>4.918789e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.926343e+00</td>\n",
              "      <td>2.025548e+00</td>\n",
              "      <td>1.715425e+00</td>\n",
              "      <td>1.563771e+00</td>\n",
              "      <td>1.552248e+00</td>\n",
              "      <td>1.310749e+00</td>\n",
              "      <td>1.122654e+00</td>\n",
              "      <td>9.971640e-01</td>\n",
              "      <td>8.897214e-01</td>\n",
              "      <td>8.551528e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>2.336073e-01</td>\n",
              "      <td>2.306442e-01</td>\n",
              "      <td>2.269877e-01</td>\n",
              "      <td>2.249403e-01</td>\n",
              "      <td>2.206017e-01</td>\n",
              "      <td>2.184982e-01</td>\n",
              "      <td>2.177163e-01</td>\n",
              "      <td>2.123025e-01</td>\n",
              "      <td>2.105501e-01</td>\n",
              "      <td>2.086684e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-4.014367e+00</td>\n",
              "      <td>-3.093603e+00</td>\n",
              "      <td>-4.107776e+00</td>\n",
              "      <td>-3.346595e+00</td>\n",
              "      <td>-3.367878e+00</td>\n",
              "      <td>-4.044148e+00</td>\n",
              "      <td>-2.566602e+00</td>\n",
              "      <td>-2.671555e+00</td>\n",
              "      <td>-2.667445e+00</td>\n",
              "      <td>-2.021720e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>-6.761023e-01</td>\n",
              "      <td>-9.508160e-01</td>\n",
              "      <td>-5.759639e-01</td>\n",
              "      <td>-6.592889e-01</td>\n",
              "      <td>-7.345520e-01</td>\n",
              "      <td>-6.942117e-01</td>\n",
              "      <td>-5.217301e-01</td>\n",
              "      <td>-6.634886e-01</td>\n",
              "      <td>-6.408812e-01</td>\n",
              "      <td>-6.918372e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-1.723334e+00</td>\n",
              "      <td>-1.306381e+00</td>\n",
              "      <td>-1.211402e+00</td>\n",
              "      <td>-1.074956e+00</td>\n",
              "      <td>-1.143476e+00</td>\n",
              "      <td>-7.126259e-01</td>\n",
              "      <td>-8.341737e-01</td>\n",
              "      <td>-7.620839e-01</td>\n",
              "      <td>-5.876057e-01</td>\n",
              "      <td>-4.443687e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>-9.926789e-02</td>\n",
              "      <td>-1.191009e-01</td>\n",
              "      <td>-1.550610e-01</td>\n",
              "      <td>-1.102020e-01</td>\n",
              "      <td>-1.187994e-01</td>\n",
              "      <td>-8.552986e-02</td>\n",
              "      <td>-1.393867e-01</td>\n",
              "      <td>-1.171828e-01</td>\n",
              "      <td>-7.715099e-02</td>\n",
              "      <td>-1.297556e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-5.665178e-01</td>\n",
              "      <td>-2.861803e-01</td>\n",
              "      <td>2.567888e-01</td>\n",
              "      <td>-3.522820e-01</td>\n",
              "      <td>-3.474005e-01</td>\n",
              "      <td>-3.080714e-02</td>\n",
              "      <td>-1.369742e-02</td>\n",
              "      <td>-2.285057e-02</td>\n",
              "      <td>-2.291334e-02</td>\n",
              "      <td>-4.000890e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>2.746347e-03</td>\n",
              "      <td>7.630020e-03</td>\n",
              "      <td>-2.648043e-02</td>\n",
              "      <td>9.784035e-03</td>\n",
              "      <td>-1.497912e-02</td>\n",
              "      <td>-1.217006e-02</td>\n",
              "      <td>7.174924e-03</td>\n",
              "      <td>-6.122857e-03</td>\n",
              "      <td>-1.886236e-03</td>\n",
              "      <td>-2.155318e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>4.666237e-01</td>\n",
              "      <td>7.602456e-01</td>\n",
              "      <td>1.180015e+00</td>\n",
              "      <td>1.120254e+00</td>\n",
              "      <td>7.643524e-01</td>\n",
              "      <td>7.919617e-01</td>\n",
              "      <td>5.364352e-01</td>\n",
              "      <td>5.710053e-01</td>\n",
              "      <td>3.321130e-01</td>\n",
              "      <td>3.686893e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>1.560046e-01</td>\n",
              "      <td>1.341316e-01</td>\n",
              "      <td>1.485926e-01</td>\n",
              "      <td>1.060697e-01</td>\n",
              "      <td>1.048423e-01</td>\n",
              "      <td>1.038653e-01</td>\n",
              "      <td>1.289528e-01</td>\n",
              "      <td>1.011102e-01</td>\n",
              "      <td>1.092009e-01</td>\n",
              "      <td>1.204874e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.025208e+00</td>\n",
              "      <td>6.278797e+00</td>\n",
              "      <td>4.384637e+00</td>\n",
              "      <td>3.212867e+00</td>\n",
              "      <td>4.699007e+00</td>\n",
              "      <td>3.282794e+00</td>\n",
              "      <td>2.740792e+00</td>\n",
              "      <td>2.124160e+00</td>\n",
              "      <td>2.745443e+00</td>\n",
              "      <td>2.651770e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>5.676768e-01</td>\n",
              "      <td>7.479881e-01</td>\n",
              "      <td>7.837525e-01</td>\n",
              "      <td>5.886874e-01</td>\n",
              "      <td>6.539001e-01</td>\n",
              "      <td>7.707458e-01</td>\n",
              "      <td>6.555741e-01</td>\n",
              "      <td>7.675595e-01</td>\n",
              "      <td>5.632073e-01</td>\n",
              "      <td>5.784544e-01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 94 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f08c626d-0d67-4285-8305-a08c5c28e7d1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f08c626d-0d67-4285-8305-a08c5c28e7d1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f08c626d-0d67-4285-8305-a08c5c28e7d1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c9f7cba1-e47a-4fae-8bd5-3bd8bc71b941\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c9f7cba1-e47a-4fae-8bd5-3bd8bc71b941')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c9f7cba1-e47a-4fae-8bd5-3bd8bc71b941 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "transformed_data.to_csv('Image_features_Densenet.csv')\n",
        "FileLink('Image_features_Densenet.csv')"
      ],
      "metadata": {
        "id": "3W61LtX_AxQe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe7c667d-c874-4790-becd-33b6da909546"
      },
      "id": "3W61LtX_AxQe",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/Image_features_Densenet.csv"
            ],
            "text/html": [
              "<a href='Image_features_Densenet.csv' target='_blank'>Image_features_Densenet.csv</a><br>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "73ce0479-4669-462f-bc90-1dd79dbb31d6",
      "metadata": {
        "id": "73ce0479-4669-462f-bc90-1dd79dbb31d6"
      },
      "outputs": [],
      "source": [
        "X_train=transformed_data.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "fb3ce829-933d-4e1e-9389-0c895b18f872",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb3ce829-933d-4e1e-9389-0c895b18f872",
        "outputId": "8f495028-22ca-4f6e-81cf-7f928b0a3436"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(106, 94)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "transformed_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "de48ad9b-39dd-462b-bb0a-57bedcc0f9d8",
      "metadata": {
        "id": "de48ad9b-39dd-462b-bb0a-57bedcc0f9d8",
        "outputId": "c59df70b-2980-4fdb-9d90-eb8d6fa55c41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 0             1             2             3             4   \\\n",
              "count  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02   \n",
              "mean  -3.058955e-07 -1.439508e-07 -4.498464e-08  1.180847e-08  2.215493e-07   \n",
              "std    2.926343e+00  2.025548e+00  1.715425e+00  1.563771e+00  1.552248e+00   \n",
              "min   -4.014367e+00 -3.093603e+00 -4.107776e+00 -3.346595e+00 -3.367878e+00   \n",
              "25%   -1.723334e+00 -1.306381e+00 -1.211402e+00 -1.074956e+00 -1.143476e+00   \n",
              "50%   -5.665178e-01 -2.861803e-01  2.567888e-01 -3.522820e-01 -3.474005e-01   \n",
              "75%    4.666237e-01  7.602456e-01  1.180015e+00  1.120254e+00  7.643524e-01   \n",
              "max    9.025208e+00  6.278797e+00  4.384637e+00  3.212867e+00  4.699007e+00   \n",
              "\n",
              "                 5             6             7             8             9   \\\n",
              "count  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02   \n",
              "mean   3.936156e-08  2.642848e-08 -2.204247e-07 -2.654094e-07  2.069293e-07   \n",
              "std    1.310749e+00  1.122654e+00  9.971640e-01  8.897214e-01  8.551528e-01   \n",
              "min   -4.044148e+00 -2.566602e+00 -2.671555e+00 -2.667445e+00 -2.021720e+00   \n",
              "25%   -7.126259e-01 -8.341737e-01 -7.620839e-01 -5.876057e-01 -4.443687e-01   \n",
              "50%   -3.080714e-02 -1.369742e-02 -2.285057e-02 -2.291334e-02 -4.000890e-02   \n",
              "75%    7.919617e-01  5.364352e-01  5.710053e-01  3.321130e-01  3.686893e-01   \n",
              "max    3.282794e+00  2.740792e+00  2.124160e+00  2.745443e+00  2.651770e+00   \n",
              "\n",
              "       ...            84            85            86            87  \\\n",
              "count  ...  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02   \n",
              "mean   ... -9.144534e-08  3.275444e-08  7.703619e-08 -9.137504e-09   \n",
              "std    ...  2.336073e-01  2.306442e-01  2.269877e-01  2.249403e-01   \n",
              "min    ... -6.761023e-01 -9.508160e-01 -5.759639e-01 -6.592889e-01   \n",
              "25%    ... -9.926789e-02 -1.191009e-01 -1.550610e-01 -1.102020e-01   \n",
              "50%    ...  2.746347e-03  7.630020e-03 -2.648043e-02  9.784035e-03   \n",
              "75%    ...  1.560046e-01  1.341316e-01  1.485926e-01  1.060697e-01   \n",
              "max    ...  5.676768e-01  7.479881e-01  7.837525e-01  5.886874e-01   \n",
              "\n",
              "                 88            89            90            91            92  \\\n",
              "count  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02  1.060000e+02   \n",
              "mean  -4.695271e-08  1.715039e-08 -4.779618e-09  2.890263e-07 -8.490851e-08   \n",
              "std    2.206017e-01  2.184982e-01  2.177163e-01  2.123025e-01  2.105501e-01   \n",
              "min   -7.345520e-01 -6.942117e-01 -5.217301e-01 -6.634886e-01 -6.408812e-01   \n",
              "25%   -1.187994e-01 -8.552986e-02 -1.393867e-01 -1.171828e-01 -7.715099e-02   \n",
              "50%   -1.497912e-02 -1.217006e-02  7.174924e-03 -6.122857e-03 -1.886236e-03   \n",
              "75%    1.048423e-01  1.038653e-01  1.289528e-01  1.011102e-01  1.092009e-01   \n",
              "max    6.539001e-01  7.707458e-01  6.555741e-01  7.675595e-01  5.632073e-01   \n",
              "\n",
              "                 93  \n",
              "count  1.060000e+02  \n",
              "mean   4.918789e-07  \n",
              "std    2.086684e-01  \n",
              "min   -6.918372e-01  \n",
              "25%   -1.297556e-01  \n",
              "50%   -2.155318e-02  \n",
              "75%    1.204874e-01  \n",
              "max    5.784544e-01  \n",
              "\n",
              "[8 rows x 94 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7fe858ef-80cd-45c9-8063-31c440dc099e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "      <td>1.060000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-3.058955e-07</td>\n",
              "      <td>-1.439508e-07</td>\n",
              "      <td>-4.498464e-08</td>\n",
              "      <td>1.180847e-08</td>\n",
              "      <td>2.215493e-07</td>\n",
              "      <td>3.936156e-08</td>\n",
              "      <td>2.642848e-08</td>\n",
              "      <td>-2.204247e-07</td>\n",
              "      <td>-2.654094e-07</td>\n",
              "      <td>2.069293e-07</td>\n",
              "      <td>...</td>\n",
              "      <td>-9.144534e-08</td>\n",
              "      <td>3.275444e-08</td>\n",
              "      <td>7.703619e-08</td>\n",
              "      <td>-9.137504e-09</td>\n",
              "      <td>-4.695271e-08</td>\n",
              "      <td>1.715039e-08</td>\n",
              "      <td>-4.779618e-09</td>\n",
              "      <td>2.890263e-07</td>\n",
              "      <td>-8.490851e-08</td>\n",
              "      <td>4.918789e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.926343e+00</td>\n",
              "      <td>2.025548e+00</td>\n",
              "      <td>1.715425e+00</td>\n",
              "      <td>1.563771e+00</td>\n",
              "      <td>1.552248e+00</td>\n",
              "      <td>1.310749e+00</td>\n",
              "      <td>1.122654e+00</td>\n",
              "      <td>9.971640e-01</td>\n",
              "      <td>8.897214e-01</td>\n",
              "      <td>8.551528e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>2.336073e-01</td>\n",
              "      <td>2.306442e-01</td>\n",
              "      <td>2.269877e-01</td>\n",
              "      <td>2.249403e-01</td>\n",
              "      <td>2.206017e-01</td>\n",
              "      <td>2.184982e-01</td>\n",
              "      <td>2.177163e-01</td>\n",
              "      <td>2.123025e-01</td>\n",
              "      <td>2.105501e-01</td>\n",
              "      <td>2.086684e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-4.014367e+00</td>\n",
              "      <td>-3.093603e+00</td>\n",
              "      <td>-4.107776e+00</td>\n",
              "      <td>-3.346595e+00</td>\n",
              "      <td>-3.367878e+00</td>\n",
              "      <td>-4.044148e+00</td>\n",
              "      <td>-2.566602e+00</td>\n",
              "      <td>-2.671555e+00</td>\n",
              "      <td>-2.667445e+00</td>\n",
              "      <td>-2.021720e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>-6.761023e-01</td>\n",
              "      <td>-9.508160e-01</td>\n",
              "      <td>-5.759639e-01</td>\n",
              "      <td>-6.592889e-01</td>\n",
              "      <td>-7.345520e-01</td>\n",
              "      <td>-6.942117e-01</td>\n",
              "      <td>-5.217301e-01</td>\n",
              "      <td>-6.634886e-01</td>\n",
              "      <td>-6.408812e-01</td>\n",
              "      <td>-6.918372e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-1.723334e+00</td>\n",
              "      <td>-1.306381e+00</td>\n",
              "      <td>-1.211402e+00</td>\n",
              "      <td>-1.074956e+00</td>\n",
              "      <td>-1.143476e+00</td>\n",
              "      <td>-7.126259e-01</td>\n",
              "      <td>-8.341737e-01</td>\n",
              "      <td>-7.620839e-01</td>\n",
              "      <td>-5.876057e-01</td>\n",
              "      <td>-4.443687e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>-9.926789e-02</td>\n",
              "      <td>-1.191009e-01</td>\n",
              "      <td>-1.550610e-01</td>\n",
              "      <td>-1.102020e-01</td>\n",
              "      <td>-1.187994e-01</td>\n",
              "      <td>-8.552986e-02</td>\n",
              "      <td>-1.393867e-01</td>\n",
              "      <td>-1.171828e-01</td>\n",
              "      <td>-7.715099e-02</td>\n",
              "      <td>-1.297556e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-5.665178e-01</td>\n",
              "      <td>-2.861803e-01</td>\n",
              "      <td>2.567888e-01</td>\n",
              "      <td>-3.522820e-01</td>\n",
              "      <td>-3.474005e-01</td>\n",
              "      <td>-3.080714e-02</td>\n",
              "      <td>-1.369742e-02</td>\n",
              "      <td>-2.285057e-02</td>\n",
              "      <td>-2.291334e-02</td>\n",
              "      <td>-4.000890e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>2.746347e-03</td>\n",
              "      <td>7.630020e-03</td>\n",
              "      <td>-2.648043e-02</td>\n",
              "      <td>9.784035e-03</td>\n",
              "      <td>-1.497912e-02</td>\n",
              "      <td>-1.217006e-02</td>\n",
              "      <td>7.174924e-03</td>\n",
              "      <td>-6.122857e-03</td>\n",
              "      <td>-1.886236e-03</td>\n",
              "      <td>-2.155318e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>4.666237e-01</td>\n",
              "      <td>7.602456e-01</td>\n",
              "      <td>1.180015e+00</td>\n",
              "      <td>1.120254e+00</td>\n",
              "      <td>7.643524e-01</td>\n",
              "      <td>7.919617e-01</td>\n",
              "      <td>5.364352e-01</td>\n",
              "      <td>5.710053e-01</td>\n",
              "      <td>3.321130e-01</td>\n",
              "      <td>3.686893e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>1.560046e-01</td>\n",
              "      <td>1.341316e-01</td>\n",
              "      <td>1.485926e-01</td>\n",
              "      <td>1.060697e-01</td>\n",
              "      <td>1.048423e-01</td>\n",
              "      <td>1.038653e-01</td>\n",
              "      <td>1.289528e-01</td>\n",
              "      <td>1.011102e-01</td>\n",
              "      <td>1.092009e-01</td>\n",
              "      <td>1.204874e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.025208e+00</td>\n",
              "      <td>6.278797e+00</td>\n",
              "      <td>4.384637e+00</td>\n",
              "      <td>3.212867e+00</td>\n",
              "      <td>4.699007e+00</td>\n",
              "      <td>3.282794e+00</td>\n",
              "      <td>2.740792e+00</td>\n",
              "      <td>2.124160e+00</td>\n",
              "      <td>2.745443e+00</td>\n",
              "      <td>2.651770e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>5.676768e-01</td>\n",
              "      <td>7.479881e-01</td>\n",
              "      <td>7.837525e-01</td>\n",
              "      <td>5.886874e-01</td>\n",
              "      <td>6.539001e-01</td>\n",
              "      <td>7.707458e-01</td>\n",
              "      <td>6.555741e-01</td>\n",
              "      <td>7.675595e-01</td>\n",
              "      <td>5.632073e-01</td>\n",
              "      <td>5.784544e-01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 94 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7fe858ef-80cd-45c9-8063-31c440dc099e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7fe858ef-80cd-45c9-8063-31c440dc099e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7fe858ef-80cd-45c9-8063-31c440dc099e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-19198af4-f4c2-4ccd-baf7-4d2474a7e31d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-19198af4-f4c2-4ccd-baf7-4d2474a7e31d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-19198af4-f4c2-4ccd-baf7-4d2474a7e31d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "X_train_df=pd.DataFrame(X_train)\n",
        "\n",
        "X_train_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "MweR00w6wl8H",
        "outputId": "20edb263-7b22-4e64-e6aa-38cbebda51d0"
      },
      "id": "MweR00w6wl8H",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6   \\\n",
              "0   -2.637099 -0.347802 -1.305130  0.477352  0.222379  2.597939  1.865067   \n",
              "1   -2.798193  3.634745  2.086230  3.196251  1.620699 -0.670871  1.726479   \n",
              "2   -1.739392 -1.241732 -1.321686 -1.510875 -0.779627 -1.543128  1.477094   \n",
              "3   -0.994942  1.074568  1.187057  1.138465 -0.079325 -4.044148  1.202367   \n",
              "4   -1.329136  2.676327  0.291800  0.271295 -0.906031 -1.966462  1.868663   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "101 -0.328603  3.061680  2.256522 -0.424378 -1.868915 -0.350414  0.226009   \n",
              "102 -0.718224  0.687967 -1.267417 -0.670412 -1.567853  1.657080  0.818342   \n",
              "103  0.445232  1.184668  0.467780 -2.138310 -1.128685 -0.514149  0.245567   \n",
              "104 -1.120806  0.495716  0.477714 -1.356863 -0.036207  0.809519 -0.160998   \n",
              "105 -1.746276  1.329017  1.284418 -0.877288 -0.101824  0.200079  0.221382   \n",
              "\n",
              "           7         8         9   ...        84        85        86  \\\n",
              "0   -0.494544  0.243539  0.256078  ...  0.245732  0.019818 -0.246347   \n",
              "1   -0.083495  0.892471  0.753701  ... -0.439034 -0.363662  0.125037   \n",
              "2   -1.396603  0.295762 -0.055510  ... -0.171100  0.180284  0.044292   \n",
              "3   -0.249646 -0.755334  1.255697  ...  0.026375  0.007201 -0.165179   \n",
              "4   -1.244299 -0.616884 -0.451323  ... -0.145272  0.047353 -0.150540   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "101  1.162425  0.603590 -0.217285  ...  0.227745  0.008060 -0.163510   \n",
              "102  0.515184 -0.144439  0.125603  ... -0.044075  0.357937  0.251036   \n",
              "103  2.021899  0.228138 -0.953547  ... -0.011740  0.419290  0.062435   \n",
              "104 -0.731385  1.327201  0.967042  ... -0.069784  0.047647 -0.162943   \n",
              "105 -0.569326  1.123788 -0.238124  ...  0.070366 -0.196414  0.009736   \n",
              "\n",
              "           87        88        89        90        91        92        93  \n",
              "0   -0.334189  0.001933  0.099690  0.005821  0.098615  0.322466 -0.026514  \n",
              "1   -0.009941  0.000198  0.598846  0.399056 -0.448141 -0.133951  0.163589  \n",
              "2    0.000196  0.003606 -0.028537  0.019995  0.014549 -0.207424 -0.134106  \n",
              "3   -0.136537  0.035306 -0.042689 -0.063572 -0.160413  0.073046 -0.025301  \n",
              "4   -0.009131 -0.106808 -0.137046 -0.175852  0.152502  0.045290 -0.086046  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "101  0.093020 -0.086098  0.027652  0.021020  0.002824 -0.078523 -0.032950  \n",
              "102 -0.005687  0.141986 -0.258837  0.109927 -0.115133 -0.061322 -0.033643  \n",
              "103  0.191918  0.126068  0.287124 -0.040200  0.194610 -0.116119  0.169397  \n",
              "104 -0.053178  0.104861  0.008137 -0.253575 -0.041771 -0.019096  0.024908  \n",
              "105  0.119265  0.025462  0.260401  0.209111 -0.092553  0.108953 -0.142083  \n",
              "\n",
              "[106 rows x 94 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6efd824a-16ae-41c7-9256-c517550392a9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-2.637099</td>\n",
              "      <td>-0.347802</td>\n",
              "      <td>-1.305130</td>\n",
              "      <td>0.477352</td>\n",
              "      <td>0.222379</td>\n",
              "      <td>2.597939</td>\n",
              "      <td>1.865067</td>\n",
              "      <td>-0.494544</td>\n",
              "      <td>0.243539</td>\n",
              "      <td>0.256078</td>\n",
              "      <td>...</td>\n",
              "      <td>0.245732</td>\n",
              "      <td>0.019818</td>\n",
              "      <td>-0.246347</td>\n",
              "      <td>-0.334189</td>\n",
              "      <td>0.001933</td>\n",
              "      <td>0.099690</td>\n",
              "      <td>0.005821</td>\n",
              "      <td>0.098615</td>\n",
              "      <td>0.322466</td>\n",
              "      <td>-0.026514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-2.798193</td>\n",
              "      <td>3.634745</td>\n",
              "      <td>2.086230</td>\n",
              "      <td>3.196251</td>\n",
              "      <td>1.620699</td>\n",
              "      <td>-0.670871</td>\n",
              "      <td>1.726479</td>\n",
              "      <td>-0.083495</td>\n",
              "      <td>0.892471</td>\n",
              "      <td>0.753701</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.439034</td>\n",
              "      <td>-0.363662</td>\n",
              "      <td>0.125037</td>\n",
              "      <td>-0.009941</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.598846</td>\n",
              "      <td>0.399056</td>\n",
              "      <td>-0.448141</td>\n",
              "      <td>-0.133951</td>\n",
              "      <td>0.163589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.739392</td>\n",
              "      <td>-1.241732</td>\n",
              "      <td>-1.321686</td>\n",
              "      <td>-1.510875</td>\n",
              "      <td>-0.779627</td>\n",
              "      <td>-1.543128</td>\n",
              "      <td>1.477094</td>\n",
              "      <td>-1.396603</td>\n",
              "      <td>0.295762</td>\n",
              "      <td>-0.055510</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.171100</td>\n",
              "      <td>0.180284</td>\n",
              "      <td>0.044292</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.003606</td>\n",
              "      <td>-0.028537</td>\n",
              "      <td>0.019995</td>\n",
              "      <td>0.014549</td>\n",
              "      <td>-0.207424</td>\n",
              "      <td>-0.134106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.994942</td>\n",
              "      <td>1.074568</td>\n",
              "      <td>1.187057</td>\n",
              "      <td>1.138465</td>\n",
              "      <td>-0.079325</td>\n",
              "      <td>-4.044148</td>\n",
              "      <td>1.202367</td>\n",
              "      <td>-0.249646</td>\n",
              "      <td>-0.755334</td>\n",
              "      <td>1.255697</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026375</td>\n",
              "      <td>0.007201</td>\n",
              "      <td>-0.165179</td>\n",
              "      <td>-0.136537</td>\n",
              "      <td>0.035306</td>\n",
              "      <td>-0.042689</td>\n",
              "      <td>-0.063572</td>\n",
              "      <td>-0.160413</td>\n",
              "      <td>0.073046</td>\n",
              "      <td>-0.025301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.329136</td>\n",
              "      <td>2.676327</td>\n",
              "      <td>0.291800</td>\n",
              "      <td>0.271295</td>\n",
              "      <td>-0.906031</td>\n",
              "      <td>-1.966462</td>\n",
              "      <td>1.868663</td>\n",
              "      <td>-1.244299</td>\n",
              "      <td>-0.616884</td>\n",
              "      <td>-0.451323</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.145272</td>\n",
              "      <td>0.047353</td>\n",
              "      <td>-0.150540</td>\n",
              "      <td>-0.009131</td>\n",
              "      <td>-0.106808</td>\n",
              "      <td>-0.137046</td>\n",
              "      <td>-0.175852</td>\n",
              "      <td>0.152502</td>\n",
              "      <td>0.045290</td>\n",
              "      <td>-0.086046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>-0.328603</td>\n",
              "      <td>3.061680</td>\n",
              "      <td>2.256522</td>\n",
              "      <td>-0.424378</td>\n",
              "      <td>-1.868915</td>\n",
              "      <td>-0.350414</td>\n",
              "      <td>0.226009</td>\n",
              "      <td>1.162425</td>\n",
              "      <td>0.603590</td>\n",
              "      <td>-0.217285</td>\n",
              "      <td>...</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.008060</td>\n",
              "      <td>-0.163510</td>\n",
              "      <td>0.093020</td>\n",
              "      <td>-0.086098</td>\n",
              "      <td>0.027652</td>\n",
              "      <td>0.021020</td>\n",
              "      <td>0.002824</td>\n",
              "      <td>-0.078523</td>\n",
              "      <td>-0.032950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>-0.718224</td>\n",
              "      <td>0.687967</td>\n",
              "      <td>-1.267417</td>\n",
              "      <td>-0.670412</td>\n",
              "      <td>-1.567853</td>\n",
              "      <td>1.657080</td>\n",
              "      <td>0.818342</td>\n",
              "      <td>0.515184</td>\n",
              "      <td>-0.144439</td>\n",
              "      <td>0.125603</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.044075</td>\n",
              "      <td>0.357937</td>\n",
              "      <td>0.251036</td>\n",
              "      <td>-0.005687</td>\n",
              "      <td>0.141986</td>\n",
              "      <td>-0.258837</td>\n",
              "      <td>0.109927</td>\n",
              "      <td>-0.115133</td>\n",
              "      <td>-0.061322</td>\n",
              "      <td>-0.033643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>0.445232</td>\n",
              "      <td>1.184668</td>\n",
              "      <td>0.467780</td>\n",
              "      <td>-2.138310</td>\n",
              "      <td>-1.128685</td>\n",
              "      <td>-0.514149</td>\n",
              "      <td>0.245567</td>\n",
              "      <td>2.021899</td>\n",
              "      <td>0.228138</td>\n",
              "      <td>-0.953547</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.011740</td>\n",
              "      <td>0.419290</td>\n",
              "      <td>0.062435</td>\n",
              "      <td>0.191918</td>\n",
              "      <td>0.126068</td>\n",
              "      <td>0.287124</td>\n",
              "      <td>-0.040200</td>\n",
              "      <td>0.194610</td>\n",
              "      <td>-0.116119</td>\n",
              "      <td>0.169397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>-1.120806</td>\n",
              "      <td>0.495716</td>\n",
              "      <td>0.477714</td>\n",
              "      <td>-1.356863</td>\n",
              "      <td>-0.036207</td>\n",
              "      <td>0.809519</td>\n",
              "      <td>-0.160998</td>\n",
              "      <td>-0.731385</td>\n",
              "      <td>1.327201</td>\n",
              "      <td>0.967042</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.069784</td>\n",
              "      <td>0.047647</td>\n",
              "      <td>-0.162943</td>\n",
              "      <td>-0.053178</td>\n",
              "      <td>0.104861</td>\n",
              "      <td>0.008137</td>\n",
              "      <td>-0.253575</td>\n",
              "      <td>-0.041771</td>\n",
              "      <td>-0.019096</td>\n",
              "      <td>0.024908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>-1.746276</td>\n",
              "      <td>1.329017</td>\n",
              "      <td>1.284418</td>\n",
              "      <td>-0.877288</td>\n",
              "      <td>-0.101824</td>\n",
              "      <td>0.200079</td>\n",
              "      <td>0.221382</td>\n",
              "      <td>-0.569326</td>\n",
              "      <td>1.123788</td>\n",
              "      <td>-0.238124</td>\n",
              "      <td>...</td>\n",
              "      <td>0.070366</td>\n",
              "      <td>-0.196414</td>\n",
              "      <td>0.009736</td>\n",
              "      <td>0.119265</td>\n",
              "      <td>0.025462</td>\n",
              "      <td>0.260401</td>\n",
              "      <td>0.209111</td>\n",
              "      <td>-0.092553</td>\n",
              "      <td>0.108953</td>\n",
              "      <td>-0.142083</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>106 rows × 94 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6efd824a-16ae-41c7-9256-c517550392a9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6efd824a-16ae-41c7-9256-c517550392a9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6efd824a-16ae-41c7-9256-c517550392a9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ed0b227f-5069-47f6-9db4-4ce3754c87e7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ed0b227f-5069-47f6-9db4-4ce3754c87e7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ed0b227f-5069-47f6-9db4-4ce3754c87e7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_bed35494-b9f5-44cd-a9ea-c5e0940f2a9e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('X_train_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_bed35494-b9f5-44cd-a9ea-c5e0940f2a9e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('X_train_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train_df"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "fa4aeaec-c6a1-49c3-94f7-de5df8c6b13d",
      "metadata": {
        "id": "fa4aeaec-c6a1-49c3-94f7-de5df8c6b13d"
      },
      "outputs": [],
      "source": [
        "X_train_df=pd.DataFrame(X_train)\n",
        "X_train_df_copy=X_train_df.copy()\n",
        "X_train_df.reset_index(inplace=True)\n",
        "\n",
        "\n",
        "Src_ID=[i for i in range(0,106) for _ in range(0,105-i)]\n",
        "Dst_ID=[i for i in range(0,106) for i in range(1+i,106)]\n",
        "elements_to_repeat=X_train.tolist()\n",
        "repetition_counts=[105-i for i in range(0,105)]\n",
        "\n",
        "Src_feature=[]\n",
        "for element, count in zip(elements_to_repeat, repetition_counts):\n",
        "    Src_feature.extend([element] * count)\n",
        "\n",
        "Dst_feature=[]\n",
        "for i in range(1,106):\n",
        "    for j in range(i,106):\n",
        "        Dst_feature.append(X_train.tolist()[j])\n",
        "\n",
        "Nodes_Data=pd.DataFrame()\n",
        "Nodes_Data['Id']=[i for i in range(0,106)]\n",
        "labels = pd.read_csv(\"/AnimalLabels.csv\")\n",
        "labels['majority_vote'] = labels.mode(axis=1, numeric_only=True).astype(int)\n",
        "Nodes_Data['features']=X_train.tolist()\n",
        "Nodes_Data['label']=labels['majority_vote']\n",
        "\n",
        "dup=[0 for i in range(0,106)]\n",
        "Edge=pd.DataFrame()\n",
        "Edge['Src Id']=Src_ID\n",
        "Edge['Src_feature']=Src_feature\n",
        "Edge['Dst_feature']=Dst_feature\n",
        "Edge['Dst Id']=Dst_ID\n",
        "\n",
        "\n",
        "Src_Ids=[i for i in range(0,106) for _ in range(0,106)]\n",
        "Dst_Ids = [i % 106 for i in range(106 * 106)]\n",
        "Src_features=[X_train.tolist()[i] for i in range(0,106)  for i in range(0,106)]\n",
        "elements_to_repeat=X_train.tolist()\n",
        "repetition_counts=[106 for i in range(0,106)]\n",
        "Dst_features=[]\n",
        "for element, count in zip(elements_to_repeat, repetition_counts):\n",
        "    Dst_features.extend([element] * count)\n",
        "\n",
        "Edge_Data=pd.DataFrame()\n",
        "Edge_Data['Src Ids']=Src_Ids\n",
        "Edge_Data['Src_features']=Src_features\n",
        "Edge_Data['Dst_features']=Dst_features\n",
        "Edge_Data['Dst Ids']=Dst_Ids\n",
        "\n",
        "edge_weight=[]\n",
        "for i in range(0,5565):\n",
        "    A=np.array(Edge['Src_feature'][i])\n",
        "    B=np.array(Edge['Dst_feature'][i])\n",
        "    Cosine_similarity=np.dot(A,B)/(norm(A)*norm(B))\n",
        "    edge_weight.append(Cosine_similarity)\n",
        "\n",
        "for i in range(len(edge_weight)):\n",
        "    if edge_weight[i]<0:\n",
        "        edge_weight[i]=0\n",
        "\n",
        "Edge['edge weights']=edge_weight\n",
        "\n",
        "\n",
        "edge_weights=[]\n",
        "for i in range(0,11236):\n",
        "    A=np.array(Edge_Data['Src_features'][i])\n",
        "    B=np.array(Edge_Data['Dst_features'][i])\n",
        "    Cosine_similarity=np.dot(A,B)/(norm(A)*norm(B))\n",
        "    edge_weights.append(Cosine_similarity)\n",
        "\n",
        "Edge_Data['edge weights']=edge_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "905150b5-6c90-4799-a6f4-0147c90c5070",
      "metadata": {
        "id": "905150b5-6c90-4799-a6f4-0147c90c5070"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "a21cb3a5-1768-4115-adbc-ca9531823828",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a21cb3a5-1768-4115-adbc-ca9531823828",
        "outputId": "a9a99c65-cf53-42f2-a45e-6bc831e07e01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    5565.000000\n",
            "mean        0.080212\n",
            "std         0.150858\n",
            "min         0.000000\n",
            "25%         0.000000\n",
            "50%         0.000000\n",
            "75%         0.105862\n",
            "max         0.951397\n",
            "Name: edge weights, dtype: float64\n",
            "95th percentile of edge weights column: 0.3811933465150467\n"
          ]
        }
      ],
      "source": [
        "print(Edge['edge weights'].describe())\n",
        "ninetyfive_percentile = Edge['edge weights'].quantile(0.95)\n",
        "print(\"95th percentile of edge weights column:\", ninetyfive_percentile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "c11cc07a-abd1-4ae6-a9c9-944acdc330ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "c11cc07a-abd1-4ae6-a9c9-944acdc330ea",
        "outputId": "10b0ee1d-07f2-4d98-9712-0deea23e5212"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-32-55db6a681009>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-55db6a681009>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    percentiles=[round(0.000000,4),round(0.000000,4),round(0.108516,4),round(,4),round(0.8181034905115703,4)]\u001b[0m\n\u001b[0m                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "percentiles=[round(0.000000,4),round(0.000000,4),round(0.108516,4),round(,4),round(0.8181034905115703,4)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "a3e0d5d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3e0d5d0",
        "outputId": "094665cd-52e0-40c4-9835-6bd9ed6a0acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph(num_nodes=106, num_edges=666,\n",
            "      ndata_schemes={'feat': Scheme(shape=(94,), dtype=torch.float64), 'label': Scheme(shape=(), dtype=torch.int8), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}\n",
            "      edata_schemes={'weight': Scheme(shape=(), dtype=torch.float64)})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-5d5d5a1f4164>:28: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "  node_labels = torch.from_numpy(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import torch\n",
        "from dgl.data import DGLDataset\n",
        "edge_weight=[]\n",
        "\n",
        "\n",
        "class KarateClubDataset(DGLDataset):\n",
        "    def __init__(self,threshold):\n",
        "        self.threshold=threshold\n",
        "        super().__init__(name=\"karate_club\")\n",
        "\n",
        "    def process(self):\n",
        "        edge_remove=[]\n",
        "        C=edge_weights\n",
        "        for i in range(0,len(C)):\n",
        "            if C[i]<=self.threshold:\n",
        "                edge_remove.append(i)\n",
        "            else:\n",
        "                edge_weight.append(C[i])\n",
        "\n",
        "        nodes_data = Nodes_Data\n",
        "        edges_data = Edge_Data\n",
        "        features_array = np.array(nodes_data[\"features\"].tolist(), dtype=float)\n",
        "        node_features = torch.from_numpy(features_array)\n",
        "        node_labels = torch.from_numpy(\n",
        "                      nodes_data[\"label\"].astype(\"category\").cat.codes.to_numpy()\n",
        "                       ).clone().detach()  # Make the tensor writable\n",
        "\n",
        "        edge_features = torch.from_numpy(edges_data[\"edge weights\"].to_numpy())\n",
        "        edges_src = torch.from_numpy(edges_data[\"Src Ids\"].to_numpy())\n",
        "        edges_dst = torch.from_numpy(edges_data[\"Dst Ids\"].to_numpy())\n",
        "\n",
        "        self.graph = dgl.graph(\n",
        "            (edges_src, edges_dst), num_nodes=nodes_data.shape[0]\n",
        "\n",
        "        )\n",
        "\n",
        "        self.graph.ndata[\"feat\"] = node_features\n",
        "        self.graph.ndata[\"label\"] = node_labels\n",
        "        self.graph.edata[\"weight\"] = edge_features\n",
        "\n",
        "        self.graph=dgl.remove_edges(self.graph, torch.tensor(edge_remove))\n",
        "        n_nodes = nodes_data.shape[0]\n",
        "        n_train = int(n_nodes * 0.6)\n",
        "        n_val = int(n_nodes * 0.2)\n",
        "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        train_mask[:n_train] = True\n",
        "        val_mask[n_train : n_train + n_val] = True\n",
        "        test_mask[n_train + n_val :] = True\n",
        "        self.graph.ndata[\"train_mask\"] = train_mask\n",
        "        self.graph.ndata[\"val_mask\"] = val_mask\n",
        "        self.graph.ndata[\"test_mask\"] = test_mask\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.graph\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "\n",
        "dataset = KarateClubDataset(0.381)\n",
        "g = dataset[0]\n",
        "\n",
        "print(g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "c6ca11d1-4e73-4c8c-ab56-5a4aede6063b",
      "metadata": {
        "id": "c6ca11d1-4e73-4c8c-ab56-5a4aede6063b"
      },
      "outputs": [],
      "source": [
        "#new_g = dgl.compact_graphs(g)\n",
        "train_mask = g.ndata[\"train_mask\"]\n",
        "\n",
        "for i in range(106):\n",
        "    train_mask[i] = True\n",
        "\n",
        "indices_to_change = [4, 105, 84, 27, 98, 88, 18, 65, 9, 2, 5, 49, 99, 69, 86, 67, 7, 28, 78, 70, 18, 74]\n",
        "train_mask[indices_to_change] = False\n",
        "g.ndata[\"train_mask\"]=train_mask\n",
        "\n",
        "test_mask = ~train_mask\n",
        "g.ndata[\"test_mask\"]=test_mask\n",
        "\n",
        "Edge_Data_train = Edge_Data[~(Edge_Data['Src Ids'].isin(indices_to_change)) & ~(Edge_Data['Dst Ids'].isin(indices_to_change)) & Edge_Data['edge weights']>0.8258]\n",
        "Edge_Data_train = Edge_Data_train[Edge_Data_train['edge weights']>0.8258]\n",
        "\n",
        "\n",
        "Edge_Data_test = Edge_Data[Edge_Data['Src Ids'].isin(indices_to_change) & Edge_Data['Dst Ids'].isin(indices_to_change) & Edge_Data['edge weights']>0.8258]\n",
        "Edge_Data_test = Edge_Data_test[Edge_Data_test['edge weights']>0.8258]\n",
        "\n",
        "\n",
        "#adj_matrix=g.adj(etype=None)\n",
        "#dense_matrix = adj_matrix.to_dense().numpy()\n",
        "\n",
        "#new_column = np.arange(0,dense_matrix.shape[0])\n",
        "#new_column=np.reshape(new_column,(106,1))\n",
        "#new_column.shape\n",
        "#dense_matrix=np.concatenate((new_column,dense_matrix),axis=1)\n",
        "\n",
        "#new_row = np.arange(0,106)\n",
        "#new_row=np.reshape(new_row,(1,106))\n",
        "#new_row = np.insert(new_row, 0, 0)\n",
        "#new_row=np.reshape(new_row,(1,107))\n",
        "\n",
        "#result_matrix = np.vstack([new_row,dense_matrix])\n",
        "\n",
        "#np.savetxt('dense_matrix_avg.csv',result_matrix , delimiter=',',fmt='%d')\n",
        "#FileLink(r'dense_matrix_avg.csv')\n",
        "\n",
        "sg_train=dgl.node_subgraph(g, train_mask)\n",
        "#sg_adjacency_train=sg_train.adj()\n",
        "#dense_matrix_sg_train = sg_adjacency_train.to_dense().numpy()\n",
        "\n",
        "#np.savetxt('dense_matrix_sg_train.csv',dense_matrix_sg_train, delimiter=',',fmt='%d')\n",
        "#FileLink(r'dense_matrix_sg_train.csv')\n",
        "\n",
        "sg_test = dgl.node_subgraph(g, test_mask)\n",
        "#sg_adjacency_test=sg_test.adj()\n",
        "#dense_matrix_sg_test = sg_adjacency_test.to_dense().numpy()\n",
        "\n",
        "#np.savetxt('dense_matrix_sg_test.csv',dense_matrix_sg_test, delimiter=',',fmt='%d')\n",
        "#FileLink(r'dense_matrix_sg_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "81296801",
      "metadata": {
        "id": "81296801"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GraphConv(in_feats, h_feats,norm='both')\n",
        "        self.conv2 = GraphConv(h_feats, num_classes)\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        in_feat = in_feat.float()\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.leaky_relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "4e5bbcac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e5bbcac",
        "outputId": "313bddca-c86c-484b-ea11-33b32b4034a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/tensor.py:352: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  assert input.numel() == input.storage().size(), \"Cannot convert view \" \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In epoch 0, loss: 0.774, train acc: 0.553 (train 0.553), test acc: 0.286 (best 0.286)\n",
            "In epoch 5, loss: 0.527, train acc: 0.741 (train 0.741), test acc: 0.429 (best 0.429)\n",
            "In epoch 10, loss: 0.495, train acc: 0.741 (train 0.741), test acc: 0.476 (best 0.476)\n",
            "In epoch 15, loss: 0.467, train acc: 0.741 (train 0.741), test acc: 0.524 (best 0.524)\n",
            "In epoch 20, loss: 0.456, train acc: 0.753 (train 0.753), test acc: 0.524 (best 0.524)\n",
            "In epoch 25, loss: 0.443, train acc: 0.729 (train 0.753), test acc: 0.524 (best 0.524)\n",
            "In epoch 30, loss: 0.435, train acc: 0.765 (train 0.765), test acc: 0.476 (best 0.524)\n",
            "In epoch 35, loss: 0.427, train acc: 0.776 (train 0.776), test acc: 0.476 (best 0.524)\n",
            "In epoch 40, loss: 0.421, train acc: 0.776 (train 0.776), test acc: 0.476 (best 0.524)\n",
            "In epoch 45, loss: 0.414, train acc: 0.776 (train 0.776), test acc: 0.524 (best 0.524)\n",
            "In epoch 50, loss: 0.409, train acc: 0.776 (train 0.776), test acc: 0.524 (best 0.524)\n",
            "In epoch 55, loss: 0.403, train acc: 0.788 (train 0.788), test acc: 0.524 (best 0.524)\n",
            "In epoch 60, loss: 0.397, train acc: 0.788 (train 0.788), test acc: 0.524 (best 0.524)\n",
            "In epoch 65, loss: 0.392, train acc: 0.788 (train 0.788), test acc: 0.524 (best 0.524)\n",
            "In epoch 70, loss: 0.386, train acc: 0.800 (train 0.800), test acc: 0.524 (best 0.524)\n",
            "In epoch 75, loss: 0.381, train acc: 0.800 (train 0.800), test acc: 0.524 (best 0.524)\n",
            "In epoch 80, loss: 0.375, train acc: 0.800 (train 0.800), test acc: 0.571 (best 0.571)\n",
            "In epoch 85, loss: 0.369, train acc: 0.800 (train 0.800), test acc: 0.571 (best 0.571)\n",
            "In epoch 90, loss: 0.362, train acc: 0.800 (train 0.800), test acc: 0.571 (best 0.571)\n",
            "In epoch 95, loss: 0.356, train acc: 0.800 (train 0.800), test acc: 0.571 (best 0.571)\n",
            "In epoch 100, loss: 0.350, train acc: 0.800 (train 0.800), test acc: 0.571 (best 0.571)\n",
            "In epoch 105, loss: 0.344, train acc: 0.824 (train 0.824), test acc: 0.571 (best 0.571)\n",
            "In epoch 110, loss: 0.338, train acc: 0.824 (train 0.824), test acc: 0.571 (best 0.571)\n",
            "In epoch 115, loss: 0.332, train acc: 0.824 (train 0.824), test acc: 0.571 (best 0.571)\n",
            "In epoch 120, loss: 0.327, train acc: 0.835 (train 0.835), test acc: 0.571 (best 0.571)\n",
            "In epoch 125, loss: 0.322, train acc: 0.835 (train 0.835), test acc: 0.571 (best 0.571)\n",
            "In epoch 130, loss: 0.317, train acc: 0.835 (train 0.835), test acc: 0.571 (best 0.571)\n",
            "In epoch 135, loss: 0.313, train acc: 0.835 (train 0.835), test acc: 0.571 (best 0.571)\n",
            "In epoch 140, loss: 0.308, train acc: 0.835 (train 0.835), test acc: 0.571 (best 0.571)\n",
            "In epoch 145, loss: 0.304, train acc: 0.847 (train 0.847), test acc: 0.571 (best 0.571)\n",
            "In epoch 150, loss: 0.300, train acc: 0.835 (train 0.847), test acc: 0.571 (best 0.571)\n",
            "In epoch 155, loss: 0.297, train acc: 0.835 (train 0.847), test acc: 0.571 (best 0.571)\n",
            "In epoch 160, loss: 0.294, train acc: 0.835 (train 0.847), test acc: 0.571 (best 0.571)\n",
            "In epoch 165, loss: 0.290, train acc: 0.835 (train 0.847), test acc: 0.571 (best 0.571)\n",
            "In epoch 170, loss: 0.289, train acc: 0.835 (train 0.847), test acc: 0.571 (best 0.571)\n",
            "In epoch 175, loss: 0.285, train acc: 0.835 (train 0.847), test acc: 0.571 (best 0.571)\n",
            "In epoch 180, loss: 0.283, train acc: 0.847 (train 0.847), test acc: 0.571 (best 0.571)\n",
            "In epoch 185, loss: 0.280, train acc: 0.847 (train 0.847), test acc: 0.571 (best 0.571)\n",
            "In epoch 190, loss: 0.278, train acc: 0.847 (train 0.847), test acc: 0.571 (best 0.571)\n",
            "In epoch 195, loss: 0.275, train acc: 0.847 (train 0.847), test acc: 0.571 (best 0.571)\n",
            "In epoch 200, loss: 0.273, train acc: 0.847 (train 0.847), test acc: 0.571 (best 0.571)\n",
            "In epoch 205, loss: 0.271, train acc: 0.847 (train 0.847), test acc: 0.571 (best 0.571)\n",
            "In epoch 210, loss: 0.269, train acc: 0.847 (train 0.847), test acc: 0.571 (best 0.571)\n",
            "In epoch 215, loss: 0.267, train acc: 0.847 (train 0.847), test acc: 0.571 (best 0.571)\n",
            "In epoch 220, loss: 0.265, train acc: 0.847 (train 0.847), test acc: 0.571 (best 0.571)\n",
            "In epoch 225, loss: 0.265, train acc: 0.847 (train 0.847), test acc: 0.571 (best 0.571)\n",
            "In epoch 230, loss: 0.263, train acc: 0.859 (train 0.859), test acc: 0.571 (best 0.571)\n",
            "In epoch 235, loss: 0.261, train acc: 0.859 (train 0.859), test acc: 0.571 (best 0.571)\n",
            "In epoch 240, loss: 0.260, train acc: 0.847 (train 0.859), test acc: 0.571 (best 0.571)\n",
            "In epoch 245, loss: 0.259, train acc: 0.859 (train 0.859), test acc: 0.571 (best 0.571)\n",
            "In epoch 250, loss: 0.257, train acc: 0.859 (train 0.859), test acc: 0.571 (best 0.571)\n",
            "In epoch 255, loss: 0.256, train acc: 0.859 (train 0.859), test acc: 0.571 (best 0.571)\n",
            "In epoch 260, loss: 0.255, train acc: 0.847 (train 0.859), test acc: 0.571 (best 0.571)\n",
            "In epoch 265, loss: 0.254, train acc: 0.847 (train 0.859), test acc: 0.571 (best 0.571)\n",
            "In epoch 270, loss: 0.253, train acc: 0.847 (train 0.859), test acc: 0.571 (best 0.571)\n",
            "In epoch 275, loss: 0.255, train acc: 0.859 (train 0.859), test acc: 0.524 (best 0.571)\n",
            "In epoch 280, loss: 0.252, train acc: 0.859 (train 0.859), test acc: 0.524 (best 0.571)\n",
            "In epoch 285, loss: 0.251, train acc: 0.859 (train 0.859), test acc: 0.524 (best 0.571)\n",
            "In epoch 290, loss: 0.250, train acc: 0.859 (train 0.859), test acc: 0.524 (best 0.571)\n",
            "In epoch 295, loss: 0.249, train acc: 0.859 (train 0.859), test acc: 0.524 (best 0.571)\n",
            "In epoch 300, loss: 0.249, train acc: 0.859 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 305, loss: 0.248, train acc: 0.871 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 310, loss: 0.247, train acc: 0.871 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 315, loss: 0.247, train acc: 0.859 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 320, loss: 0.245, train acc: 0.859 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 325, loss: 0.245, train acc: 0.859 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 330, loss: 0.245, train acc: 0.859 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 335, loss: 0.244, train acc: 0.859 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 340, loss: 0.243, train acc: 0.871 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 345, loss: 0.243, train acc: 0.859 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 350, loss: 0.244, train acc: 0.859 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 355, loss: 0.243, train acc: 0.859 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 360, loss: 0.242, train acc: 0.859 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 365, loss: 0.241, train acc: 0.859 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 370, loss: 0.240, train acc: 0.859 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 375, loss: 0.240, train acc: 0.859 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 380, loss: 0.240, train acc: 0.859 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 385, loss: 0.240, train acc: 0.871 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 390, loss: 0.239, train acc: 0.859 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 395, loss: 0.238, train acc: 0.859 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 400, loss: 0.238, train acc: 0.871 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 405, loss: 0.237, train acc: 0.871 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 410, loss: 0.237, train acc: 0.871 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 415, loss: 0.237, train acc: 0.871 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 420, loss: 0.237, train acc: 0.871 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 425, loss: 0.237, train acc: 0.871 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 430, loss: 0.236, train acc: 0.871 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 435, loss: 0.236, train acc: 0.871 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 440, loss: 0.235, train acc: 0.871 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 445, loss: 0.235, train acc: 0.871 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 450, loss: 0.235, train acc: 0.871 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 455, loss: 0.235, train acc: 0.871 (train 0.871), test acc: 0.524 (best 0.571)\n",
            "In epoch 460, loss: 0.235, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 465, loss: 0.235, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 470, loss: 0.234, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 475, loss: 0.233, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 480, loss: 0.233, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 485, loss: 0.233, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 490, loss: 0.233, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 495, loss: 0.233, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 500, loss: 0.238, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 505, loss: 0.233, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 510, loss: 0.234, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 515, loss: 0.231, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 520, loss: 0.232, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 525, loss: 0.236, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 530, loss: 0.271, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 535, loss: 0.264, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 540, loss: 0.269, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 545, loss: 0.246, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 550, loss: 0.238, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 555, loss: 0.238, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 560, loss: 0.237, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 565, loss: 0.234, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 570, loss: 0.233, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 575, loss: 0.231, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 580, loss: 0.231, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 585, loss: 0.230, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 590, loss: 0.230, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 595, loss: 0.230, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 600, loss: 0.230, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 605, loss: 0.229, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 610, loss: 0.229, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 615, loss: 0.229, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 620, loss: 0.229, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 625, loss: 0.229, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 630, loss: 0.229, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 635, loss: 0.229, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 640, loss: 0.229, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 645, loss: 0.229, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 650, loss: 0.228, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 655, loss: 0.228, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 660, loss: 0.228, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 665, loss: 0.228, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 670, loss: 0.228, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 675, loss: 0.228, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 680, loss: 0.228, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 685, loss: 0.228, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 690, loss: 0.228, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 695, loss: 0.228, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 700, loss: 0.228, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 705, loss: 0.227, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 710, loss: 0.227, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 715, loss: 0.227, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 720, loss: 0.227, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 725, loss: 0.227, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 730, loss: 0.227, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 735, loss: 0.227, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 740, loss: 0.227, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 745, loss: 0.227, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 750, loss: 0.227, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 755, loss: 0.227, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 760, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 765, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 770, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 775, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 780, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 785, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 790, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 795, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 800, loss: 0.228, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 805, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 810, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.619 (best 0.619)\n",
            "In epoch 815, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 820, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 825, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 830, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 835, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 840, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 845, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 850, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 855, loss: 0.225, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 860, loss: 0.225, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 865, loss: 0.225, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 870, loss: 0.225, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 875, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 880, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 885, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 890, loss: 0.225, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 895, loss: 0.225, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 900, loss: 0.225, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 905, loss: 0.225, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 910, loss: 0.225, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 915, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 920, loss: 0.225, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 925, loss: 0.225, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 930, loss: 0.225, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 935, loss: 0.225, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 940, loss: 0.225, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 945, loss: 0.225, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 950, loss: 0.225, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 955, loss: 0.226, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 960, loss: 0.225, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 965, loss: 0.225, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 970, loss: 0.225, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 975, loss: 0.224, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 980, loss: 0.224, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 985, loss: 0.224, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 990, loss: 0.224, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n",
            "In epoch 995, loss: 0.224, train acc: 0.871 (train 0.871), test acc: 0.571 (best 0.619)\n"
          ]
        }
      ],
      "source": [
        "los=[]\n",
        "train_accuracy=[]\n",
        "test_accuracy=[]\n",
        "epoch=[]\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "def train(sg_train,sg_test, model):\n",
        "    model.train()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    best_train_acc = 0\n",
        "    best_test_acc = 0\n",
        "\n",
        "    features_train = sg_train.ndata[\"feat\"]\n",
        "    labels = g.ndata[\"label\"]\n",
        "    train_mask = g.ndata[\"train_mask\"]\n",
        "    test_mask = g.ndata[\"test_mask\"]\n",
        "\n",
        "\n",
        "    for e in range(1000):\n",
        "        # Forward\n",
        "        logits = model(sg_train,features_train)\n",
        "\n",
        "        # Compute prediction\n",
        "        pred = logits.argmax(1)\n",
        "\n",
        "        labels = g.ndata[\"label\"].long()\n",
        "        loss = F.cross_entropy(logits, labels[train_mask])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            logits_test = model(sg_test,sg_test.ndata[\"feat\"])\n",
        "\n",
        "\n",
        "        pred_test = logits_test.argmax(1)\n",
        "\n",
        "        train_acc = (pred == labels[train_mask]).float().mean()\n",
        "        test_acc = (pred_test == labels[test_mask]).float().mean()\n",
        "\n",
        "        if best_train_acc < train_acc:\n",
        "            best_train_acc = train_acc\n",
        "        if best_test_acc < test_acc:\n",
        "            best_test_acc = test_acc\n",
        "\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        if e % 5 == 0:\n",
        "            epoch.append(e)\n",
        "            los.append(round(loss.item(),3))\n",
        "            train_accuracy.append(round(train_acc.item(),3))\n",
        "            test_accuracy.append(round(test_acc.item(),3))\n",
        "            print(\n",
        "                f\"In epoch {e}, loss: {loss:.3f}, train acc: {train_acc:.3f} (train {best_train_acc:.3f}), test acc: {test_acc:.3f} (best {best_test_acc:.3f})\"\n",
        "            )\n",
        "\n",
        "\n",
        "model = GCN(g.ndata[\"feat\"].shape[1],20, 2)\n",
        "train(sg_train,sg_test,model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "25868d11",
      "metadata": {
        "id": "25868d11"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import torch\n",
        "from dgl.data import DGLDataset\n",
        "\n",
        "\n",
        "class KarateClubDataset(DGLDataset):\n",
        "    def __init__(self,params):\n",
        "        self.params=params\n",
        "        #self.threshold=threshold\n",
        "        super().__init__(name=\"karate_club\")\n",
        "        self.process()\n",
        "    def process(self):\n",
        "        edge_remove=[]\n",
        "        C=edge_weights\n",
        "        for i in range(0,len(C)):\n",
        "            if C[i]<=self.params['percentile']:\n",
        "                edge_remove.append(i)\n",
        "        nodes_data = Nodes_Data\n",
        "        edges_data = Edge_Data\n",
        "        features_array = np.array(nodes_data[\"features\"].tolist(), dtype=float)\n",
        "        node_features = torch.from_numpy(features_array)\n",
        "        node_labels = torch.from_numpy(\n",
        "                      nodes_data[\"label\"].astype(\"category\").cat.codes.to_numpy()\n",
        "                       ).clone().detach()\n",
        "        edge_features = torch.from_numpy(edges_data[\"edge weights\"].to_numpy())\n",
        "        edges_src = torch.from_numpy(edges_data[\"Src Ids\"].to_numpy())\n",
        "        edges_dst = torch.from_numpy(edges_data[\"Dst Ids\"].to_numpy())\n",
        "        self.graph = dgl.graph(\n",
        "            (edges_src, edges_dst), num_nodes=nodes_data.shape[0]\n",
        "        )\n",
        "        self.graph.ndata[\"feat\"] = node_features\n",
        "        self.graph.ndata[\"label\"] = node_labels\n",
        "        self.graph.edata[\"weight\"] = edge_features\n",
        "\n",
        "        self.graph=dgl.remove_edges(self.graph, torch.tensor(edge_remove))\n",
        "        n_nodes = nodes_data.shape[0]\n",
        "        n_train = int(n_nodes * 0.6)\n",
        "        n_val = int(n_nodes * 0.2)\n",
        "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        train_mask[:n_train] = True\n",
        "        val_mask[n_train : n_train + n_val] = True\n",
        "        test_mask[n_train + n_val :] = True\n",
        "        self.graph.ndata[\"train_mask\"] = train_mask\n",
        "        self.graph.ndata[\"val_mask\"] = val_mask\n",
        "        self.graph.ndata[\"test_mask\"] = test_mask\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.graph\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "98b3db11",
      "metadata": {
        "id": "98b3db11"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self,in_feats,params,num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GraphConv(in_feats, params['output_features'])\n",
        "        self.conv2 = GraphConv(params['output_features'], num_classes)\n",
        "\n",
        "    def forward(self, g,in_feat,params):\n",
        "        in_feat = in_feat.float()\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = getattr(F, params['activation'])(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "efe9aac8",
      "metadata": {
        "id": "efe9aac8"
      },
      "outputs": [],
      "source": [
        "los=[]\n",
        "train_accuracy=[]\n",
        "test_accuracy=[]\n",
        "TT=[]\n",
        "CT=[]\n",
        "epoch=[]\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "def train(g,sg_train,sg_test, model,params):\n",
        "    model.train()\n",
        "    optimizer = getattr(optim, params['optimizer'])(model.parameters(), lr= params['learning_rate'])\n",
        "    best_train_acc = 0\n",
        "    best_test_acc = 0\n",
        "    corresponding_test=0\n",
        "\n",
        "    features_train = sg_train.ndata[\"feat\"]\n",
        "    labels = g.ndata[\"label\"]\n",
        "    train_mask = g.ndata[\"train_mask\"]\n",
        "    test_mask = g.ndata[\"test_mask\"]\n",
        "\n",
        "\n",
        "    for e in range(100):\n",
        "        # Forward\n",
        "        logits = model(sg_train,features_train,params)\n",
        "\n",
        "\n",
        "        # Compute prediction\n",
        "        pred = logits.argmax(1)\n",
        "\n",
        "\n",
        "        labels = g.ndata[\"label\"].long()\n",
        "        if params['loss'] == 'binary_cross_entropy':\n",
        "            loss = getattr(F, params['loss'])(torch.sigmoid(logits), labels[train_mask])\n",
        "        else:\n",
        "            loss = getattr(F, params['loss'])(logits, labels[train_mask])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            logits_test = model(sg_test,sg_test.ndata[\"feat\"],params)\n",
        "\n",
        "\n",
        "        pred_test = logits_test.argmax(1)\n",
        "\n",
        "        train_acc = (pred == labels[train_mask]).float().mean()\n",
        "        test_acc = (pred_test == labels[test_mask]).float().mean()\n",
        "\n",
        "        if best_train_acc < train_acc:\n",
        "            best_train_acc = train_acc\n",
        "            corresponding_test=test_acc\n",
        "        if best_test_acc < test_acc:\n",
        "            best_test_acc = test_acc\n",
        "\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if e % 5 == 0:\n",
        "            epoch.append(e)\n",
        "            los.append(round(loss.item(),3))\n",
        "            train_accuracy.append(round(train_acc.item(),3))\n",
        "\n",
        "    TT.append({'train_accuracy':best_train_acc})\n",
        "    CT.append({'test_accuracy': corresponding_test})\n",
        "\n",
        "    print(f'Train Accuracy: {best_train_acc}, Test Accuracy: {corresponding_test}')\n",
        "    return best_train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "23fca1dd",
      "metadata": {
        "id": "23fca1dd",
        "outputId": "b5d91e9c-6dac-4bd2-bafc-ff8b06df1017",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.6 alembic-1.13.3 colorlog-6.8.2 optuna-4.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "daf23785",
      "metadata": {
        "id": "daf23785"
      },
      "outputs": [],
      "source": [
        "percentiles=[round(0.052869,4),round(0.16715442625713778,4),round(0.043810,4)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "80df91ce",
      "metadata": {
        "id": "80df91ce"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    percentile_values=[percentiles[0],percentiles[1],percentiles[2]]\n",
        "    learning_rate=[0.01,0.1,1,10,100,200]\n",
        "    output_features=[10,12,14,16,18,20,22]\n",
        "    params={'output_features':trial.suggest_categorical('output_features',output_features),\n",
        "           'optimizer':trial.suggest_categorical('optimizer',[\"Adam\", \"RMSprop\", \"SGD\"]),\n",
        "           'percentile':trial.suggest_categorical('percentile',percentile_values),\n",
        "           'learning_rate':trial.suggest_categorical('learning_rate',learning_rate),\n",
        "           'loss':trial.suggest_categorical('loss',['cross_entropy']),\n",
        "           'activation':trial.suggest_categorical('activation',['relu','selu','elu','leaky_relu','tanh'])}\n",
        "    dataset = KarateClubDataset(params)\n",
        "    g = dataset[0]\n",
        "    train_mask = g.ndata[\"train_mask\"]\n",
        "    for i in range(106):\n",
        "        train_mask[i] = True\n",
        "\n",
        "    indices_to_change = [4, 105, 84, 27, 98, 88, 18, 65, 9, 2, 5, 49, 99, 69, 86, 67, 7, 28, 78, 70, 18, 74]\n",
        "    train_mask[indices_to_change] = False\n",
        "    g.ndata[\"train_mask\"]=train_mask\n",
        "    test_mask = ~train_mask\n",
        "    g.ndata[\"test_mask\"]=test_mask\n",
        "    sg_train=dgl.node_subgraph(g, train_mask)\n",
        "    sg_test = dgl.node_subgraph(g, test_mask)\n",
        "    model=GCN(g.ndata[\"feat\"].shape[1],params, 2)\n",
        "    accuracy=train(g,sg_train,sg_test,model,params)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "c178e8ba",
      "metadata": {
        "id": "c178e8ba",
        "outputId": "f6291229-7e74-48ba-8eca-bb1b389cffb3",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:07:44,710] A new study created in memory with name: no-name-0a9902a6-ad3a-4967-9334-d32caf13bb47\n",
            "[I 2024-10-28 23:07:45,576] Trial 0 finished with value: 0.8705882430076599 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 0 with value: 0.8705882430076599.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:07:46,381] Trial 1 finished with value: 0.7176470756530762 and parameters: {'output_features': 12, 'optimizer': 'RMSprop', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 0 with value: 0.8705882430076599.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:07:47,246] Trial 2 finished with value: 0.7647058963775635 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 0 with value: 0.8705882430076599.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7647058963775635, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:07:48,107] Trial 3 finished with value: 0.7529411911964417 and parameters: {'output_features': 12, 'optimizer': 'RMSprop', 'percentile': 0.0529, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 0 with value: 0.8705882430076599.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7529411911964417, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:07:48,987] Trial 4 finished with value: 0.7058823704719543 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 0 with value: 0.8705882430076599.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:07:49,815] Trial 5 finished with value: 0.6470588445663452 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 0 with value: 0.8705882430076599.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:07:50,586] Trial 6 finished with value: 0.6470588445663452 and parameters: {'output_features': 22, 'optimizer': 'SGD', 'percentile': 0.0438, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 0 with value: 0.8705882430076599.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:07:51,364] Trial 7 finished with value: 0.6941176652908325 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 0 with value: 0.8705882430076599.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:07:52,164] Trial 8 finished with value: 0.7411764860153198 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 0 with value: 0.8705882430076599.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:07:52,933] Trial 9 finished with value: 0.8705882430076599 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 0 with value: 0.8705882430076599.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:07:53,690] Trial 10 finished with value: 0.7058823704719543 and parameters: {'output_features': 18, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 0 with value: 0.8705882430076599.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:07:54,696] Trial 11 finished with value: 0.8470588326454163 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 0 with value: 0.8705882430076599.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:07:55,784] Trial 12 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:07:57,232] Trial 13 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:07:58,227] Trial 14 finished with value: 0.729411780834198 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:07:59,045] Trial 15 finished with value: 0.800000011920929 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.800000011920929, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:07:59,879] Trial 16 finished with value: 0.800000011920929 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.800000011920929, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:00,716] Trial 17 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:01,467] Trial 18 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:02,267] Trial 19 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:03,184] Trial 20 finished with value: 0.800000011920929 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.800000011920929, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:04,012] Trial 21 finished with value: 0.8588235378265381 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:04,834] Trial 22 finished with value: 0.8352941274642944 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:05,666] Trial 23 finished with value: 0.7647058963775635 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7647058963775635, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:06,464] Trial 24 finished with value: 0.8823529481887817 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:07,275] Trial 25 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:08,270] Trial 26 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:09,454] Trial 27 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:10,671] Trial 28 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:11,786] Trial 29 finished with value: 0.729411780834198 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:12,695] Trial 30 finished with value: 0.7764706015586853 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7764706015586853, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:13,520] Trial 31 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:14,380] Trial 32 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:15,181] Trial 33 finished with value: 0.800000011920929 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.800000011920929, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:16,105] Trial 34 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:16,930] Trial 35 finished with value: 0.7411764860153198 and parameters: {'output_features': 12, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:17,729] Trial 36 finished with value: 0.7529411911964417 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7529411911964417, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:18,543] Trial 37 finished with value: 0.7411764860153198 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:19,464] Trial 38 finished with value: 0.8117647171020508 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:20,295] Trial 39 finished with value: 0.6470588445663452 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:21,085] Trial 40 finished with value: 0.7058823704719543 and parameters: {'output_features': 12, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:21,882] Trial 41 finished with value: 0.8823529481887817 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 12 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:22,977] Trial 42 finished with value: 0.9411764740943909 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 42 with value: 0.9411764740943909.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:24,057] Trial 43 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 42 with value: 0.9411764740943909.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:25,268] Trial 44 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 42 with value: 0.9411764740943909.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:26,183] Trial 45 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 42 with value: 0.9411764740943909.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:26,962] Trial 46 finished with value: 0.6470588445663452 and parameters: {'output_features': 16, 'optimizer': 'SGD', 'percentile': 0.0529, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 42 with value: 0.9411764740943909.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:27,842] Trial 47 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 42 with value: 0.9411764740943909.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:28,672] Trial 48 finished with value: 0.7411764860153198 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 42 with value: 0.9411764740943909.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:29,526] Trial 49 finished with value: 0.9529411792755127 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 49 with value: 0.9529411792755127.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:30,309] Trial 50 finished with value: 0.7176470756530762 and parameters: {'output_features': 16, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 49 with value: 0.9529411792755127.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:31,123] Trial 51 finished with value: 0.9529411792755127 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 49 with value: 0.9529411792755127.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:31,942] Trial 52 finished with value: 0.9529411792755127 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 49 with value: 0.9529411792755127.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:32,750] Trial 53 finished with value: 0.9529411792755127 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 49 with value: 0.9529411792755127.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:33,566] Trial 54 finished with value: 0.9647058844566345 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:34,429] Trial 55 finished with value: 0.9529411792755127 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:35,278] Trial 56 finished with value: 0.9529411792755127 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:36,386] Trial 57 finished with value: 0.8470588326454163 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:37,520] Trial 58 finished with value: 0.9529411792755127 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:38,675] Trial 59 finished with value: 0.8235294222831726 and parameters: {'output_features': 22, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:39,856] Trial 60 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:40,657] Trial 61 finished with value: 0.9647058844566345 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:41,497] Trial 62 finished with value: 0.9647058844566345 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:42,322] Trial 63 finished with value: 0.9529411792755127 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:43,138] Trial 64 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:43,968] Trial 65 finished with value: 0.9529411792755127 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:44,805] Trial 66 finished with value: 0.9529411792755127 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:45,616] Trial 67 finished with value: 0.7176470756530762 and parameters: {'output_features': 22, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:46,533] Trial 68 finished with value: 0.8352941274642944 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:47,381] Trial 69 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:48,262] Trial 70 finished with value: 0.8117647171020508 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:49,091] Trial 71 finished with value: 0.9529411792755127 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:49,998] Trial 72 finished with value: 0.9647058844566345 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:51,037] Trial 73 finished with value: 0.9529411792755127 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:52,244] Trial 74 finished with value: 0.9411764740943909 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:53,427] Trial 75 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:54,248] Trial 76 finished with value: 0.8117647171020508 and parameters: {'output_features': 10, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:55,055] Trial 77 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:55,871] Trial 78 finished with value: 0.9529411792755127 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:56,736] Trial 79 finished with value: 0.6941176652908325 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:57,571] Trial 80 finished with value: 0.8941176533699036 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:58,398] Trial 81 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:08:59,215] Trial 82 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:00,058] Trial 83 finished with value: 0.9529411792755127 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:00,888] Trial 84 finished with value: 0.9529411792755127 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:01,731] Trial 85 finished with value: 0.7411764860153198 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:02,506] Trial 86 finished with value: 0.6705882549285889 and parameters: {'output_features': 10, 'optimizer': 'SGD', 'percentile': 0.0438, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6705882549285889, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:03,332] Trial 87 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:04,393] Trial 88 finished with value: 0.8470588326454163 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:05,455] Trial 89 finished with value: 0.8470588326454163 and parameters: {'output_features': 10, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:06,605] Trial 90 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:07,620] Trial 91 finished with value: 0.9529411792755127 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:08,419] Trial 92 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:09,211] Trial 93 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:10,030] Trial 94 finished with value: 0.9411764740943909 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:10,840] Trial 95 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:11,674] Trial 96 finished with value: 0.729411780834198 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:12,502] Trial 97 finished with value: 0.8352941274642944 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:13,289] Trial 98 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:14,098] Trial 99 finished with value: 0.9647058844566345 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:14,861] Trial 100 finished with value: 0.7176470756530762 and parameters: {'output_features': 14, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:15,670] Trial 101 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:16,500] Trial 102 finished with value: 0.9529411792755127 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:17,370] Trial 103 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:18,416] Trial 104 finished with value: 0.6941176652908325 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:19,641] Trial 105 finished with value: 0.729411780834198 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:20,835] Trial 106 finished with value: 0.8235294222831726 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:21,640] Trial 107 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:22,483] Trial 108 finished with value: 0.9411764740943909 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:23,288] Trial 109 finished with value: 0.8470588326454163 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:24,093] Trial 110 finished with value: 0.8117647171020508 and parameters: {'output_features': 16, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:24,893] Trial 111 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 54 with value: 0.9647058844566345.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:25,708] Trial 112 finished with value: 0.9764705896377563 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 112 with value: 0.9764705896377563.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:26,513] Trial 113 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 112 with value: 0.9764705896377563.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:27,340] Trial 114 finished with value: 0.9882352948188782 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:28,846] Trial 115 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:29,796] Trial 116 finished with value: 0.9647058844566345 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:30,657] Trial 117 finished with value: 0.9647058844566345 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:31,735] Trial 118 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:32,910] Trial 119 finished with value: 0.8470588326454163 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:34,221] Trial 120 finished with value: 0.7058823704719543 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:35,052] Trial 121 finished with value: 0.9764705896377563 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:35,884] Trial 122 finished with value: 0.9529411792755127 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:36,705] Trial 123 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:37,552] Trial 124 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:38,369] Trial 125 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:39,207] Trial 126 finished with value: 0.9411764740943909 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:39,989] Trial 127 finished with value: 0.6470588445663452 and parameters: {'output_features': 10, 'optimizer': 'SGD', 'percentile': 0.0438, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:40,792] Trial 128 finished with value: 0.7764706015586853 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7764706015586853, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:41,647] Trial 129 finished with value: 0.8470588326454163 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:42,438] Trial 130 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:43,249] Trial 131 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:44,082] Trial 132 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:45,179] Trial 133 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:46,342] Trial 134 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:47,547] Trial 135 finished with value: 0.9529411792755127 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:48,496] Trial 136 finished with value: 0.8235294222831726 and parameters: {'output_features': 10, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:49,308] Trial 137 finished with value: 0.9529411792755127 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:50,138] Trial 138 finished with value: 0.9411764740943909 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:50,959] Trial 139 finished with value: 0.9529411792755127 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:51,800] Trial 140 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:52,632] Trial 141 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:53,483] Trial 142 finished with value: 0.9764705896377563 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:54,307] Trial 143 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:55,110] Trial 144 finished with value: 0.9411764740943909 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:55,921] Trial 145 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:56,777] Trial 146 finished with value: 0.9647058844566345 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:57,632] Trial 147 finished with value: 0.7411764860153198 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:58,643] Trial 148 finished with value: 0.9647058844566345 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:09:59,655] Trial 149 finished with value: 0.6941176652908325 and parameters: {'output_features': 10, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:00,899] Trial 150 finished with value: 0.8823529481887817 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:02,102] Trial 151 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:03,044] Trial 152 finished with value: 0.9529411792755127 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:03,888] Trial 153 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:04,727] Trial 154 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:05,531] Trial 155 finished with value: 0.8235294222831726 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:06,393] Trial 156 finished with value: 0.6941176652908325 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:07,249] Trial 157 finished with value: 0.9647058844566345 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:08,109] Trial 158 finished with value: 0.6823529601097107 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:08,913] Trial 159 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:09,710] Trial 160 finished with value: 0.8470588326454163 and parameters: {'output_features': 22, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:10,513] Trial 161 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:11,308] Trial 162 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:12,126] Trial 163 finished with value: 0.9529411792755127 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:13,147] Trial 164 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:14,276] Trial 165 finished with value: 0.929411768913269 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:15,578] Trial 166 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:16,494] Trial 167 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:17,317] Trial 168 finished with value: 0.9647058844566345 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:18,176] Trial 169 finished with value: 0.9647058844566345 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:19,009] Trial 170 finished with value: 0.9647058844566345 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:19,848] Trial 171 finished with value: 0.9529411792755127 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:20,666] Trial 172 finished with value: 0.9529411792755127 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:21,484] Trial 173 finished with value: 0.9529411792755127 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:22,289] Trial 174 finished with value: 0.9529411792755127 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:23,087] Trial 175 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:23,900] Trial 176 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:24,740] Trial 177 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:25,577] Trial 178 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:26,596] Trial 179 finished with value: 0.6941176652908325 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:27,736] Trial 180 finished with value: 0.8117647171020508 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:28,945] Trial 181 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:30,130] Trial 182 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:30,993] Trial 183 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:31,827] Trial 184 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:32,640] Trial 185 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:33,543] Trial 186 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:34,393] Trial 187 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:35,245] Trial 188 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:36,088] Trial 189 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:36,938] Trial 190 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:37,767] Trial 191 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:38,611] Trial 192 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:39,439] Trial 193 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:40,342] Trial 194 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:41,422] Trial 195 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:42,608] Trial 196 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:43,681] Trial 197 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:44,520] Trial 198 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:45,346] Trial 199 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:46,251] Trial 200 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:47,119] Trial 201 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:47,964] Trial 202 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:48,819] Trial 203 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:49,671] Trial 204 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:50,539] Trial 205 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:51,382] Trial 206 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:52,183] Trial 207 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:53,009] Trial 208 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:54,049] Trial 209 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:55,174] Trial 210 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:56,469] Trial 211 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:57,497] Trial 212 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:58,355] Trial 213 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:10:59,193] Trial 214 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:00,042] Trial 215 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:00,895] Trial 216 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:01,723] Trial 217 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:02,553] Trial 218 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:03,404] Trial 219 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:04,253] Trial 220 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:05,113] Trial 221 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:05,945] Trial 222 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:06,805] Trial 223 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:07,941] Trial 224 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:09,113] Trial 225 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:10,331] Trial 226 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:11,442] Trial 227 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:12,208] Trial 228 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:13,052] Trial 229 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:13,890] Trial 230 finished with value: 0.8117647171020508 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:14,737] Trial 231 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:16,479] Trial 232 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:17,307] Trial 233 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:18,162] Trial 234 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:18,990] Trial 235 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:19,829] Trial 236 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:20,649] Trial 237 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:21,630] Trial 238 finished with value: 0.7411764860153198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:22,697] Trial 239 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:23,904] Trial 240 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:25,080] Trial 241 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:26,027] Trial 242 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:26,863] Trial 243 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:27,694] Trial 244 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:28,539] Trial 245 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:29,394] Trial 246 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:30,230] Trial 247 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:31,111] Trial 248 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:31,923] Trial 249 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:32,766] Trial 250 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:33,614] Trial 251 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:34,489] Trial 252 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:35,323] Trial 253 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:36,394] Trial 254 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:37,529] Trial 255 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:38,687] Trial 256 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:39,848] Trial 257 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:40,676] Trial 258 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:41,491] Trial 259 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:42,249] Trial 260 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:43,078] Trial 261 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:43,908] Trial 262 finished with value: 0.9647058844566345 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:44,779] Trial 263 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:45,599] Trial 264 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:46,475] Trial 265 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:47,324] Trial 266 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:48,142] Trial 267 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:48,958] Trial 268 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:49,852] Trial 269 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:50,995] Trial 270 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:52,156] Trial 271 finished with value: 0.7647058963775635 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7647058963775635, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:53,300] Trial 272 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:54,248] Trial 273 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:55,086] Trial 274 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:55,871] Trial 275 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:56,674] Trial 276 finished with value: 0.9764705896377563 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:57,503] Trial 277 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:58,327] Trial 278 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:59,151] Trial 279 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:11:59,959] Trial 280 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:00,827] Trial 281 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:01,676] Trial 282 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:02,464] Trial 283 finished with value: 0.6823529601097107 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:03,285] Trial 284 finished with value: 0.929411768913269 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:04,312] Trial 285 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:05,399] Trial 286 finished with value: 0.8235294222831726 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:06,629] Trial 287 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:07,701] Trial 288 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:08,559] Trial 289 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:09,364] Trial 290 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:10,213] Trial 291 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:11,036] Trial 292 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:11,904] Trial 293 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:12,772] Trial 294 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:13,593] Trial 295 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:14,426] Trial 296 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:15,255] Trial 297 finished with value: 0.9647058844566345 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:16,089] Trial 298 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:16,923] Trial 299 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:17,986] Trial 300 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:19,138] Trial 301 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:20,326] Trial 302 finished with value: 0.9647058844566345 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:21,478] Trial 303 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:22,335] Trial 304 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:23,156] Trial 305 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:23,934] Trial 306 finished with value: 0.6823529601097107 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:24,749] Trial 307 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:25,585] Trial 308 finished with value: 0.800000011920929 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.800000011920929, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:26,409] Trial 309 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:27,223] Trial 310 finished with value: 0.9764705896377563 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:28,032] Trial 311 finished with value: 0.8352941274642944 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:28,860] Trial 312 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:29,681] Trial 313 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:30,531] Trial 314 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:31,413] Trial 315 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:32,498] Trial 316 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:33,635] Trial 317 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:34,852] Trial 318 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:35,886] Trial 319 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:36,732] Trial 320 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:37,567] Trial 321 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:38,416] Trial 322 finished with value: 0.9647058844566345 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:39,239] Trial 323 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:40,055] Trial 324 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:40,890] Trial 325 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:41,693] Trial 326 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:42,512] Trial 327 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:43,331] Trial 328 finished with value: 0.9647058844566345 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:44,115] Trial 329 finished with value: 0.6823529601097107 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:44,943] Trial 330 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:45,921] Trial 331 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:47,005] Trial 332 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:48,238] Trial 333 finished with value: 0.7647058963775635 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7647058963775635, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:49,409] Trial 334 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:50,243] Trial 335 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:51,063] Trial 336 finished with value: 0.8588235378265381 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:51,889] Trial 337 finished with value: 0.9647058844566345 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:52,706] Trial 338 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:53,528] Trial 339 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:54,353] Trial 340 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:55,167] Trial 341 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:55,992] Trial 342 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:57,302] Trial 343 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:58,451] Trial 344 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:12:59,739] Trial 345 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:01,529] Trial 346 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:04,851] Trial 347 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:05,755] Trial 348 finished with value: 0.9647058844566345 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:06,620] Trial 349 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:07,451] Trial 350 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:08,217] Trial 351 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:09,078] Trial 352 finished with value: 0.7411764860153198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:09,901] Trial 353 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:10,743] Trial 354 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:11,555] Trial 355 finished with value: 0.8705882430076599 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:12,359] Trial 356 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:13,193] Trial 357 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:14,038] Trial 358 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:14,855] Trial 359 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:15,930] Trial 360 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:17,079] Trial 361 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:18,357] Trial 362 finished with value: 0.9647058844566345 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:19,215] Trial 363 finished with value: 0.9647058844566345 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:20,083] Trial 364 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:20,908] Trial 365 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:21,731] Trial 366 finished with value: 0.8117647171020508 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:22,583] Trial 367 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:23,411] Trial 368 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:24,236] Trial 369 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:25,082] Trial 370 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:25,887] Trial 371 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:26,738] Trial 372 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:27,520] Trial 373 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:28,350] Trial 374 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:29,430] Trial 375 finished with value: 0.800000011920929 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.800000011920929, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:30,594] Trial 376 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:31,775] Trial 377 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:32,875] Trial 378 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:33,692] Trial 379 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:34,529] Trial 380 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:35,355] Trial 381 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:36,166] Trial 382 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:37,017] Trial 383 finished with value: 0.8705882430076599 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:37,851] Trial 384 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:38,669] Trial 385 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:39,485] Trial 386 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:40,307] Trial 387 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:41,120] Trial 388 finished with value: 0.9764705896377563 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:41,940] Trial 389 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:42,913] Trial 390 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:44,009] Trial 391 finished with value: 0.9764705896377563 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:45,220] Trial 392 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:46,383] Trial 393 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:47,359] Trial 394 finished with value: 0.6941176652908325 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.8095238208770752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:48,179] Trial 395 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:49,001] Trial 396 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:49,815] Trial 397 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:50,627] Trial 398 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:51,450] Trial 399 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:52,269] Trial 400 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:53,107] Trial 401 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:53,904] Trial 402 finished with value: 0.9529411792755127 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:54,786] Trial 403 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:55,620] Trial 404 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:56,436] Trial 405 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:57,382] Trial 406 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:58,428] Trial 407 finished with value: 0.8352941274642944 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:13:59,586] Trial 408 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:00,750] Trial 409 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:01,781] Trial 410 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:02,603] Trial 411 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:03,432] Trial 412 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:04,268] Trial 413 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:05,085] Trial 414 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:05,878] Trial 415 finished with value: 0.9882352948188782 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:06,702] Trial 416 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:07,483] Trial 417 finished with value: 0.6823529601097107 and parameters: {'output_features': 18, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:08,327] Trial 418 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:09,168] Trial 419 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:10,014] Trial 420 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:10,823] Trial 421 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:11,772] Trial 422 finished with value: 0.7647058963775635 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7647058963775635, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:12,844] Trial 423 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:14,064] Trial 424 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:15,291] Trial 425 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:16,132] Trial 426 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:16,976] Trial 427 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:17,791] Trial 428 finished with value: 0.9647058844566345 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:18,617] Trial 429 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:19,454] Trial 430 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:20,269] Trial 431 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:21,087] Trial 432 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:21,912] Trial 433 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:22,727] Trial 434 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:23,558] Trial 435 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:24,380] Trial 436 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:25,187] Trial 437 finished with value: 0.9176470637321472 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:26,253] Trial 438 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:27,451] Trial 439 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:28,641] Trial 440 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:29,754] Trial 441 finished with value: 0.6941176652908325 and parameters: {'output_features': 18, 'optimizer': 'SGD', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:30,584] Trial 442 finished with value: 0.9764705896377563 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:31,441] Trial 443 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:32,324] Trial 444 finished with value: 0.729411780834198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:33,160] Trial 445 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:33,999] Trial 446 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:34,830] Trial 447 finished with value: 0.729411780834198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:35,649] Trial 448 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:36,480] Trial 449 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:37,289] Trial 450 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:38,122] Trial 451 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:38,929] Trial 452 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:39,808] Trial 453 finished with value: 0.9647058844566345 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:40,899] Trial 454 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:42,126] Trial 455 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:43,304] Trial 456 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:44,265] Trial 457 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:45,098] Trial 458 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:45,935] Trial 459 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:46,760] Trial 460 finished with value: 0.9764705896377563 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:47,606] Trial 461 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:48,392] Trial 462 finished with value: 0.6705882549285889 and parameters: {'output_features': 22, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6705882549285889, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:49,229] Trial 463 finished with value: 0.8117647171020508 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:50,053] Trial 464 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:50,861] Trial 465 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:51,697] Trial 466 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:52,531] Trial 467 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:53,358] Trial 468 finished with value: 0.9647058844566345 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:54,368] Trial 469 finished with value: 0.9529411792755127 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:55,498] Trial 470 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:56,676] Trial 471 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:57,881] Trial 472 finished with value: 0.6705882549285889 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6705882549285889, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:58,758] Trial 473 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:14:59,607] Trial 474 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:00,452] Trial 475 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:01,285] Trial 476 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:02,197] Trial 477 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:03,041] Trial 478 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:03,889] Trial 479 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:04,776] Trial 480 finished with value: 0.9647058844566345 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:05,624] Trial 481 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:06,451] Trial 482 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:07,217] Trial 483 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:08,178] Trial 484 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:09,246] Trial 485 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:10,470] Trial 486 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:11,563] Trial 487 finished with value: 0.9764705896377563 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:12,409] Trial 488 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:13,238] Trial 489 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:14,042] Trial 490 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:14,857] Trial 491 finished with value: 0.8352941274642944 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:15,677] Trial 492 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:16,492] Trial 493 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:17,313] Trial 494 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:18,132] Trial 495 finished with value: 0.9882352948188782 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:18,979] Trial 496 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:19,806] Trial 497 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:20,596] Trial 498 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:21,468] Trial 499 finished with value: 0.7882353067398071 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7882353067398071, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:22,525] Trial 500 finished with value: 0.9647058844566345 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:23,656] Trial 501 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:24,819] Trial 502 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:25,665] Trial 503 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:26,473] Trial 504 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:27,259] Trial 505 finished with value: 0.6941176652908325 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:28,066] Trial 506 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:28,883] Trial 507 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:29,689] Trial 508 finished with value: 0.9647058844566345 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:30,502] Trial 509 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:31,320] Trial 510 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:32,161] Trial 511 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:32,942] Trial 512 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:33,741] Trial 513 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:34,568] Trial 514 finished with value: 0.9882352948188782 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:35,581] Trial 515 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:36,793] Trial 516 finished with value: 0.8588235378265381 and parameters: {'output_features': 22, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:38,018] Trial 517 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:39,097] Trial 518 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:39,884] Trial 519 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:40,683] Trial 520 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:41,495] Trial 521 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.1428571492433548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:42,294] Trial 522 finished with value: 0.8941176533699036 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:43,105] Trial 523 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:43,900] Trial 524 finished with value: 0.9882352948188782 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:44,735] Trial 525 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:45,549] Trial 526 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:46,335] Trial 527 finished with value: 0.6941176652908325 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:47,179] Trial 528 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:47,994] Trial 529 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:48,844] Trial 530 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:49,951] Trial 531 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:51,119] Trial 532 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:52,317] Trial 533 finished with value: 0.7764706015586853 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7764706015586853, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:53,330] Trial 534 finished with value: 0.9176470637321472 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:54,140] Trial 535 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:54,988] Trial 536 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:55,810] Trial 537 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:56,609] Trial 538 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:57,426] Trial 539 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:58,270] Trial 540 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:59,076] Trial 541 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:15:59,864] Trial 542 finished with value: 0.9764705896377563 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:00,680] Trial 543 finished with value: 0.9882352948188782 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:01,533] Trial 544 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:02,385] Trial 545 finished with value: 0.7764706015586853 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7764706015586853, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:03,406] Trial 546 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:04,439] Trial 547 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:05,622] Trial 548 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:06,659] Trial 549 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:07,469] Trial 550 finished with value: 0.9764705896377563 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:08,299] Trial 551 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:09,152] Trial 552 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:09,964] Trial 553 finished with value: 0.9764705896377563 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:10,822] Trial 554 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:11,620] Trial 555 finished with value: 0.800000011920929 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.800000011920929, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:12,448] Trial 556 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:13,258] Trial 557 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:14,061] Trial 558 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:14,857] Trial 559 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:15,705] Trial 560 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:16,578] Trial 561 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:17,663] Trial 562 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:18,898] Trial 563 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:20,067] Trial 564 finished with value: 0.9529411792755127 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:20,867] Trial 565 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:21,670] Trial 566 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:22,479] Trial 567 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:23,270] Trial 568 finished with value: 0.9647058844566345 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:24,058] Trial 569 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:24,891] Trial 570 finished with value: 0.9882352948188782 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:25,654] Trial 571 finished with value: 0.6823529601097107 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:26,540] Trial 572 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:27,417] Trial 573 finished with value: 0.7647058963775635 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7647058963775635, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:28,243] Trial 574 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:29,068] Trial 575 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:29,880] Trial 576 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:31,005] Trial 577 finished with value: 0.8352941274642944 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:32,197] Trial 578 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:33,403] Trial 579 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:34,243] Trial 580 finished with value: 0.9882352948188782 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:35,044] Trial 581 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:35,860] Trial 582 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:36,668] Trial 583 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:37,519] Trial 584 finished with value: 0.7882353067398071 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7882353067398071, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:38,351] Trial 585 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:39,178] Trial 586 finished with value: 0.8117647171020508 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:39,986] Trial 587 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:40,793] Trial 588 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:41,611] Trial 589 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:42,440] Trial 590 finished with value: 0.9764705896377563 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:43,262] Trial 591 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:44,366] Trial 592 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:45,457] Trial 593 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:46,614] Trial 594 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:47,636] Trial 595 finished with value: 0.8117647171020508 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:48,466] Trial 596 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:49,267] Trial 597 finished with value: 0.9176470637321472 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:50,061] Trial 598 finished with value: 0.9882352948188782 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:50,865] Trial 599 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:51,665] Trial 600 finished with value: 0.7058823704719543 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:52,502] Trial 601 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:53,327] Trial 602 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:54,134] Trial 603 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:54,958] Trial 604 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:55,783] Trial 605 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:56,614] Trial 606 finished with value: 0.9764705896377563 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:57,538] Trial 607 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:58,614] Trial 608 finished with value: 0.9411764740943909 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:16:59,832] Trial 609 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:00,983] Trial 610 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:01,818] Trial 611 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:02,646] Trial 612 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:03,452] Trial 613 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:04,264] Trial 614 finished with value: 0.7882353067398071 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7882353067398071, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:05,069] Trial 615 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:05,835] Trial 616 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:06,659] Trial 617 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:07,484] Trial 618 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:08,289] Trial 619 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:09,087] Trial 620 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:09,902] Trial 621 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:10,710] Trial 622 finished with value: 0.9764705896377563 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:11,763] Trial 623 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:12,895] Trial 624 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:14,053] Trial 625 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:15,135] Trial 626 finished with value: 0.8588235378265381 and parameters: {'output_features': 22, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:15,998] Trial 627 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:16,831] Trial 628 finished with value: 0.7647058963775635 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7647058963775635, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:17,666] Trial 629 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:18,476] Trial 630 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:19,263] Trial 631 finished with value: 0.8352941274642944 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:20,103] Trial 632 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:20,911] Trial 633 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:21,719] Trial 634 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:22,529] Trial 635 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:23,348] Trial 636 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:24,118] Trial 637 finished with value: 0.7058823704719543 and parameters: {'output_features': 16, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:25,058] Trial 638 finished with value: 0.8352941274642944 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:26,137] Trial 639 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:27,349] Trial 640 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:28,442] Trial 641 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:29,296] Trial 642 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:30,213] Trial 643 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:31,057] Trial 644 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:31,874] Trial 645 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:32,729] Trial 646 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:33,557] Trial 647 finished with value: 0.7882353067398071 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7882353067398071, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:34,386] Trial 648 finished with value: 0.9647058844566345 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:35,210] Trial 649 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:36,022] Trial 650 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:36,867] Trial 651 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:37,691] Trial 652 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:38,668] Trial 653 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:39,740] Trial 654 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:40,928] Trial 655 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:42,028] Trial 656 finished with value: 0.9411764740943909 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:42,824] Trial 657 finished with value: 0.9411764740943909 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:43,664] Trial 658 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:44,424] Trial 659 finished with value: 0.7764706015586853 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7764706015586853, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:45,248] Trial 660 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:46,092] Trial 661 finished with value: 0.8117647171020508 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:46,937] Trial 662 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:47,803] Trial 663 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:48,602] Trial 664 finished with value: 0.9764705896377563 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:49,405] Trial 665 finished with value: 0.7647058963775635 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7647058963775635, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:50,229] Trial 666 finished with value: 0.9058823585510254 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:51,031] Trial 667 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:51,869] Trial 668 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:52,922] Trial 669 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:54,049] Trial 670 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:55,215] Trial 671 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:56,165] Trial 672 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:56,958] Trial 673 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:57,753] Trial 674 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:58,553] Trial 675 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:17:59,361] Trial 676 finished with value: 0.9764705896377563 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:00,170] Trial 677 finished with value: 0.9764705896377563 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:00,973] Trial 678 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:01,864] Trial 679 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:02,721] Trial 680 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:03,512] Trial 681 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:04,351] Trial 682 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:05,164] Trial 683 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:06,144] Trial 684 finished with value: 0.9647058844566345 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:07,185] Trial 685 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:08,436] Trial 686 finished with value: 0.6705882549285889 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6705882549285889, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:09,630] Trial 687 finished with value: 0.7176470756530762 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:10,442] Trial 688 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:11,248] Trial 689 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:12,059] Trial 690 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:12,884] Trial 691 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:13,670] Trial 692 finished with value: 0.8352941274642944 and parameters: {'output_features': 10, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:14,483] Trial 693 finished with value: 0.8352941274642944 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:15,295] Trial 694 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:16,116] Trial 695 finished with value: 0.9764705896377563 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:16,945] Trial 696 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:17,803] Trial 697 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:18,710] Trial 698 finished with value: 0.6823529601097107 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:19,547] Trial 699 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:20,641] Trial 700 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:21,751] Trial 701 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:22,964] Trial 702 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:23,735] Trial 703 finished with value: 0.6941176652908325 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:24,545] Trial 704 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:25,360] Trial 705 finished with value: 0.800000011920929 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.800000011920929, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:26,187] Trial 706 finished with value: 0.9764705896377563 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:27,011] Trial 707 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:27,838] Trial 708 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:28,667] Trial 709 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:29,525] Trial 710 finished with value: 0.7411764860153198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:30,383] Trial 711 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:31,209] Trial 712 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:32,019] Trial 713 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:32,926] Trial 714 finished with value: 0.8823529481887817 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:34,125] Trial 715 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:35,351] Trial 716 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:36,577] Trial 717 finished with value: 0.9882352948188782 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:37,414] Trial 718 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:38,281] Trial 719 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:39,144] Trial 720 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:39,990] Trial 721 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:40,854] Trial 722 finished with value: 0.9882352948188782 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:41,665] Trial 723 finished with value: 0.9764705896377563 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:42,486] Trial 724 finished with value: 0.8117647171020508 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:43,326] Trial 725 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:44,175] Trial 726 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:44,966] Trial 727 finished with value: 0.6823529601097107 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:45,827] Trial 728 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:46,742] Trial 729 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:47,851] Trial 730 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:49,082] Trial 731 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:50,293] Trial 732 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:51,249] Trial 733 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:52,085] Trial 734 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:52,910] Trial 735 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:53,737] Trial 736 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:54,578] Trial 737 finished with value: 0.9882352948188782 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:55,414] Trial 738 finished with value: 0.7529411911964417 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7529411911964417, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:56,260] Trial 739 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:57,102] Trial 740 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:57,932] Trial 741 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:58,760] Trial 742 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:18:59,594] Trial 743 finished with value: 0.9882352948188782 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:00,452] Trial 744 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:01,506] Trial 745 finished with value: 0.9764705896377563 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:02,599] Trial 746 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:03,823] Trial 747 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:04,957] Trial 748 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:05,848] Trial 749 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:06,678] Trial 750 finished with value: 0.7764706015586853 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7764706015586853, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:07,501] Trial 751 finished with value: 0.6823529601097107 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:08,329] Trial 752 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:09,173] Trial 753 finished with value: 0.8588235378265381 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:09,989] Trial 754 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:10,782] Trial 755 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:11,584] Trial 756 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:12,373] Trial 757 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:13,193] Trial 758 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:13,995] Trial 759 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:14,844] Trial 760 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:15,915] Trial 761 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:17,015] Trial 762 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:18,184] Trial 763 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:19,283] Trial 764 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:20,114] Trial 765 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:20,919] Trial 766 finished with value: 0.9764705896377563 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:21,723] Trial 767 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:22,555] Trial 768 finished with value: 0.9764705896377563 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:23,387] Trial 769 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:24,154] Trial 770 finished with value: 0.6705882549285889 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6705882549285889, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:25,011] Trial 771 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:25,848] Trial 772 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:26,678] Trial 773 finished with value: 0.9764705896377563 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:27,497] Trial 774 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:28,329] Trial 775 finished with value: 0.9764705896377563 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:29,230] Trial 776 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:30,446] Trial 777 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:31,641] Trial 778 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:32,729] Trial 779 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:33,620] Trial 780 finished with value: 0.9882352948188782 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:34,459] Trial 781 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:35,283] Trial 782 finished with value: 0.9529411792755127 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:36,140] Trial 783 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:36,949] Trial 784 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:37,762] Trial 785 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:38,638] Trial 786 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:39,475] Trial 787 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:40,297] Trial 788 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:41,105] Trial 789 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:41,916] Trial 790 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:42,811] Trial 791 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:43,881] Trial 792 finished with value: 0.6823529601097107 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:45,116] Trial 793 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:46,343] Trial 794 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:47,170] Trial 795 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:48,037] Trial 796 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:48,879] Trial 797 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:49,701] Trial 798 finished with value: 0.729411780834198 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:50,526] Trial 799 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:51,341] Trial 800 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:52,164] Trial 801 finished with value: 0.8470588326454163 and parameters: {'output_features': 22, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:52,978] Trial 802 finished with value: 0.9647058844566345 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:53,823] Trial 803 finished with value: 0.6941176652908325 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:54,638] Trial 804 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:55,494] Trial 805 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:56,323] Trial 806 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:57,457] Trial 807 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:58,653] Trial 808 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:19:59,846] Trial 809 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:00,971] Trial 810 finished with value: 0.9882352948188782 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:01,809] Trial 811 finished with value: 0.9882352948188782 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:02,684] Trial 812 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:03,527] Trial 813 finished with value: 0.6941176652908325 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:04,389] Trial 814 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:05,232] Trial 815 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:06,059] Trial 816 finished with value: 0.8117647171020508 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:06,932] Trial 817 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:07,774] Trial 818 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:08,626] Trial 819 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:09,510] Trial 820 finished with value: 0.7411764860153198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:10,341] Trial 821 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:11,435] Trial 822 finished with value: 0.9764705896377563 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:12,538] Trial 823 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:13,786] Trial 824 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:14,877] Trial 825 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:15,784] Trial 826 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:16,625] Trial 827 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:17,502] Trial 828 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:18,359] Trial 829 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:19,216] Trial 830 finished with value: 0.9647058844566345 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:20,053] Trial 831 finished with value: 0.9764705896377563 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:20,883] Trial 832 finished with value: 0.8235294222831726 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:21,711] Trial 833 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:22,568] Trial 834 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:23,362] Trial 835 finished with value: 0.6470588445663452 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:24,209] Trial 836 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:25,329] Trial 837 finished with value: 0.8117647171020508 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:26,481] Trial 838 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:27,716] Trial 839 finished with value: 0.9529411792755127 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:28,667] Trial 840 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:29,505] Trial 841 finished with value: 0.8235294222831726 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:30,380] Trial 842 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:31,228] Trial 843 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:32,073] Trial 844 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:32,949] Trial 845 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:33,850] Trial 846 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:34,692] Trial 847 finished with value: 0.9764705896377563 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:35,548] Trial 848 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:36,445] Trial 849 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:37,317] Trial 850 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:38,265] Trial 851 finished with value: 0.9764705896377563 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:39,399] Trial 852 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:40,672] Trial 853 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:41,777] Trial 854 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:42,643] Trial 855 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:43,493] Trial 856 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:44,283] Trial 857 finished with value: 0.6941176652908325 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:45,125] Trial 858 finished with value: 0.9882352948188782 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:46,033] Trial 859 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:46,881] Trial 860 finished with value: 0.9647058844566345 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:47,701] Trial 861 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:48,541] Trial 862 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:49,424] Trial 863 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:50,258] Trial 864 finished with value: 0.7176470756530762 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:51,099] Trial 865 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:52,151] Trial 866 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:53,240] Trial 867 finished with value: 0.800000011920929 and parameters: {'output_features': 18, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.800000011920929, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:54,534] Trial 868 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:55,546] Trial 869 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:56,419] Trial 870 finished with value: 0.9647058844566345 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:57,291] Trial 871 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:58,128] Trial 872 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:58,972] Trial 873 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:20:59,815] Trial 874 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:00,705] Trial 875 finished with value: 0.800000011920929 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.800000011920929, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:01,565] Trial 876 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:02,413] Trial 877 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:03,226] Trial 878 finished with value: 0.9764705896377563 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:04,130] Trial 879 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:04,972] Trial 880 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:06,205] Trial 881 finished with value: 0.8352941274642944 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:07,306] Trial 882 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:08,481] Trial 883 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:09,599] Trial 884 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:10,460] Trial 885 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:11,362] Trial 886 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:12,203] Trial 887 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:13,018] Trial 888 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:13,832] Trial 889 finished with value: 0.9411764740943909 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:14,677] Trial 890 finished with value: 0.8352941274642944 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:15,532] Trial 891 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:16,421] Trial 892 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:17,296] Trial 893 finished with value: 0.9764705896377563 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:18,138] Trial 894 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:18,981] Trial 895 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:20,035] Trial 896 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:21,127] Trial 897 finished with value: 0.9764705896377563 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:22,439] Trial 898 finished with value: 0.9529411792755127 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:23,654] Trial 899 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:24,478] Trial 900 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:25,291] Trial 901 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:26,136] Trial 902 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:26,991] Trial 903 finished with value: 0.729411780834198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:27,813] Trial 904 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:28,684] Trial 905 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:29,504] Trial 906 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:30,337] Trial 907 finished with value: 0.929411768913269 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:31,188] Trial 908 finished with value: 0.8117647171020508 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:32,025] Trial 909 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:32,828] Trial 910 finished with value: 0.8705882430076599 and parameters: {'output_features': 14, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:33,691] Trial 911 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:34,840] Trial 912 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:36,024] Trial 913 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:37,315] Trial 914 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:38,327] Trial 915 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:39,162] Trial 916 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:39,968] Trial 917 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:40,793] Trial 918 finished with value: 0.9764705896377563 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:41,597] Trial 919 finished with value: 0.9647058844566345 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:42,421] Trial 920 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:43,256] Trial 921 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:44,017] Trial 922 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:44,875] Trial 923 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:45,735] Trial 924 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:46,610] Trial 925 finished with value: 0.929411768913269 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:47,449] Trial 926 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:48,478] Trial 927 finished with value: 0.9764705896377563 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:49,598] Trial 928 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:50,820] Trial 929 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:51,944] Trial 930 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:52,769] Trial 931 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:53,591] Trial 932 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:54,435] Trial 933 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:55,266] Trial 934 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:56,092] Trial 935 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:56,944] Trial 936 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:57,779] Trial 937 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:58,588] Trial 938 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:21:59,412] Trial 939 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:00,254] Trial 940 finished with value: 0.9764705896377563 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:01,104] Trial 941 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:02,036] Trial 942 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:03,148] Trial 943 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:04,305] Trial 944 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:05,491] Trial 945 finished with value: 0.8117647171020508 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:06,390] Trial 946 finished with value: 0.800000011920929 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.800000011920929, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:07,253] Trial 947 finished with value: 0.9764705896377563 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:08,093] Trial 948 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:08,917] Trial 949 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:09,757] Trial 950 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:10,584] Trial 951 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:11,488] Trial 952 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:12,331] Trial 953 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:13,130] Trial 954 finished with value: 0.8352941274642944 and parameters: {'output_features': 10, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:13,972] Trial 955 finished with value: 0.9764705896377563 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:14,800] Trial 956 finished with value: 0.9647058844566345 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:15,672] Trial 957 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:16,809] Trial 958 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:17,949] Trial 959 finished with value: 0.8117647171020508 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:19,106] Trial 960 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:20,237] Trial 961 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:21,077] Trial 962 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:21,933] Trial 963 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:22,758] Trial 964 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:23,610] Trial 965 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:24,449] Trial 966 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:25,241] Trial 967 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:26,055] Trial 968 finished with value: 0.9882352948188782 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:26,917] Trial 969 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:27,729] Trial 970 finished with value: 0.9764705896377563 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:28,575] Trial 971 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:29,424] Trial 972 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:30,332] Trial 973 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:31,396] Trial 974 finished with value: 0.9529411792755127 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:32,639] Trial 975 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:33,837] Trial 976 finished with value: 0.9764705896377563 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:34,847] Trial 977 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:35,657] Trial 978 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:36,472] Trial 979 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:37,318] Trial 980 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:38,148] Trial 981 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:38,981] Trial 982 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:39,798] Trial 983 finished with value: 0.9764705896377563 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:40,634] Trial 984 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:41,466] Trial 985 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:42,343] Trial 986 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:43,184] Trial 987 finished with value: 0.9764705896377563 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:43,987] Trial 988 finished with value: 0.6823529601097107 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:44,988] Trial 989 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:46,132] Trial 990 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:47,340] Trial 991 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:48,554] Trial 992 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:49,394] Trial 993 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:50,251] Trial 994 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:51,122] Trial 995 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:51,976] Trial 996 finished with value: 0.7058823704719543 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:52,807] Trial 997 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:53,646] Trial 998 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:54,533] Trial 999 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:55,382] Trial 1000 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:56,210] Trial 1001 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:57,041] Trial 1002 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:57,916] Trial 1003 finished with value: 0.9647058844566345 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:58,866] Trial 1004 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:22:59,979] Trial 1005 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:01,235] Trial 1006 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:02,380] Trial 1007 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:03,211] Trial 1008 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:04,083] Trial 1009 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:04,903] Trial 1010 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:05,747] Trial 1011 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:06,755] Trial 1012 finished with value: 0.800000011920929 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.800000011920929, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:07,785] Trial 1013 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:08,795] Trial 1014 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:09,783] Trial 1015 finished with value: 0.9529411792755127 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:10,768] Trial 1016 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:11,866] Trial 1017 finished with value: 0.7176470756530762 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:14,253] Trial 1018 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:16,311] Trial 1019 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:17,493] Trial 1020 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:18,474] Trial 1021 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:19,340] Trial 1022 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:20,192] Trial 1023 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:21,025] Trial 1024 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:21,860] Trial 1025 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:22,689] Trial 1026 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:23,556] Trial 1027 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:24,385] Trial 1028 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:25,205] Trial 1029 finished with value: 0.9764705896377563 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:26,056] Trial 1030 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:26,888] Trial 1031 finished with value: 0.9882352948188782 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:27,674] Trial 1032 finished with value: 0.6705882549285889 and parameters: {'output_features': 12, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6705882549285889, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:28,743] Trial 1033 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:29,884] Trial 1034 finished with value: 0.8117647171020508 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:31,101] Trial 1035 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:32,165] Trial 1036 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:33,065] Trial 1037 finished with value: 0.9764705896377563 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:33,903] Trial 1038 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:34,760] Trial 1039 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:35,594] Trial 1040 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:36,424] Trial 1041 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:37,248] Trial 1042 finished with value: 0.8235294222831726 and parameters: {'output_features': 18, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:38,168] Trial 1043 finished with value: 0.9882352948188782 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:38,996] Trial 1044 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:39,835] Trial 1045 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:40,675] Trial 1046 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:41,522] Trial 1047 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:42,593] Trial 1048 finished with value: 0.9882352948188782 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:43,732] Trial 1049 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:44,941] Trial 1050 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:46,105] Trial 1051 finished with value: 0.7882353067398071 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7882353067398071, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:46,942] Trial 1052 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:47,791] Trial 1053 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:48,573] Trial 1054 finished with value: 0.6941176652908325 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:49,402] Trial 1055 finished with value: 0.8823529481887817 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:50,265] Trial 1056 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:51,104] Trial 1057 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:51,973] Trial 1058 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:52,814] Trial 1059 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:53,633] Trial 1060 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:54,478] Trial 1061 finished with value: 0.8352941274642944 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:55,315] Trial 1062 finished with value: 0.9411764740943909 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:56,225] Trial 1063 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:57,362] Trial 1064 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:58,563] Trial 1065 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:23:59,746] Trial 1066 finished with value: 0.9647058844566345 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:00,800] Trial 1067 finished with value: 0.7411764860153198 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:01,643] Trial 1068 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:02,495] Trial 1069 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:03,341] Trial 1070 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:04,176] Trial 1071 finished with value: 0.9529411792755127 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:05,076] Trial 1072 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:05,951] Trial 1073 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:06,772] Trial 1074 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:07,610] Trial 1075 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:08,411] Trial 1076 finished with value: 0.6941176652908325 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:09,242] Trial 1077 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:10,114] Trial 1078 finished with value: 0.800000011920929 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.800000011920929, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:11,136] Trial 1079 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:12,266] Trial 1080 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:13,457] Trial 1081 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:14,674] Trial 1082 finished with value: 0.800000011920929 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.800000011920929, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:15,570] Trial 1083 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:16,432] Trial 1084 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:17,262] Trial 1085 finished with value: 0.9882352948188782 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:18,069] Trial 1086 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:18,937] Trial 1087 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:19,805] Trial 1088 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:20,676] Trial 1089 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:21,508] Trial 1090 finished with value: 0.9764705896377563 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:22,350] Trial 1091 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:23,200] Trial 1092 finished with value: 0.9764705896377563 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:24,038] Trial 1093 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:24,945] Trial 1094 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:26,050] Trial 1095 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:27,286] Trial 1096 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:28,515] Trial 1097 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:29,513] Trial 1098 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:30,372] Trial 1099 finished with value: 0.8470588326454163 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:31,209] Trial 1100 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:32,040] Trial 1101 finished with value: 0.9764705896377563 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:32,891] Trial 1102 finished with value: 0.729411780834198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:33,729] Trial 1103 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:34,661] Trial 1104 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:35,519] Trial 1105 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:36,353] Trial 1106 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:37,209] Trial 1107 finished with value: 0.800000011920929 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.800000011920929, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:38,094] Trial 1108 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:38,946] Trial 1109 finished with value: 0.9411764740943909 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:40,084] Trial 1110 finished with value: 0.7764706015586853 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7764706015586853, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:41,272] Trial 1111 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:42,584] Trial 1112 finished with value: 0.929411768913269 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:43,423] Trial 1113 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:44,246] Trial 1114 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:45,085] Trial 1115 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:45,953] Trial 1116 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:46,776] Trial 1117 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:47,618] Trial 1118 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:48,409] Trial 1119 finished with value: 0.6941176652908325 and parameters: {'output_features': 12, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:49,260] Trial 1120 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:50,131] Trial 1121 finished with value: 0.8235294222831726 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:50,948] Trial 1122 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:51,792] Trial 1123 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:52,676] Trial 1124 finished with value: 0.7882353067398071 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7882353067398071, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:53,803] Trial 1125 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:55,032] Trial 1126 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:56,210] Trial 1127 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:57,277] Trial 1128 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:58,116] Trial 1129 finished with value: 0.7529411911964417 and parameters: {'output_features': 18, 'optimizer': 'RMSprop', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7529411911964417, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:58,957] Trial 1130 finished with value: 0.9647058844566345 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:24:59,806] Trial 1131 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:00,651] Trial 1132 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:01,524] Trial 1133 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:02,374] Trial 1134 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:03,208] Trial 1135 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:04,046] Trial 1136 finished with value: 0.7882353067398071 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7882353067398071, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:04,928] Trial 1137 finished with value: 0.800000011920929 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.800000011920929, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:05,738] Trial 1138 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:06,585] Trial 1139 finished with value: 0.9764705896377563 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:07,690] Trial 1140 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:08,796] Trial 1141 finished with value: 0.9647058844566345 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:09,957] Trial 1142 finished with value: 0.6823529601097107 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:11,076] Trial 1143 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:11,902] Trial 1144 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:12,762] Trial 1145 finished with value: 0.8117647171020508 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:13,626] Trial 1146 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:14,462] Trial 1147 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:15,320] Trial 1148 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:16,153] Trial 1149 finished with value: 0.9647058844566345 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:16,998] Trial 1150 finished with value: 0.9647058844566345 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:17,900] Trial 1151 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:18,778] Trial 1152 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:19,669] Trial 1153 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:20,533] Trial 1154 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:21,585] Trial 1155 finished with value: 0.8823529481887817 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:22,716] Trial 1156 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:24,010] Trial 1157 finished with value: 0.9764705896377563 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:24,884] Trial 1158 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:25,715] Trial 1159 finished with value: 0.9882352948188782 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:26,551] Trial 1160 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:27,394] Trial 1161 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:28,248] Trial 1162 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:29,047] Trial 1163 finished with value: 0.6941176652908325 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:29,932] Trial 1164 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:30,791] Trial 1165 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:31,643] Trial 1166 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:32,500] Trial 1167 finished with value: 0.8117647171020508 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:33,367] Trial 1168 finished with value: 0.8117647171020508 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:34,264] Trial 1169 finished with value: 0.8470588326454163 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:35,488] Trial 1170 finished with value: 0.9176470637321472 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:36,767] Trial 1171 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:37,961] Trial 1172 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:38,820] Trial 1173 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:39,664] Trial 1174 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:40,542] Trial 1175 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:41,418] Trial 1176 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:42,260] Trial 1177 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:43,076] Trial 1178 finished with value: 0.8352941274642944 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:43,923] Trial 1179 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:44,749] Trial 1180 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:45,599] Trial 1181 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:46,451] Trial 1182 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:47,282] Trial 1183 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:48,179] Trial 1184 finished with value: 0.9529411792755127 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:49,243] Trial 1185 finished with value: 0.7764706015586853 and parameters: {'output_features': 22, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7764706015586853, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:50,484] Trial 1186 finished with value: 0.9647058844566345 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:51,716] Trial 1187 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:52,702] Trial 1188 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:53,524] Trial 1189 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:54,347] Trial 1190 finished with value: 0.9764705896377563 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:55,227] Trial 1191 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:56,080] Trial 1192 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:56,932] Trial 1193 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:57,764] Trial 1194 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:58,588] Trial 1195 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:25:59,436] Trial 1196 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:00,303] Trial 1197 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:01,181] Trial 1198 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:02,045] Trial 1199 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:03,187] Trial 1200 finished with value: 0.9411764740943909 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:04,342] Trial 1201 finished with value: 0.9882352948188782 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:05,517] Trial 1202 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:06,690] Trial 1203 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:07,540] Trial 1204 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:08,388] Trial 1205 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:09,230] Trial 1206 finished with value: 0.9647058844566345 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:10,101] Trial 1207 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:10,908] Trial 1208 finished with value: 0.6470588445663452 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:11,764] Trial 1209 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:12,625] Trial 1210 finished with value: 0.729411780834198 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:13,476] Trial 1211 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:14,310] Trial 1212 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:15,180] Trial 1213 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:16,044] Trial 1214 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:17,016] Trial 1215 finished with value: 0.9764705896377563 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:18,136] Trial 1216 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:19,342] Trial 1217 finished with value: 0.8470588326454163 and parameters: {'output_features': 10, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:20,606] Trial 1218 finished with value: 0.9411764740943909 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:21,453] Trial 1219 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:22,305] Trial 1220 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:23,157] Trial 1221 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:23,985] Trial 1222 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:24,848] Trial 1223 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:25,673] Trial 1224 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:26,520] Trial 1225 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:27,361] Trial 1226 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:28,237] Trial 1227 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:29,061] Trial 1228 finished with value: 0.9764705896377563 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:29,931] Trial 1229 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:30,842] Trial 1230 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:31,953] Trial 1231 finished with value: 0.8117647171020508 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:33,187] Trial 1232 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:34,423] Trial 1233 finished with value: 0.7882353067398071 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7882353067398071, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:35,435] Trial 1234 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:36,303] Trial 1235 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:37,167] Trial 1236 finished with value: 0.9882352948188782 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:37,998] Trial 1237 finished with value: 0.8705882430076599 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:38,842] Trial 1238 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:39,666] Trial 1239 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:40,520] Trial 1240 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:41,389] Trial 1241 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:42,262] Trial 1242 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:43,129] Trial 1243 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:43,974] Trial 1244 finished with value: 0.9411764740943909 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:44,804] Trial 1245 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:45,964] Trial 1246 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:47,102] Trial 1247 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:48,326] Trial 1248 finished with value: 0.9764705896377563 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:49,423] Trial 1249 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:50,335] Trial 1250 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:51,177] Trial 1251 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:51,990] Trial 1252 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:52,871] Trial 1253 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:53,732] Trial 1254 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:54,581] Trial 1255 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:55,434] Trial 1256 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:56,274] Trial 1257 finished with value: 0.8352941274642944 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:57,179] Trial 1258 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:58,032] Trial 1259 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:58,891] Trial 1260 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:26:59,981] Trial 1261 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:01,136] Trial 1262 finished with value: 0.8705882430076599 and parameters: {'output_features': 10, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:02,398] Trial 1263 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:03,279] Trial 1264 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:04,112] Trial 1265 finished with value: 0.9882352948188782 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:04,982] Trial 1266 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:05,851] Trial 1267 finished with value: 0.9411764740943909 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:06,706] Trial 1268 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:07,590] Trial 1269 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:08,436] Trial 1270 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:09,318] Trial 1271 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:10,165] Trial 1272 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:10,976] Trial 1273 finished with value: 0.6470588445663452 and parameters: {'output_features': 18, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:11,831] Trial 1274 finished with value: 0.8117647171020508 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:12,757] Trial 1275 finished with value: 0.9764705896377563 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:13,859] Trial 1276 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:15,101] Trial 1277 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:16,329] Trial 1278 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:17,310] Trial 1279 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:18,148] Trial 1280 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:19,082] Trial 1281 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:19,915] Trial 1282 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:20,790] Trial 1283 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:21,672] Trial 1284 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:22,518] Trial 1285 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:23,353] Trial 1286 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:24,228] Trial 1287 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:25,139] Trial 1288 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:25,987] Trial 1289 finished with value: 0.8470588326454163 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:26,878] Trial 1290 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:28,023] Trial 1291 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:29,279] Trial 1292 finished with value: 0.9411764740943909 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:30,430] Trial 1293 finished with value: 0.8470588326454163 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:31,282] Trial 1294 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:32,198] Trial 1295 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:33,063] Trial 1296 finished with value: 0.8352941274642944 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:33,927] Trial 1297 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:34,899] Trial 1298 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:35,829] Trial 1299 finished with value: 0.7882353067398071 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7882353067398071, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:36,686] Trial 1300 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:37,570] Trial 1301 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:38,431] Trial 1302 finished with value: 0.9764705896377563 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:39,326] Trial 1303 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:40,179] Trial 1304 finished with value: 0.8823529481887817 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:41,325] Trial 1305 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:42,561] Trial 1306 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:43,775] Trial 1307 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:44,633] Trial 1308 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:45,487] Trial 1309 finished with value: 0.9882352948188782 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:46,339] Trial 1310 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:47,198] Trial 1311 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:48,075] Trial 1312 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:48,925] Trial 1313 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:49,771] Trial 1314 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:50,612] Trial 1315 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:51,409] Trial 1316 finished with value: 0.6823529601097107 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:52,264] Trial 1317 finished with value: 0.9647058844566345 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:53,170] Trial 1318 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:54,138] Trial 1319 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:55,253] Trial 1320 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:56,588] Trial 1321 finished with value: 0.9647058844566345 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:57,648] Trial 1322 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:58,499] Trial 1323 finished with value: 0.9764705896377563 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:27:59,361] Trial 1324 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:00,211] Trial 1325 finished with value: 0.9764705896377563 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:01,032] Trial 1326 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:01,932] Trial 1327 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:02,792] Trial 1328 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:03,647] Trial 1329 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:04,499] Trial 1330 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:05,384] Trial 1331 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:06,242] Trial 1332 finished with value: 0.9647058844566345 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:07,093] Trial 1333 finished with value: 0.9882352948188782 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:08,261] Trial 1334 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:09,473] Trial 1335 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:10,680] Trial 1336 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:11,740] Trial 1337 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:12,604] Trial 1338 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:13,480] Trial 1339 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:14,328] Trial 1340 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:15,164] Trial 1341 finished with value: 0.6823529601097107 and parameters: {'output_features': 10, 'optimizer': 'SGD', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:16,052] Trial 1342 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:16,953] Trial 1343 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:17,846] Trial 1344 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:18,714] Trial 1345 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:19,592] Trial 1346 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:20,449] Trial 1347 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:21,394] Trial 1348 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:22,583] Trial 1349 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:23,879] Trial 1350 finished with value: 0.8117647171020508 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:25,093] Trial 1351 finished with value: 0.9764705896377563 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:25,958] Trial 1352 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:26,838] Trial 1353 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:27,707] Trial 1354 finished with value: 0.9882352948188782 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:28,551] Trial 1355 finished with value: 0.7764706015586853 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7764706015586853, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:29,393] Trial 1356 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:30,249] Trial 1357 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:31,083] Trial 1358 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:31,944] Trial 1359 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:32,799] Trial 1360 finished with value: 0.9647058844566345 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:33,608] Trial 1361 finished with value: 0.6941176652908325 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:34,443] Trial 1362 finished with value: 0.9529411792755127 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:35,528] Trial 1363 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:36,685] Trial 1364 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:37,891] Trial 1365 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:39,184] Trial 1366 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:40,121] Trial 1367 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:40,971] Trial 1368 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:41,818] Trial 1369 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:42,641] Trial 1370 finished with value: 0.8588235378265381 and parameters: {'output_features': 10, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:43,492] Trial 1371 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:44,503] Trial 1372 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:45,362] Trial 1373 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:46,313] Trial 1374 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:47,154] Trial 1375 finished with value: 0.8470588326454163 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:48,014] Trial 1376 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:48,848] Trial 1377 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:49,866] Trial 1378 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:51,049] Trial 1379 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:52,314] Trial 1380 finished with value: 0.9764705896377563 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:53,466] Trial 1381 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:54,319] Trial 1382 finished with value: 0.9647058844566345 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:55,115] Trial 1383 finished with value: 0.6470588445663452 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:55,997] Trial 1384 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:56,834] Trial 1385 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:57,692] Trial 1386 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:58,544] Trial 1387 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:28:59,384] Trial 1388 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:00,230] Trial 1389 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:01,084] Trial 1390 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:01,957] Trial 1391 finished with value: 0.9647058844566345 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:02,801] Trial 1392 finished with value: 0.8235294222831726 and parameters: {'output_features': 16, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:03,874] Trial 1393 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:05,044] Trial 1394 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:06,340] Trial 1395 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:07,340] Trial 1396 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:08,197] Trial 1397 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:09,033] Trial 1398 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:09,884] Trial 1399 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:10,726] Trial 1400 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:11,563] Trial 1401 finished with value: 0.9529411792755127 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:12,445] Trial 1402 finished with value: 0.8823529481887817 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:13,319] Trial 1403 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:14,113] Trial 1404 finished with value: 0.7529411911964417 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7529411911964417, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:14,961] Trial 1405 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:15,836] Trial 1406 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:16,744] Trial 1407 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:17,841] Trial 1408 finished with value: 0.9647058844566345 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:19,117] Trial 1409 finished with value: 0.8117647171020508 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:20,387] Trial 1410 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:21,501] Trial 1411 finished with value: 0.9882352948188782 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:22,347] Trial 1412 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:23,226] Trial 1413 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:24,090] Trial 1414 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:24,934] Trial 1415 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:25,775] Trial 1416 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:26,625] Trial 1417 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:27,478] Trial 1418 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:28,315] Trial 1419 finished with value: 0.9882352948188782 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:29,148] Trial 1420 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:30,004] Trial 1421 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:30,854] Trial 1422 finished with value: 0.9764705896377563 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:31,968] Trial 1423 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:33,153] Trial 1424 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:34,359] Trial 1425 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:35,466] Trial 1426 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:36,340] Trial 1427 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:37,192] Trial 1428 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:38,072] Trial 1429 finished with value: 0.8117647171020508 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:38,921] Trial 1430 finished with value: 0.9882352948188782 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:39,767] Trial 1431 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:40,619] Trial 1432 finished with value: 0.7882353067398071 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7882353067398071, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:41,450] Trial 1433 finished with value: 0.929411768913269 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:42,299] Trial 1434 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:43,147] Trial 1435 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:43,983] Trial 1436 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:44,827] Trial 1437 finished with value: 0.8117647171020508 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:45,884] Trial 1438 finished with value: 0.9647058844566345 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:47,003] Trial 1439 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:48,243] Trial 1440 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:49,421] Trial 1441 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:50,246] Trial 1442 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:51,090] Trial 1443 finished with value: 0.9647058844566345 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:51,972] Trial 1444 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:52,844] Trial 1445 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:53,688] Trial 1446 finished with value: 0.9764705896377563 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:54,478] Trial 1447 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:55,375] Trial 1448 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:56,214] Trial 1449 finished with value: 0.8117647171020508 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:57,121] Trial 1450 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:57,968] Trial 1451 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:58,823] Trial 1452 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:29:59,836] Trial 1453 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:01,021] Trial 1454 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:02,359] Trial 1455 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:03,494] Trial 1456 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:04,380] Trial 1457 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:05,240] Trial 1458 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:06,101] Trial 1459 finished with value: 0.9176470637321472 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:06,956] Trial 1460 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:07,824] Trial 1461 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:08,669] Trial 1462 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:09,511] Trial 1463 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:10,374] Trial 1464 finished with value: 0.7764706015586853 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7764706015586853, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:11,213] Trial 1465 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:12,078] Trial 1466 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:12,952] Trial 1467 finished with value: 0.9882352948188782 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:14,070] Trial 1468 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:15,314] Trial 1469 finished with value: 0.7058823704719543 and parameters: {'output_features': 22, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:16,599] Trial 1470 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:17,474] Trial 1471 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:18,364] Trial 1472 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:19,226] Trial 1473 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:20,092] Trial 1474 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:21,006] Trial 1475 finished with value: 0.9647058844566345 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:21,879] Trial 1476 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:22,712] Trial 1477 finished with value: 0.9882352948188782 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:23,580] Trial 1478 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:24,417] Trial 1479 finished with value: 0.8352941274642944 and parameters: {'output_features': 10, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:25,296] Trial 1480 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:26,197] Trial 1481 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:27,298] Trial 1482 finished with value: 0.7882353067398071 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7882353067398071, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:28,480] Trial 1483 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:29,759] Trial 1484 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:30,849] Trial 1485 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:31,730] Trial 1486 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:32,600] Trial 1487 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:33,460] Trial 1488 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:34,302] Trial 1489 finished with value: 0.9647058844566345 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:35,168] Trial 1490 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:36,005] Trial 1491 finished with value: 0.6941176652908325 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:36,887] Trial 1492 finished with value: 0.7764706015586853 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7764706015586853, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:37,728] Trial 1493 finished with value: 0.8352941274642944 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:38,599] Trial 1494 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:39,476] Trial 1495 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:40,392] Trial 1496 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:41,555] Trial 1497 finished with value: 0.9764705896377563 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:42,811] Trial 1498 finished with value: 0.8470588326454163 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:44,040] Trial 1499 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:45,136] Trial 1500 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:46,030] Trial 1501 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:46,895] Trial 1502 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:47,740] Trial 1503 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:48,621] Trial 1504 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:49,477] Trial 1505 finished with value: 0.9647058844566345 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:50,355] Trial 1506 finished with value: 0.9882352948188782 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:51,238] Trial 1507 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:52,104] Trial 1508 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:52,954] Trial 1509 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:53,802] Trial 1510 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:54,656] Trial 1511 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:55,758] Trial 1512 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:56,835] Trial 1513 finished with value: 0.7647058963775635 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7647058963775635, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:58,092] Trial 1514 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:30:59,332] Trial 1515 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:00,221] Trial 1516 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:01,086] Trial 1517 finished with value: 0.9529411792755127 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:01,950] Trial 1518 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:02,862] Trial 1519 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:03,788] Trial 1520 finished with value: 0.6823529601097107 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:04,645] Trial 1521 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:05,493] Trial 1522 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:06,380] Trial 1523 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:07,226] Trial 1524 finished with value: 0.9882352948188782 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:08,089] Trial 1525 finished with value: 0.9764705896377563 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:08,949] Trial 1526 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:10,019] Trial 1527 finished with value: 0.9882352948188782 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:11,178] Trial 1528 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:12,470] Trial 1529 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:13,375] Trial 1530 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:14,275] Trial 1531 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:15,159] Trial 1532 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:16,044] Trial 1533 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:16,932] Trial 1534 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:17,803] Trial 1535 finished with value: 0.9764705896377563 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:18,608] Trial 1536 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:19,485] Trial 1537 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:20,344] Trial 1538 finished with value: 0.729411780834198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:21,204] Trial 1539 finished with value: 0.9647058844566345 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:22,053] Trial 1540 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:23,092] Trial 1541 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:24,191] Trial 1542 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:25,461] Trial 1543 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:26,549] Trial 1544 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:27,411] Trial 1545 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:28,311] Trial 1546 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:29,171] Trial 1547 finished with value: 0.658823549747467 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.658823549747467, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:30,106] Trial 1548 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:30,976] Trial 1549 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:31,849] Trial 1550 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:32,720] Trial 1551 finished with value: 0.7647058963775635 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7647058963775635, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:33,600] Trial 1552 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:34,457] Trial 1553 finished with value: 0.9764705896377563 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:35,350] Trial 1554 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:36,207] Trial 1555 finished with value: 0.929411768913269 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:37,383] Trial 1556 finished with value: 0.9882352948188782 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:38,520] Trial 1557 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:39,771] Trial 1558 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:40,791] Trial 1559 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:41,659] Trial 1560 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:42,522] Trial 1561 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:43,366] Trial 1562 finished with value: 0.9647058844566345 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:44,214] Trial 1563 finished with value: 0.7411764860153198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:45,059] Trial 1564 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:45,916] Trial 1565 finished with value: 0.9882352948188782 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:46,824] Trial 1566 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:47,746] Trial 1567 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:48,596] Trial 1568 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:49,449] Trial 1569 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:50,384] Trial 1570 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:51,535] Trial 1571 finished with value: 0.8470588326454163 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:52,850] Trial 1572 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:54,022] Trial 1573 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:54,871] Trial 1574 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:55,721] Trial 1575 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:56,587] Trial 1576 finished with value: 0.9647058844566345 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:57,488] Trial 1577 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:58,325] Trial 1578 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:31:59,155] Trial 1579 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:00,008] Trial 1580 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:00,946] Trial 1581 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:01,840] Trial 1582 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:02,749] Trial 1583 finished with value: 0.9764705896377563 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:03,630] Trial 1584 finished with value: 0.9882352948188782 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:04,742] Trial 1585 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:05,924] Trial 1586 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:07,144] Trial 1587 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:08,347] Trial 1588 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:09,212] Trial 1589 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:10,090] Trial 1590 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:10,980] Trial 1591 finished with value: 0.7882353067398071 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7882353067398071, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:11,873] Trial 1592 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:12,751] Trial 1593 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:13,637] Trial 1594 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:14,489] Trial 1595 finished with value: 0.9529411792755127 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:15,385] Trial 1596 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:16,232] Trial 1597 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:17,132] Trial 1598 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:18,027] Trial 1599 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:19,112] Trial 1600 finished with value: 0.6941176652908325 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:20,376] Trial 1601 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:21,644] Trial 1602 finished with value: 0.7176470756530762 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:22,746] Trial 1603 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:23,623] Trial 1604 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:24,477] Trial 1605 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:25,347] Trial 1606 finished with value: 0.8470588326454163 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:26,230] Trial 1607 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:27,073] Trial 1608 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:27,981] Trial 1609 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:28,841] Trial 1610 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:29,690] Trial 1611 finished with value: 0.9647058844566345 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:30,569] Trial 1612 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:31,478] Trial 1613 finished with value: 0.9882352948188782 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:32,331] Trial 1614 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:33,552] Trial 1615 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:34,790] Trial 1616 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:35,991] Trial 1617 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:37,056] Trial 1618 finished with value: 0.7882353067398071 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7882353067398071, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:37,928] Trial 1619 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:38,806] Trial 1620 finished with value: 0.9882352948188782 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:39,657] Trial 1621 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:40,526] Trial 1622 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:41,392] Trial 1623 finished with value: 0.7647058963775635 and parameters: {'output_features': 16, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7647058963775635, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:42,307] Trial 1624 finished with value: 0.800000011920929 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.800000011920929, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:43,177] Trial 1625 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:44,048] Trial 1626 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:45,017] Trial 1627 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:45,880] Trial 1628 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:46,939] Trial 1629 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:48,106] Trial 1630 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:49,366] Trial 1631 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:50,602] Trial 1632 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:51,466] Trial 1633 finished with value: 0.8235294222831726 and parameters: {'output_features': 10, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:52,374] Trial 1634 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:53,247] Trial 1635 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:54,099] Trial 1636 finished with value: 0.9764705896377563 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:54,976] Trial 1637 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:55,836] Trial 1638 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:56,698] Trial 1639 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:57,586] Trial 1640 finished with value: 0.9764705896377563 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:58,446] Trial 1641 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:32:59,299] Trial 1642 finished with value: 0.9764705896377563 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:00,167] Trial 1643 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:01,191] Trial 1644 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:02,312] Trial 1645 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:03,617] Trial 1646 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:04,746] Trial 1647 finished with value: 0.729411780834198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:05,592] Trial 1648 finished with value: 0.9764705896377563 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:06,475] Trial 1649 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:07,359] Trial 1650 finished with value: 0.9764705896377563 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:08,234] Trial 1651 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:09,105] Trial 1652 finished with value: 0.8941176533699036 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:09,963] Trial 1653 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:10,845] Trial 1654 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:11,758] Trial 1655 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:12,675] Trial 1656 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:13,554] Trial 1657 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:14,584] Trial 1658 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:15,757] Trial 1659 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:17,264] Trial 1660 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:18,607] Trial 1661 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.8095238208770752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:19,915] Trial 1662 finished with value: 0.929411768913269 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:21,155] Trial 1663 finished with value: 0.9764705896377563 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:22,189] Trial 1664 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:23,792] Trial 1665 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:25,039] Trial 1666 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:26,286] Trial 1667 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:27,184] Trial 1668 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:28,100] Trial 1669 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:28,949] Trial 1670 finished with value: 0.9764705896377563 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:30,093] Trial 1671 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:31,278] Trial 1672 finished with value: 0.8235294222831726 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:32,604] Trial 1673 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:33,748] Trial 1674 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:34,644] Trial 1675 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.190476194024086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:35,525] Trial 1676 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:36,436] Trial 1677 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:37,372] Trial 1678 finished with value: 0.929411768913269 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:38,246] Trial 1679 finished with value: 0.8588235378265381 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:39,151] Trial 1680 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:40,010] Trial 1681 finished with value: 0.9882352948188782 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:40,880] Trial 1682 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:41,757] Trial 1683 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:42,658] Trial 1684 finished with value: 0.6705882549285889 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6705882549285889, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:43,669] Trial 1685 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:44,849] Trial 1686 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:46,153] Trial 1687 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:47,268] Trial 1688 finished with value: 0.6941176652908325 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:48,174] Trial 1689 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:49,027] Trial 1690 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:49,880] Trial 1691 finished with value: 0.9882352948188782 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:50,735] Trial 1692 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:51,632] Trial 1693 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:52,528] Trial 1694 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:53,394] Trial 1695 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:54,252] Trial 1696 finished with value: 0.9529411792755127 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:55,132] Trial 1697 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:55,993] Trial 1698 finished with value: 0.8352941274642944 and parameters: {'output_features': 12, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:56,884] Trial 1699 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:58,026] Trial 1700 finished with value: 0.9882352948188782 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:33:59,248] Trial 1701 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:00,522] Trial 1702 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:01,728] Trial 1703 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:02,654] Trial 1704 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:03,539] Trial 1705 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:04,398] Trial 1706 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:05,305] Trial 1707 finished with value: 0.8823529481887817 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:06,179] Trial 1708 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:07,064] Trial 1709 finished with value: 0.9529411792755127 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:07,892] Trial 1710 finished with value: 0.6823529601097107 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:08,743] Trial 1711 finished with value: 0.8235294222831726 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:09,579] Trial 1712 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:10,450] Trial 1713 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:11,307] Trial 1714 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:12,357] Trial 1715 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:13,527] Trial 1716 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:14,794] Trial 1717 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:15,912] Trial 1718 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:16,803] Trial 1719 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:17,671] Trial 1720 finished with value: 0.9882352948188782 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:18,564] Trial 1721 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:19,444] Trial 1722 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:20,312] Trial 1723 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:21,135] Trial 1724 finished with value: 0.9529411792755127 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:22,008] Trial 1725 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:22,892] Trial 1726 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:23,739] Trial 1727 finished with value: 0.9176470637321472 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:24,615] Trial 1728 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:25,513] Trial 1729 finished with value: 0.9647058844566345 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:26,653] Trial 1730 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:27,954] Trial 1731 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:29,208] Trial 1732 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:30,020] Trial 1733 finished with value: 0.6470588445663452 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:30,930] Trial 1734 finished with value: 0.7764706015586853 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7764706015586853, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:31,826] Trial 1735 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:32,704] Trial 1736 finished with value: 0.9647058844566345 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:33,552] Trial 1737 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:34,434] Trial 1738 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:35,325] Trial 1739 finished with value: 0.8470588326454163 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:36,197] Trial 1740 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:37,029] Trial 1741 finished with value: 0.8588235378265381 and parameters: {'output_features': 10, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:37,954] Trial 1742 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:38,822] Trial 1743 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:39,930] Trial 1744 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:41,093] Trial 1745 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:42,331] Trial 1746 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:43,540] Trial 1747 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:44,445] Trial 1748 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:45,331] Trial 1749 finished with value: 0.9882352948188782 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:46,226] Trial 1750 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:47,121] Trial 1751 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:48,014] Trial 1752 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:48,938] Trial 1753 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:49,761] Trial 1754 finished with value: 0.6823529601097107 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:50,634] Trial 1755 finished with value: 0.9058823585510254 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:51,556] Trial 1756 finished with value: 0.729411780834198 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:52,473] Trial 1757 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:53,348] Trial 1758 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:54,529] Trial 1759 finished with value: 0.9647058844566345 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:55,851] Trial 1760 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:57,170] Trial 1761 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:58,316] Trial 1762 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:34:59,257] Trial 1763 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:00,140] Trial 1764 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:01,045] Trial 1765 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:01,961] Trial 1766 finished with value: 0.9882352948188782 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:02,931] Trial 1767 finished with value: 0.8352941274642944 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:03,854] Trial 1768 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:04,818] Trial 1769 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:05,765] Trial 1770 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:06,715] Trial 1771 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:07,669] Trial 1772 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:08,947] Trial 1773 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:10,327] Trial 1774 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:11,628] Trial 1775 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:12,610] Trial 1776 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:13,522] Trial 1777 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:14,407] Trial 1778 finished with value: 0.9647058844566345 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:15,341] Trial 1779 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:16,284] Trial 1780 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:17,217] Trial 1781 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:18,173] Trial 1782 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:19,159] Trial 1783 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:20,097] Trial 1784 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:21,032] Trial 1785 finished with value: 0.8705882430076599 and parameters: {'output_features': 12, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:22,048] Trial 1786 finished with value: 0.9647058844566345 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:23,299] Trial 1787 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:24,702] Trial 1788 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:25,865] Trial 1789 finished with value: 0.9529411792755127 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:26,833] Trial 1790 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:27,794] Trial 1791 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:28,770] Trial 1792 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:29,790] Trial 1793 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:30,900] Trial 1794 finished with value: 0.8823529481887817 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:31,939] Trial 1795 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:33,021] Trial 1796 finished with value: 0.9764705896377563 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:34,066] Trial 1797 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:34,968] Trial 1798 finished with value: 0.6470588445663452 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:36,177] Trial 1799 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:37,510] Trial 1800 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:38,866] Trial 1801 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:39,773] Trial 1802 finished with value: 0.8117647171020508 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:40,682] Trial 1803 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:41,670] Trial 1804 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:42,640] Trial 1805 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:43,638] Trial 1806 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:44,577] Trial 1807 finished with value: 0.8705882430076599 and parameters: {'output_features': 14, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:45,617] Trial 1808 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:46,605] Trial 1809 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:47,578] Trial 1810 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:48,555] Trial 1811 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:49,862] Trial 1812 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:51,281] Trial 1813 finished with value: 0.9647058844566345 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:52,547] Trial 1814 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:53,458] Trial 1815 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:54,394] Trial 1816 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:55,332] Trial 1817 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:56,287] Trial 1818 finished with value: 0.9529411792755127 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:57,244] Trial 1819 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:58,189] Trial 1820 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:35:59,112] Trial 1821 finished with value: 0.6941176652908325 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:00,064] Trial 1822 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:00,980] Trial 1823 finished with value: 0.9882352948188782 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:01,945] Trial 1824 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:03,192] Trial 1825 finished with value: 0.8352941274642944 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:04,603] Trial 1826 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:05,994] Trial 1827 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:06,983] Trial 1828 finished with value: 0.729411780834198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:07,994] Trial 1829 finished with value: 0.8352941274642944 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:09,016] Trial 1830 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:10,111] Trial 1831 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:11,079] Trial 1832 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:12,060] Trial 1833 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:13,073] Trial 1834 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:14,052] Trial 1835 finished with value: 0.9764705896377563 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:15,053] Trial 1836 finished with value: 0.9647058844566345 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:16,188] Trial 1837 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:17,484] Trial 1838 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:18,922] Trial 1839 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:20,195] Trial 1840 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:21,209] Trial 1841 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:22,130] Trial 1842 finished with value: 0.6823529601097107 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:23,180] Trial 1843 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:24,199] Trial 1844 finished with value: 0.7647058963775635 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7647058963775635, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:25,157] Trial 1845 finished with value: 0.9764705896377563 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:26,163] Trial 1846 finished with value: 0.9764705896377563 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:27,139] Trial 1847 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:28,106] Trial 1848 finished with value: 0.9411764740943909 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:29,116] Trial 1849 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:30,324] Trial 1850 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:31,682] Trial 1851 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:33,143] Trial 1852 finished with value: 0.9882352948188782 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:34,133] Trial 1853 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:35,090] Trial 1854 finished with value: 0.729411780834198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:36,011] Trial 1855 finished with value: 0.9764705896377563 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:36,983] Trial 1856 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:37,992] Trial 1857 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:38,913] Trial 1858 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:39,869] Trial 1859 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:40,788] Trial 1860 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:41,772] Trial 1861 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:42,692] Trial 1862 finished with value: 0.6823529601097107 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:43,985] Trial 1863 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:45,348] Trial 1864 finished with value: 0.9764705896377563 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:46,725] Trial 1865 finished with value: 0.8235294222831726 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:47,871] Trial 1866 finished with value: 0.800000011920929 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.800000011920929, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:48,905] Trial 1867 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:49,884] Trial 1868 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:50,844] Trial 1869 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0529, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:51,807] Trial 1870 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:52,813] Trial 1871 finished with value: 0.9647058844566345 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9647058844566345, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:53,765] Trial 1872 finished with value: 0.8705882430076599 and parameters: {'output_features': 12, 'optimizer': 'RMSprop', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:54,735] Trial 1873 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:55,743] Trial 1874 finished with value: 0.9764705896377563 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:56,703] Trial 1875 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:57,863] Trial 1876 finished with value: 0.8470588326454163 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8470588326454163, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:36:59,101] Trial 1877 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:37:00,569] Trial 1878 finished with value: 0.9411764740943909 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9411764740943909, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:37:01,780] Trial 1879 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:37:02,822] Trial 1880 finished with value: 0.8235294222831726 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:37:03,838] Trial 1881 finished with value: 0.929411768913269 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:37:04,853] Trial 1882 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:37:05,854] Trial 1883 finished with value: 0.9764705896377563 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:37:06,887] Trial 1884 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:37:07,933] Trial 1885 finished with value: 0.9882352948188782 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9882352948188782, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:37:09,020] Trial 1886 finished with value: 0.8235294222831726 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.0438, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:37:10,025] Trial 1887 finished with value: 0.9529411792755127 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9529411792755127, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:37:10,970] Trial 1888 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-28 23:37:12,331] Trial 1889 finished with value: 0.9764705896377563 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.1672, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 114 with value: 0.9882352948188782.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9764705896377563, Test Accuracy: 0.6666666865348816\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=1890)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "0bc976b9",
      "metadata": {
        "id": "0bc976b9",
        "outputId": "b89f7776-62fd-40e8-e0fa-5b97eb4c7e77",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index: 394, Test Accuracy: 0.8095238208770752\n",
            "Index: 1661, Test Accuracy: 0.8095238208770752\n",
            "Index: 571, Test Accuracy: 0.761904776096344\n",
            "Index: 1230, Test Accuracy: 0.761904776096344\n",
            "Index: 1252, Test Accuracy: 0.761904776096344\n",
            "Index: 1710, Test Accuracy: 0.761904776096344\n",
            "Index: 1821, Test Accuracy: 0.761904776096344\n",
            "Index: 0, Test Accuracy: 0.7142857313156128\n",
            "Index: 24, Test Accuracy: 0.7142857313156128\n",
            "Index: 32, Test Accuracy: 0.7142857313156128\n",
            "Index: 129, Test Accuracy: 0.7142857313156128\n",
            "Index: 196, Test Accuracy: 0.7142857313156128\n",
            "Index: 205, Test Accuracy: 0.7142857313156128\n",
            "Index: 208, Test Accuracy: 0.7142857313156128\n",
            "Index: 212, Test Accuracy: 0.7142857313156128\n",
            "Index: 264, Test Accuracy: 0.7142857313156128\n",
            "Index: 289, Test Accuracy: 0.7142857313156128\n",
            "Index: 299, Test Accuracy: 0.7142857313156128\n",
            "Index: 301, Test Accuracy: 0.7142857313156128\n",
            "Index: 306, Test Accuracy: 0.7142857313156128\n",
            "Index: 310, Test Accuracy: 0.7142857313156128\n",
            "Index: 323, Test Accuracy: 0.7142857313156128\n",
            "Index: 389, Test Accuracy: 0.7142857313156128\n",
            "Index: 401, Test Accuracy: 0.7142857313156128\n",
            "Index: 418, Test Accuracy: 0.7142857313156128\n",
            "Index: 427, Test Accuracy: 0.7142857313156128\n",
            "Index: 441, Test Accuracy: 0.7142857313156128\n",
            "Index: 448, Test Accuracy: 0.7142857313156128\n",
            "Index: 487, Test Accuracy: 0.7142857313156128\n",
            "Index: 493, Test Accuracy: 0.7142857313156128\n",
            "Index: 502, Test Accuracy: 0.7142857313156128\n",
            "Index: 506, Test Accuracy: 0.7142857313156128\n",
            "Index: 530, Test Accuracy: 0.7142857313156128\n",
            "Index: 594, Test Accuracy: 0.7142857313156128\n",
            "Index: 611, Test Accuracy: 0.7142857313156128\n",
            "Index: 638, Test Accuracy: 0.7142857313156128\n",
            "Index: 664, Test Accuracy: 0.7142857313156128\n",
            "Index: 679, Test Accuracy: 0.7142857313156128\n",
            "Index: 716, Test Accuracy: 0.7142857313156128\n",
            "Index: 729, Test Accuracy: 0.7142857313156128\n",
            "Index: 736, Test Accuracy: 0.7142857313156128\n",
            "Index: 746, Test Accuracy: 0.7142857313156128\n",
            "Index: 765, Test Accuracy: 0.7142857313156128\n",
            "Index: 786, Test Accuracy: 0.7142857313156128\n",
            "Index: 787, Test Accuracy: 0.7142857313156128\n",
            "Index: 823, Test Accuracy: 0.7142857313156128\n",
            "Index: 826, Test Accuracy: 0.7142857313156128\n",
            "Index: 861, Test Accuracy: 0.7142857313156128\n",
            "Index: 883, Test Accuracy: 0.7142857313156128\n",
            "Index: 901, Test Accuracy: 0.7142857313156128\n",
            "Index: 911, Test Accuracy: 0.7142857313156128\n",
            "Index: 917, Test Accuracy: 0.7142857313156128\n",
            "Index: 944, Test Accuracy: 0.7142857313156128\n",
            "Index: 957, Test Accuracy: 0.7142857313156128\n",
            "Index: 987, Test Accuracy: 0.7142857313156128\n",
            "Index: 990, Test Accuracy: 0.7142857313156128\n",
            "Index: 995, Test Accuracy: 0.7142857313156128\n",
            "Index: 1000, Test Accuracy: 0.7142857313156128\n",
            "Index: 1018, Test Accuracy: 0.7142857313156128\n",
            "Index: 1031, Test Accuracy: 0.7142857313156128\n",
            "Index: 1043, Test Accuracy: 0.7142857313156128\n",
            "Index: 1083, Test Accuracy: 0.7142857313156128\n",
            "Index: 1096, Test Accuracy: 0.7142857313156128\n",
            "Index: 1117, Test Accuracy: 0.7142857313156128\n",
            "Index: 1120, Test Accuracy: 0.7142857313156128\n",
            "Index: 1132, Test Accuracy: 0.7142857313156128\n",
            "Index: 1159, Test Accuracy: 0.7142857313156128\n",
            "Index: 1161, Test Accuracy: 0.7142857313156128\n",
            "Index: 1189, Test Accuracy: 0.7142857313156128\n",
            "Index: 1231, Test Accuracy: 0.7142857313156128\n",
            "Index: 1342, Test Accuracy: 0.7142857313156128\n",
            "Index: 1353, Test Accuracy: 0.7142857313156128\n",
            "Index: 1367, Test Accuracy: 0.7142857313156128\n",
            "Index: 1372, Test Accuracy: 0.7142857313156128\n",
            "Index: 1410, Test Accuracy: 0.7142857313156128\n",
            "Index: 1416, Test Accuracy: 0.7142857313156128\n",
            "Index: 1436, Test Accuracy: 0.7142857313156128\n",
            "Index: 1470, Test Accuracy: 0.7142857313156128\n",
            "Index: 1476, Test Accuracy: 0.7142857313156128\n",
            "Index: 1508, Test Accuracy: 0.7142857313156128\n",
            "Index: 1527, Test Accuracy: 0.7142857313156128\n",
            "Index: 1565, Test Accuracy: 0.7142857313156128\n",
            "Index: 1577, Test Accuracy: 0.7142857313156128\n",
            "Index: 1584, Test Accuracy: 0.7142857313156128\n",
            "Index: 1585, Test Accuracy: 0.7142857313156128\n",
            "Index: 1593, Test Accuracy: 0.7142857313156128\n",
            "Index: 1625, Test Accuracy: 0.7142857313156128\n",
            "Index: 1672, Test Accuracy: 0.7142857313156128\n",
            "Index: 1681, Test Accuracy: 0.7142857313156128\n",
            "Index: 1701, Test Accuracy: 0.7142857313156128\n",
            "Index: 1757, Test Accuracy: 0.7142857313156128\n",
            "Index: 1773, Test Accuracy: 0.7142857313156128\n",
            "Index: 1799, Test Accuracy: 0.7142857313156128\n",
            "Index: 1824, Test Accuracy: 0.7142857313156128\n",
            "Index: 1845, Test Accuracy: 0.7142857313156128\n",
            "Index: 1, Test Accuracy: 0.6666666865348816\n",
            "Index: 2, Test Accuracy: 0.6666666865348816\n",
            "Index: 3, Test Accuracy: 0.6666666865348816\n",
            "Index: 4, Test Accuracy: 0.6666666865348816\n",
            "Index: 5, Test Accuracy: 0.6666666865348816\n"
          ]
        }
      ],
      "source": [
        "import heapq\n",
        "\n",
        "# Assuming CT is a list of dictionaries and each dictionary has a 'test_accuracy' key\n",
        "top_ten_max_results = heapq.nlargest(100, enumerate(CT), key=lambda x: x[1]['test_accuracy'])\n",
        "\n",
        "# Print the top ten results along with their indices\n",
        "for index, result in top_ten_max_results:\n",
        "    print(f'Index: {index}, Test Accuracy: {result[\"test_accuracy\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "775cea5d-4518-4830-8d48-559ebd7214a7",
      "metadata": {
        "id": "775cea5d-4518-4830-8d48-559ebd7214a7",
        "outputId": "580e5763-b898-4ae3-a787-d7cfb2e5e4a6",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index: 114, Train Accuracy: 0.9882352948188782\n",
            "Index: 182, Train Accuracy: 0.9882352948188782\n",
            "Index: 184, Train Accuracy: 0.9882352948188782\n",
            "Index: 187, Train Accuracy: 0.9882352948188782\n",
            "Index: 191, Train Accuracy: 0.9882352948188782\n",
            "Index: 192, Train Accuracy: 0.9882352948188782\n",
            "Index: 194, Train Accuracy: 0.9882352948188782\n",
            "Index: 196, Train Accuracy: 0.9882352948188782\n",
            "Index: 202, Train Accuracy: 0.9882352948188782\n",
            "Index: 203, Train Accuracy: 0.9882352948188782\n",
            "Index: 204, Train Accuracy: 0.9882352948188782\n",
            "Index: 208, Train Accuracy: 0.9882352948188782\n",
            "Index: 212, Train Accuracy: 0.9882352948188782\n",
            "Index: 221, Train Accuracy: 0.9882352948188782\n",
            "Index: 222, Train Accuracy: 0.9882352948188782\n",
            "Index: 225, Train Accuracy: 0.9882352948188782\n",
            "Index: 226, Train Accuracy: 0.9882352948188782\n",
            "Index: 233, Train Accuracy: 0.9882352948188782\n",
            "Index: 236, Train Accuracy: 0.9882352948188782\n",
            "Index: 242, Train Accuracy: 0.9882352948188782\n"
          ]
        }
      ],
      "source": [
        "import heapq\n",
        "\n",
        "# Assuming CT is a list of dictionaries and each dictionary has a 'test_accuracy' key\n",
        "top_ten_max_results = heapq.nlargest(20, enumerate(TT), key=lambda x: x[1]['train_accuracy'])\n",
        "\n",
        "# Print the top ten results along with their indices\n",
        "for index, result in top_ten_max_results:\n",
        "    print(f'Index: {index}, Train Accuracy: {result[\"train_accuracy\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "e5cfd08e",
      "metadata": {
        "id": "e5cfd08e"
      },
      "outputs": [],
      "source": [
        "train=[]\n",
        "test=[]\n",
        "for i in range(0,1890):\n",
        "    train.append(round(TT[i]['train_accuracy'].item(),4))\n",
        "    test.append(round(CT[i]['test_accuracy'].item(),4))\n",
        "\n",
        "Final_results=pd.DataFrame()\n",
        "\n",
        "Final_results['train']=train\n",
        "Final_results['test']=test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "5c82dac5",
      "metadata": {
        "id": "5c82dac5",
        "outputId": "6f45c6b4-976a-4dfa-c02e-151bf8b15e98",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       train    test\n",
              "0     0.8706  0.7143\n",
              "1     0.7176  0.6667\n",
              "2     0.7647  0.6667\n",
              "3     0.7529  0.6667\n",
              "4     0.7059  0.6667\n",
              "5     0.6471  0.6667\n",
              "6     0.6471  0.6667\n",
              "7     0.6941  0.3333\n",
              "8     0.7412  0.6190\n",
              "9     0.8706  0.4286\n",
              "10    0.7059  0.6190\n",
              "11    0.8471  0.3333\n",
              "12    0.9294  0.6667\n",
              "13    0.9294  0.3333\n",
              "14    0.7294  0.6190\n",
              "15    0.8000  0.6667\n",
              "16    0.8000  0.6667\n",
              "17    0.8471  0.3333\n",
              "18    0.7176  0.6190\n",
              "19    0.8706  0.6667\n",
              "20    0.8000  0.6667\n",
              "21    0.8588  0.3333\n",
              "22    0.8353  0.3333\n",
              "23    0.7647  0.3333\n",
              "24    0.8824  0.7143\n",
              "25    0.9176  0.3810\n",
              "26    0.8941  0.3333\n",
              "27    0.8588  0.3333\n",
              "28    0.8824  0.6667\n",
              "29    0.7294  0.6190\n",
              "30    0.7765  0.6667\n",
              "31    0.8235  0.6667\n",
              "32    0.8588  0.7143\n",
              "33    0.8000  0.3333\n",
              "34    0.7059  0.6667\n",
              "35    0.7412  0.6667\n",
              "36    0.7529  0.3333\n",
              "37    0.7412  0.4286\n",
              "38    0.8118  0.3333\n",
              "39    0.6471  0.6667\n",
              "40    0.7059  0.1905\n",
              "41    0.8824  0.3333\n",
              "42    0.9412  0.6667\n",
              "43    0.9059  0.6667\n",
              "44    0.8824  0.2381\n",
              "45    0.9059  0.3333\n",
              "46    0.6471  0.6667\n",
              "47    0.9176  0.6667\n",
              "48    0.7412  0.6667\n",
              "49    0.9529  0.2381\n",
              "50    0.7176  0.5714\n",
              "51    0.9529  0.2381\n",
              "52    0.9529  0.3333\n",
              "53    0.9529  0.2857\n",
              "54    0.9647  0.2857\n",
              "55    0.9529  0.2381\n",
              "56    0.9529  0.3333\n",
              "57    0.8471  0.6190\n",
              "58    0.9529  0.2381\n",
              "59    0.8235  0.6667\n",
              "60    0.9294  0.2857\n",
              "61    0.9647  0.3333\n",
              "62    0.9647  0.3333\n",
              "63    0.9529  0.2857\n",
              "64    0.9176  0.5238\n",
              "65    0.9529  0.2381\n",
              "66    0.9529  0.3333\n",
              "67    0.7176  0.6667\n",
              "68    0.8353  0.6190\n",
              "69    0.9294  0.6667\n",
              "70    0.8118  0.6667\n",
              "71    0.9529  0.2381\n",
              "72    0.9647  0.2857\n",
              "73    0.9529  0.2857\n",
              "74    0.9412  0.2857\n",
              "75    0.9647  0.2857\n",
              "76    0.8118  0.6667\n",
              "77    0.9647  0.6667\n",
              "78    0.9529  0.2381\n",
              "79    0.6941  0.3810\n",
              "80    0.8941  0.6667\n",
              "81    0.9647  0.3333\n",
              "82    0.9647  0.3333\n",
              "83    0.9529  0.3810\n",
              "84    0.9529  0.2381\n",
              "85    0.7412  0.6667\n",
              "86    0.6706  0.6667\n",
              "87    0.9647  0.2857\n",
              "88    0.8471  0.6667\n",
              "89    0.8471  0.6190\n",
              "90    0.9647  0.2857\n",
              "91    0.9529  0.5714\n",
              "92    0.9647  0.5714\n",
              "93    0.9647  0.6667\n",
              "94    0.9412  0.3333\n",
              "95    0.9647  0.2857\n",
              "96    0.7294  0.6667\n",
              "97    0.8353  0.5238\n",
              "98    0.9647  0.1905\n",
              "99    0.9647  0.2381\n",
              "100   0.7176  0.6190\n",
              "101   0.9647  0.6667\n",
              "102   0.9529  0.1905\n",
              "103   0.9647  0.3810\n",
              "104   0.6941  0.6667\n",
              "105   0.7294  0.3333\n",
              "106   0.8235  0.3333\n",
              "107   0.9647  0.2857\n",
              "108   0.9412  0.2381\n",
              "109   0.8471  0.6667\n",
              "110   0.8118  0.5238\n",
              "111   0.9647  0.6190\n",
              "112   0.9765  0.6667\n",
              "113   0.9647  0.6667\n",
              "114   0.9882  0.3333\n",
              "115   0.9647  0.4286\n",
              "116   0.9647  0.2381\n",
              "117   0.9647  0.3333\n",
              "118   0.9647  0.2857\n",
              "119   0.8471  0.3333\n",
              "120   0.7059  0.6667\n",
              "121   0.9765  0.6667\n",
              "122   0.9529  0.3333\n",
              "123   0.9647  0.3333\n",
              "124   0.9647  0.2381\n",
              "125   0.9647  0.3333\n",
              "126   0.9412  0.2857\n",
              "127   0.6471  0.6667\n",
              "128   0.7765  0.2857\n",
              "129   0.8471  0.7143\n",
              "130   0.9647  0.2857\n",
              "131   0.9647  0.3333\n",
              "132   0.9647  0.3333\n",
              "133   0.9647  0.6190\n",
              "134   0.9647  0.3333\n",
              "135   0.9529  0.2381\n",
              "136   0.8235  0.6667\n",
              "137   0.9529  0.3333\n",
              "138   0.9412  0.3333\n",
              "139   0.9529  0.2381\n",
              "140   0.9294  0.2857\n",
              "141   0.9647  0.3333\n",
              "142   0.9765  0.6667\n",
              "143   0.9647  0.6667\n",
              "144   0.9412  0.4762\n",
              "145   0.9647  0.2857\n",
              "146   0.9647  0.4286\n",
              "147   0.7412  0.6667\n",
              "148   0.9647  0.3810\n",
              "149   0.6941  0.6190\n",
              "150   0.8824  0.5714\n",
              "151   0.9647  0.6667\n",
              "152   0.9529  0.3333\n",
              "153   0.9647  0.5238\n",
              "154   0.9647  0.2857\n",
              "155   0.8235  0.6190\n",
              "156   0.6941  0.5714\n",
              "157   0.9647  0.3333\n",
              "158   0.6824  0.6667\n",
              "159   0.9647  0.3333\n",
              "160   0.8471  0.6667\n",
              "161   0.9647  0.4286\n",
              "162   0.9647  0.2381\n",
              "163   0.9529  0.2857\n",
              "164   0.9647  0.3810\n",
              "165   0.9294  0.2381\n",
              "166   0.9647  0.3333\n",
              "167   0.9647  0.2857\n",
              "168   0.9647  0.3333\n",
              "169   0.9647  0.5238\n",
              "170   0.9647  0.3333\n",
              "171   0.9529  0.2381\n",
              "172   0.9529  0.2381\n",
              "173   0.9529  0.4762\n",
              "174   0.9529  0.3810\n",
              "175   0.9647  0.3333\n",
              "176   0.9647  0.4762\n",
              "177   0.9765  0.6667\n",
              "178   0.8588  0.6667\n",
              "179   0.6941  0.6667\n",
              "180   0.8118  0.6190\n",
              "181   0.9765  0.6667\n",
              "182   0.9882  0.6667\n",
              "183   0.9647  0.2381\n",
              "184   0.9882  0.2381\n",
              "185   0.8588  0.5714\n",
              "186   0.9647  0.6667\n",
              "187   0.9882  0.6190\n",
              "188   0.9765  0.6667\n",
              "189   0.8353  0.3333\n",
              "190   0.8471  0.3810\n",
              "191   0.9882  0.2857\n",
              "192   0.9882  0.6667\n",
              "193   0.9647  0.2857\n",
              "194   0.9882  0.6667\n",
              "195   0.9765  0.3333\n",
              "196   0.9882  0.7143\n",
              "197   0.9765  0.3810\n",
              "198   0.9765  0.6667\n",
              "199   0.9765  0.3333\n",
              "200   0.8353  0.3333\n",
              "201   0.9765  0.2381\n",
              "202   0.9882  0.2381\n",
              "203   0.9882  0.3333\n",
              "204   0.9882  0.2381\n",
              "205   0.9529  0.7143\n",
              "206   0.9529  0.6667\n",
              "207   0.8471  0.6667\n",
              "208   0.9882  0.7143\n",
              "209   0.9765  0.6667\n",
              "210   0.9765  0.3333\n",
              "211   0.9765  0.3333\n",
              "212   0.9882  0.7143\n",
              "213   0.9647  0.5714\n",
              "214   0.9765  0.6190\n",
              "215   0.9647  0.6667\n",
              "216   0.9529  0.6667\n",
              "217   0.9765  0.2381\n",
              "218   0.9529  0.6667\n",
              "219   0.9765  0.2381\n",
              "220   0.9765  0.2381\n",
              "221   0.9882  0.6667\n",
              "222   0.9882  0.6667\n",
              "223   0.9647  0.3333\n",
              "224   0.9765  0.3810\n",
              "225   0.9882  0.6667\n",
              "226   0.9882  0.6667\n",
              "227   0.9529  0.6667\n",
              "228   0.7059  0.6667\n",
              "229   0.9765  0.2857\n",
              "230   0.8118  0.3333\n",
              "231   0.9647  0.6667\n",
              "232   0.9647  0.6667\n",
              "233   0.9882  0.6667\n",
              "234   0.9765  0.6667\n",
              "235   0.8235  0.3333\n",
              "236   0.9882  0.6667\n",
              "237   0.9765  0.2381\n",
              "238   0.7412  0.3333\n",
              "239   0.8824  0.6667\n",
              "240   0.9765  0.2381\n",
              "241   0.9765  0.3333\n",
              "242   0.9882  0.3810\n",
              "243   0.9765  0.4286\n",
              "244   0.9882  0.6667\n",
              "245   0.9765  0.6667\n",
              "246   0.9882  0.6667\n",
              "247   0.9765  0.6667\n",
              "248   0.9765  0.3333\n",
              "249   0.9529  0.6667\n",
              "250   0.8706  0.6190\n",
              "251   0.8588  0.6667\n",
              "252   0.9765  0.3333\n",
              "253   0.9765  0.6667\n",
              "254   0.9765  0.6667\n",
              "255   0.9765  0.5714\n",
              "256   0.9765  0.6190\n",
              "257   0.9882  0.2381\n",
              "258   0.9647  0.4762\n",
              "259   0.9529  0.6667\n",
              "260   0.7059  0.6190\n",
              "261   0.9765  0.6667\n",
              "262   0.9647  0.6667\n",
              "263   0.8471  0.6667\n",
              "264   0.9882  0.7143\n",
              "265   0.9412  0.6190\n",
              "266   0.8588  0.3333\n",
              "267   0.8235  0.3333\n",
              "268   0.9765  0.6667\n",
              "269   0.9882  0.6667\n",
              "270   0.9765  0.2381\n",
              "271   0.7647  0.6667\n",
              "272   0.8824  0.6667\n",
              "273   0.9882  0.6667\n",
              "274   0.9765  0.6667\n",
              "275   0.9647  0.3810\n",
              "276   0.9765  0.2381\n",
              "277   0.8588  0.6667\n",
              "278   0.9882  0.2857\n",
              "279   0.8824  0.6190\n",
              "280   0.9647  0.2381\n",
              "281   0.9647  0.5714\n",
              "282   0.9765  0.6667\n",
              "283   0.6824  0.6190\n",
              "284   0.9294  0.3333\n",
              "285   0.9765  0.6190\n",
              "286   0.8235  0.6667\n",
              "287   0.9529  0.5714\n",
              "288   0.9765  0.2381\n",
              "289   0.8471  0.7143\n",
              "290   0.9647  0.6667\n",
              "291   0.9765  0.2857\n",
              "292   0.9765  0.6667\n",
              "293   0.8588  0.6667\n",
              "294   0.9882  0.5714\n",
              "295   0.9882  0.6667\n",
              "296   0.9765  0.2857\n",
              "297   0.9647  0.6667\n",
              "298   0.8235  0.6667\n",
              "299   0.8824  0.7143\n",
              "300   0.8824  0.3333\n",
              "301   0.9647  0.7143\n",
              "302   0.9647  0.2381\n",
              "303   0.9765  0.6667\n",
              "304   0.9882  0.6667\n",
              "305   0.8235  0.3333\n",
              "306   0.6824  0.7143\n",
              "307   0.9647  0.6667\n",
              "308   0.8000  0.6667\n",
              "309   0.8353  0.6667\n",
              "310   0.9765  0.7143\n",
              "311   0.8353  0.6667\n",
              "312   0.9765  0.6667\n",
              "313   0.9765  0.6667\n",
              "314   0.9765  0.3810\n",
              "315   0.9647  0.2857\n",
              "316   0.8706  0.5714\n",
              "317   0.9647  0.6667\n",
              "318   0.9882  0.6190\n",
              "319   0.9882  0.6667\n",
              "320   0.9765  0.2857\n",
              "321   0.9882  0.6667\n",
              "322   0.9647  0.6667\n",
              "323   0.9647  0.7143\n",
              "324   0.8353  0.3333\n",
              "325   0.9765  0.2381\n",
              "326   0.9765  0.6190\n",
              "327   0.9882  0.6667\n",
              "328   0.9647  0.2381\n",
              "329   0.6824  0.4762\n",
              "330   0.9294  0.6667\n",
              "331   0.9647  0.3810\n",
              "332   0.8235  0.6190\n",
              "333   0.7647  0.3333\n",
              "334   0.9647  0.6667\n",
              "335   0.9529  0.2381\n",
              "336   0.8588  0.6667\n",
              "337   0.9647  0.6667\n",
              "338   0.8824  0.6667\n",
              "339   0.9765  0.5238\n",
              "340   0.9647  0.6190\n",
              "341   0.9882  0.2857\n",
              "342   0.9882  0.6667\n",
              "343   0.9882  0.1905\n",
              "344   0.9882  0.6667\n",
              "345   0.9765  0.6190\n",
              "346   0.9647  0.1905\n",
              "347   0.9765  0.2857\n",
              "348   0.9647  0.6667\n",
              "349   0.9647  0.6667\n",
              "350   0.9647  0.6667\n",
              "351   0.7176  0.6190\n",
              "352   0.7412  0.3333\n",
              "353   0.9647  0.4762\n",
              "354   0.8941  0.4762\n",
              "355   0.8706  0.6667\n",
              "356   0.9765  0.6667\n",
              "357   0.9765  0.5238\n",
              "358   0.8588  0.3333\n",
              "359   0.9882  0.6667\n",
              "360   0.8235  0.3810\n",
              "361   0.8235  0.3333\n",
              "362   0.9647  0.3810\n",
              "363   0.9647  0.4286\n",
              "364   0.9529  0.5238\n",
              "365   0.9765  0.2857\n",
              "366   0.8118  0.6667\n",
              "367   0.9529  0.6190\n",
              "368   0.9882  0.2857\n",
              "369   0.9765  0.6190\n",
              "370   0.9765  0.5238\n",
              "371   0.9647  0.3810\n",
              "372   0.9765  0.5238\n",
              "373   0.7059  0.5238\n",
              "374   0.9765  0.6190\n",
              "375   0.8000  0.3333\n",
              "376   0.9529  0.6667\n",
              "377   0.9412  0.5714\n",
              "378   0.8471  0.4286\n",
              "379   0.8941  0.6667\n",
              "380   0.9529  0.2857\n",
              "381   0.9412  0.6667\n",
              "382   0.8824  0.2857\n",
              "383   0.8706  0.4762\n",
              "384   0.9647  0.6667\n",
              "385   0.9882  0.2381\n",
              "386   0.9765  0.6667\n",
              "387   0.9647  0.2381\n",
              "388   0.9765  0.2381\n",
              "389   0.8353  0.7143\n",
              "390   0.9647  0.6667\n",
              "391   0.9765  0.3810\n",
              "392   0.9882  0.5238\n",
              "393   0.8353  0.3333\n",
              "394   0.6941  0.8095\n",
              "395   0.9647  0.5714\n",
              "396   0.9765  0.6190\n",
              "397   0.8235  0.6190\n",
              "398   0.9765  0.6667\n",
              "399   0.9882  0.6667\n",
              "400   0.9647  0.3333\n",
              "401   0.8588  0.7143\n",
              "402   0.9529  0.6667\n",
              "403   0.9529  0.5714\n",
              "404   0.9882  0.4762\n",
              "405   0.8588  0.6667\n",
              "406   0.9882  0.3810\n",
              "407   0.8353  0.6667\n",
              "408   0.9647  0.6667\n",
              "409   0.9765  0.2381\n",
              "410   0.9765  0.5238\n",
              "411   0.9765  0.6667\n",
              "412   0.8706  0.6190\n",
              "413   0.8588  0.3333\n",
              "414   0.9765  0.6667\n",
              "415   0.9882  0.6667\n",
              "416   0.8706  0.5238\n",
              "417   0.6824  0.6667\n",
              "418   0.9882  0.7143\n",
              "419   0.9647  0.3810\n",
              "420   0.8235  0.3333\n",
              "421   0.9765  0.5714\n",
              "422   0.7647  0.3333\n",
              "423   0.9765  0.2381\n",
              "424   0.9882  0.6667\n",
              "425   0.9647  0.6190\n",
              "426   0.9882  0.3333\n",
              "427   0.8588  0.7143\n",
              "428   0.9647  0.6190\n",
              "429   0.9765  0.6667\n",
              "430   0.9765  0.5238\n",
              "431   0.9882  0.6667\n",
              "432   0.9647  0.3333\n",
              "433   0.9882  0.5238\n",
              "434   0.9765  0.6667\n",
              "435   0.9765  0.6667\n",
              "436   0.8588  0.3333\n",
              "437   0.9176  0.6190\n",
              "438   0.8588  0.4762\n",
              "439   0.9647  0.4286\n",
              "440   0.9294  0.6667\n",
              "441   0.6941  0.7143\n",
              "442   0.9765  0.2381\n",
              "443   0.9882  0.2857\n",
              "444   0.7294  0.6667\n",
              "445   0.9647  0.6667\n",
              "446   0.9765  0.2381\n",
              "447   0.7294  0.3333\n",
              "448   0.9882  0.7143\n",
              "449   0.9765  0.6667\n",
              "450   0.8588  0.4762\n",
              "451   0.9765  0.2381\n",
              "452   0.9882  0.2381\n",
              "453   0.9647  0.6667\n",
              "454   0.9882  0.4286\n",
              "455   0.9882  0.5238\n",
              "456   0.9765  0.6667\n",
              "457   0.9647  0.6190\n",
              "458   0.9765  0.6667\n",
              "459   0.9412  0.6667\n",
              "460   0.9765  0.6667\n",
              "461   0.9882  0.6667\n",
              "462   0.6706  0.6190\n",
              "463   0.8118  0.3333\n",
              "464   0.9882  0.6667\n",
              "465   0.9765  0.4762\n",
              "466   0.8824  0.6190\n",
              "467   0.8353  0.6667\n",
              "468   0.9647  0.6667\n",
              "469   0.9529  0.6667\n",
              "470   0.9529  0.6667\n",
              "471   0.9765  0.6667\n",
              "472   0.6706  0.3333\n",
              "473   0.9765  0.2381\n",
              "474   0.9882  0.5714\n",
              "475   0.8471  0.6667\n",
              "476   0.9765  0.4286\n",
              "477   0.9765  0.3333\n",
              "478   0.9882  0.6190\n",
              "479   0.9882  0.6667\n",
              "480   0.9647  0.4762\n",
              "481   0.9765  0.3333\n",
              "482   0.9882  0.2381\n",
              "483   0.7059  0.6190\n",
              "484   0.9882  0.6667\n",
              "485   0.9882  0.2857\n",
              "486   0.8353  0.6667\n",
              "487   0.9765  0.7143\n",
              "488   0.9765  0.3810\n",
              "489   0.8353  0.6190\n",
              "490   0.9765  0.3333\n",
              "491   0.8353  0.3333\n",
              "492   0.9412  0.3333\n",
              "493   0.8588  0.7143\n",
              "494   0.8706  0.5238\n",
              "495   0.9882  0.2381\n",
              "496   0.9765  0.2381\n",
              "497   0.9765  0.2381\n",
              "498   0.9529  0.4286\n",
              "499   0.7882  0.3333\n",
              "500   0.9647  0.6667\n",
              "501   0.9882  0.6667\n",
              "502   0.8706  0.7143\n",
              "503   0.9765  0.2381\n",
              "504   0.9882  0.6667\n",
              "505   0.6941  0.5714\n",
              "506   0.9647  0.7143\n",
              "507   0.8471  0.6190\n",
              "508   0.9647  0.3810\n",
              "509   0.9765  0.3333\n",
              "510   0.9882  0.6190\n",
              "511   0.9765  0.6667\n",
              "512   0.9647  0.6667\n",
              "513   0.9765  0.6667\n",
              "514   0.9882  0.4762\n",
              "515   0.8588  0.3333\n",
              "516   0.8588  0.3810\n",
              "517   0.8824  0.3333\n",
              "518   0.9647  0.6667\n",
              "519   0.9882  0.6190\n",
              "520   0.9647  0.4286\n",
              "521   0.9765  0.1429\n",
              "522   0.8941  0.4762\n",
              "523   0.9529  0.6667\n",
              "524   0.9882  0.6667\n",
              "525   0.9647  0.6667\n",
              "526   0.9647  0.2381\n",
              "527   0.6941  0.6190\n",
              "528   0.9176  0.6667\n",
              "529   0.9882  0.6667\n",
              "530   0.8471  0.7143\n",
              "531   0.8706  0.6667\n",
              "532   0.9765  0.6190\n",
              "533   0.7765  0.3333\n",
              "534   0.9176  0.5714\n",
              "535   0.9647  0.2857\n",
              "536   0.9647  0.6667\n",
              "537   0.9529  0.6667\n",
              "538   0.8706  0.6667\n",
              "539   0.9647  0.2381\n",
              "540   0.9882  0.2381\n",
              "541   0.9882  0.5238\n",
              "542   0.9765  0.5238\n",
              "543   0.9882  0.2857\n",
              "544   0.9882  0.6667\n",
              "545   0.7765  0.3333\n",
              "546   0.9765  0.4286\n",
              "547   0.9647  0.2857\n",
              "548   0.8824  0.5714\n",
              "549   0.7059  0.6190\n",
              "550   0.9765  0.6667\n",
              "551   0.8235  0.3333\n",
              "552   0.9647  0.6667\n",
              "553   0.9765  0.6667\n",
              "554   0.8941  0.6667\n",
              "555   0.8000  0.3333\n",
              "556   0.9765  0.6667\n",
              "557   0.9765  0.5714\n",
              "558   0.8235  0.6667\n",
              "559   0.8353  0.6190\n",
              "560   0.9765  0.2857\n",
              "561   0.9412  0.6667\n",
              "562   0.9647  0.1905\n",
              "563   0.9882  0.4762\n",
              "564   0.9529  0.6667\n",
              "565   0.9882  0.2381\n",
              "566   0.9882  0.6667\n",
              "567   0.9765  0.4762\n",
              "568   0.9647  0.6667\n",
              "569   0.9765  0.5238\n",
              "570   0.9882  0.3333\n",
              "571   0.6824  0.7619\n",
              "572   0.9765  0.5714\n",
              "573   0.7647  0.6190\n",
              "574   0.9882  0.6667\n",
              "575   0.9765  0.5714\n",
              "576   0.8471  0.4286\n",
              "577   0.8353  0.6190\n",
              "578   0.9059  0.6667\n",
              "579   0.9882  0.2381\n",
              "580   0.9882  0.6667\n",
              "581   0.8588  0.6667\n",
              "582   0.9882  0.6667\n",
              "583   0.9882  0.6190\n",
              "584   0.7882  0.6667\n",
              "585   0.9647  0.2857\n",
              "586   0.8118  0.2857\n",
              "587   0.9647  0.6667\n",
              "588   0.9647  0.6667\n",
              "589   0.9647  0.5238\n",
              "590   0.9765  0.6667\n",
              "591   0.9882  0.6190\n",
              "592   0.9765  0.4286\n",
              "593   0.7176  0.6190\n",
              "594   0.9882  0.7143\n",
              "595   0.8118  0.3333\n",
              "596   0.9765  0.6667\n",
              "597   0.9176  0.6667\n",
              "598   0.9882  0.3810\n",
              "599   0.9882  0.6190\n",
              "600   0.7059  0.3333\n",
              "601   0.9765  0.5714\n",
              "602   0.9647  0.3333\n",
              "603   0.8706  0.6667\n",
              "604   0.9647  0.6667\n",
              "605   0.8824  0.3333\n",
              "606   0.9765  0.6667\n",
              "607   0.9647  0.3333\n",
              "608   0.9412  0.2381\n",
              "609   0.9882  0.6667\n",
              "610   0.9647  0.2857\n",
              "611   0.9059  0.7143\n",
              "612   0.9882  0.2381\n",
              "613   0.9765  0.6667\n",
              "614   0.7882  0.3333\n",
              "615   0.9647  0.6190\n",
              "616   0.7059  0.6190\n",
              "617   0.8235  0.5714\n",
              "618   0.9412  0.4286\n",
              "619   0.9882  0.6667\n",
              "620   0.9882  0.2857\n",
              "621   0.8353  0.3333\n",
              "622   0.9765  0.6667\n",
              "623   0.9294  0.3333\n",
              "624   0.9765  0.6667\n",
              "625   0.9765  0.6667\n",
              "626   0.8588  0.3810\n",
              "627   0.9882  0.6190\n",
              "628   0.7647  0.4762\n",
              "629   0.9647  0.4286\n",
              "630   0.9765  0.6667\n",
              "631   0.8353  0.6667\n",
              "632   0.9765  0.6667\n",
              "633   0.8824  0.3333\n",
              "634   0.9529  0.6190\n",
              "635   0.9765  0.6667\n",
              "636   0.9647  0.6667\n",
              "637   0.7059  0.3333\n",
              "638   0.8353  0.7143\n",
              "639   0.8235  0.3333\n",
              "640   0.9765  0.6667\n",
              "641   0.8235  0.3333\n",
              "642   0.9765  0.6667\n",
              "643   0.8471  0.6667\n",
              "644   0.9882  0.6667\n",
              "645   0.9765  0.4762\n",
              "646   0.9647  0.6667\n",
              "647   0.7882  0.3333\n",
              "648   0.9647  0.6667\n",
              "649   0.9765  0.6667\n",
              "650   0.9882  0.6667\n",
              "651   0.9882  0.5714\n",
              "652   0.9765  0.6667\n",
              "653   0.9882  0.3810\n",
              "654   0.9647  0.1905\n",
              "655   0.8235  0.5238\n",
              "656   0.9412  0.4286\n",
              "657   0.9412  0.6667\n",
              "658   0.9765  0.3810\n",
              "659   0.7765  0.3810\n",
              "660   0.9059  0.6667\n",
              "661   0.8118  0.3810\n",
              "662   0.9412  0.3810\n",
              "663   0.9647  0.6667\n",
              "664   0.9765  0.7143\n",
              "665   0.7647  0.3333\n",
              "666   0.9059  0.6667\n",
              "667   0.9529  0.6667\n",
              "668   0.9765  0.3810\n",
              "669   0.8471  0.6667\n",
              "670   0.9412  0.6667\n",
              "671   0.9765  0.6667\n",
              "672   0.9647  0.4286\n",
              "673   0.9765  0.2381\n",
              "674   0.9412  0.2857\n",
              "675   0.9647  0.6667\n",
              "676   0.9765  0.2857\n",
              "677   0.9765  0.2857\n",
              "678   0.9294  0.6667\n",
              "679   0.9765  0.7143\n",
              "680   0.9765  0.6190\n",
              "681   0.7059  0.6190\n",
              "682   0.9765  0.2857\n",
              "683   0.9647  0.5714\n",
              "684   0.9647  0.2381\n",
              "685   0.9647  0.6667\n",
              "686   0.6706  0.3333\n",
              "687   0.7176  0.3333\n",
              "688   0.8706  0.6667\n",
              "689   0.9647  0.6667\n",
              "690   0.9647  0.2381\n",
              "691   0.9882  0.6667\n",
              "692   0.8353  0.6667\n",
              "693   0.8353  0.3333\n",
              "694   0.9882  0.6190\n",
              "695   0.9765  0.5238\n",
              "696   0.9882  0.4762\n",
              "697   0.9882  0.5714\n",
              "698   0.6824  0.3810\n",
              "699   0.9882  0.6667\n",
              "700   0.9647  0.2857\n",
              "701   0.9765  0.6667\n",
              "702   0.9765  0.6667\n",
              "703   0.6941  0.5714\n",
              "704   0.9529  0.1905\n",
              "705   0.8000  0.6667\n",
              "706   0.9765  0.6190\n",
              "707   0.9647  0.2857\n",
              "708   0.9765  0.5714\n",
              "709   0.9882  0.3333\n",
              "710   0.7412  0.3333\n",
              "711   0.9529  0.3810\n",
              "712   0.9882  0.6667\n",
              "713   0.8471  0.2857\n",
              "714   0.8824  0.5714\n",
              "715   0.9765  0.6667\n",
              "716   0.8824  0.7143\n",
              "717   0.9882  0.5238\n",
              "718   0.9765  0.6190\n",
              "719   0.9882  0.3333\n",
              "720   0.9765  0.2381\n",
              "721   0.8353  0.6667\n",
              "722   0.9882  0.2381\n",
              "723   0.9765  0.2381\n",
              "724   0.8118  0.3333\n",
              "725   0.9059  0.5238\n",
              "726   0.9765  0.6667\n",
              "727   0.6824  0.6190\n",
              "728   0.8235  0.3333\n",
              "729   0.9765  0.7143\n",
              "730   0.9647  0.2857\n",
              "731   0.9647  0.6667\n",
              "732   0.8471  0.3333\n",
              "733   0.9882  0.6667\n",
              "734   0.9765  0.6667\n",
              "735   0.8588  0.6667\n",
              "736   0.9647  0.7143\n",
              "737   0.9882  0.6667\n",
              "738   0.7529  0.6667\n",
              "739   0.9529  0.6190\n",
              "740   0.9765  0.5714\n",
              "741   0.8353  0.5714\n",
              "742   0.9647  0.5238\n",
              "743   0.9882  0.6667\n",
              "744   0.9059  0.3333\n",
              "745   0.9765  0.2857\n",
              "746   0.9882  0.7143\n",
              "747   0.9647  0.2381\n",
              "748   0.7059  0.6190\n",
              "749   0.9294  0.6667\n",
              "750   0.7765  0.3333\n",
              "751   0.6824  0.5714\n",
              "752   0.9647  0.2857\n",
              "753   0.8588  0.6667\n",
              "754   0.9882  0.2857\n",
              "755   0.9647  0.5714\n",
              "756   0.9647  0.4286\n",
              "757   0.8588  0.6667\n",
              "758   0.9765  0.6667\n",
              "759   0.9765  0.6190\n",
              "760   0.9529  0.6667\n",
              "761   0.9647  0.5714\n",
              "762   0.9765  0.6667\n",
              "763   0.9647  0.6667\n",
              "764   0.9765  0.6190\n",
              "765   0.8941  0.7143\n",
              "766   0.9765  0.6667\n",
              "767   0.9529  0.4762\n",
              "768   0.9765  0.2381\n",
              "769   0.9765  0.5238\n",
              "770   0.6706  0.3810\n",
              "771   0.8235  0.6667\n",
              "772   0.9176  0.3333\n",
              "773   0.9765  0.6667\n",
              "774   0.9882  0.6667\n",
              "775   0.9765  0.1905\n",
              "776   0.8824  0.6667\n",
              "777   0.8471  0.5714\n",
              "778   0.9765  0.4762\n",
              "779   0.8588  0.6190\n",
              "780   0.9882  0.6667\n",
              "781   0.9882  0.6667\n",
              "782   0.9529  0.5714\n",
              "783   0.9882  0.2381\n",
              "784   0.9647  0.3333\n",
              "785   0.9647  0.5238\n",
              "786   0.9882  0.7143\n",
              "787   0.9882  0.7143\n",
              "788   0.9882  0.3810\n",
              "789   0.9647  0.2381\n",
              "790   0.9882  0.3810\n",
              "791   0.9647  0.6667\n",
              "792   0.6824  0.6190\n",
              "793   0.8235  0.3333\n",
              "794   0.8941  0.6667\n",
              "795   0.9765  0.6667\n",
              "796   0.9647  0.2381\n",
              "797   0.9294  0.3333\n",
              "798   0.7294  0.3333\n",
              "799   0.9882  0.6667\n",
              "800   0.9647  0.4286\n",
              "801   0.8471  0.5238\n",
              "802   0.9647  0.6667\n",
              "803   0.6941  0.2381\n",
              "804   0.9882  0.4762\n",
              "805   0.9529  0.4762\n",
              "806   0.8353  0.3333\n",
              "807   0.9765  0.4762\n",
              "808   0.9647  0.6667\n",
              "809   0.9765  0.6667\n",
              "810   0.9882  0.2381\n",
              "811   0.9882  0.3333\n",
              "812   0.9529  0.6190\n",
              "813   0.6941  0.5714\n",
              "814   0.9647  0.2857\n",
              "815   0.9882  0.4286\n",
              "816   0.8118  0.6190\n",
              "817   0.9647  0.2857\n",
              "818   0.9176  0.4286\n",
              "819   0.9765  0.6667\n",
              "820   0.7412  0.3333\n",
              "821   0.9765  0.6667\n",
              "822   0.9765  0.2857\n",
              "823   0.8235  0.7143\n",
              "824   0.8471  0.5714\n",
              "825   0.9765  0.3333\n",
              "826   0.9059  0.7143\n",
              "827   0.9765  0.5238\n",
              "828   0.9765  0.4762\n",
              "829   0.9412  0.3810\n",
              "830   0.9647  0.1905\n",
              "831   0.9765  0.6190\n",
              "832   0.8235  0.6667\n",
              "833   0.9882  0.6667\n",
              "834   0.9647  0.6667\n",
              "835   0.6471  0.6667\n",
              "836   0.9765  0.5714\n",
              "837   0.8118  0.6190\n",
              "838   0.9882  0.6667\n",
              "839   0.9529  0.6667\n",
              "840   0.9647  0.3333\n",
              "841   0.8235  0.3333\n",
              "842   0.9882  0.6667\n",
              "843   0.9647  0.4286\n",
              "844   0.9882  0.6667\n",
              "845   0.8353  0.6667\n",
              "846   0.9647  0.6667\n",
              "847   0.9765  0.2857\n",
              "848   0.9059  0.6667\n",
              "849   0.9529  0.2381\n",
              "850   0.9529  0.6190\n",
              "851   0.9765  0.2381\n",
              "852   0.9059  0.3333\n",
              "853   0.9882  0.6667\n",
              "854   0.9176  0.5714\n",
              "855   0.8588  0.3810\n",
              "856   0.9647  0.4286\n",
              "857   0.6941  0.3333\n",
              "858   0.9882  0.6190\n",
              "859   0.8235  0.3333\n",
              "860   0.9647  0.5714\n",
              "861   0.9176  0.7143\n",
              "862   0.9529  0.2381\n",
              "863   0.9647  0.6190\n",
              "864   0.7176  0.3333\n",
              "865   0.9529  0.2381\n",
              "866   0.9647  0.3333\n",
              "867   0.8000  0.6667\n",
              "868   0.9882  0.2381\n",
              "869   0.9765  0.5714\n",
              "870   0.9647  0.4286\n",
              "871   0.9765  0.5714\n",
              "872   0.9294  0.5238\n",
              "873   0.9647  0.1905\n",
              "874   0.9882  0.6667\n",
              "875   0.8000  0.3333\n",
              "876   0.9882  0.6667\n",
              "877   0.9765  0.2381\n",
              "878   0.9765  0.6190\n",
              "879   0.9294  0.6667\n",
              "880   0.9294  0.3810\n",
              "881   0.8353  0.6190\n",
              "882   0.7176  0.6190\n",
              "883   0.9294  0.7143\n",
              "884   0.9765  0.6667\n",
              "885   0.8588  0.3333\n",
              "886   0.9765  0.2857\n",
              "887   0.9882  0.6667\n",
              "888   0.8824  0.6667\n",
              "889   0.9412  0.3333\n",
              "890   0.8353  0.5714\n",
              "891   0.9647  0.2381\n",
              "892   0.9882  0.4286\n",
              "893   0.9765  0.6667\n",
              "894   0.9765  0.3333\n",
              "895   0.9529  0.2381\n",
              "896   0.9765  0.6667\n",
              "897   0.9765  0.6667\n",
              "898   0.9529  0.6667\n",
              "899   0.9882  0.6190\n",
              "900   0.7059  0.6667\n",
              "901   0.9882  0.7143\n",
              "902   0.9647  0.6667\n",
              "903   0.7294  0.3333\n",
              "904   0.9882  0.6667\n",
              "905   0.9882  0.1905\n",
              "906   0.9529  0.3333\n",
              "907   0.9294  0.4286\n",
              "908   0.8118  0.4286\n",
              "909   0.8824  0.3333\n",
              "910   0.8706  0.6190\n",
              "911   0.9647  0.7143\n",
              "912   0.9647  0.3810\n",
              "913   0.9529  0.6667\n",
              "914   0.9765  0.5238\n",
              "915   0.9059  0.6667\n",
              "916   0.9765  0.6667\n",
              "917   0.8471  0.7143\n",
              "918   0.9765  0.6667\n",
              "919   0.9647  0.4762\n",
              "920   0.9529  0.3333\n",
              "921   0.9765  0.5238\n",
              "922   0.7059  0.6667\n",
              "923   0.9647  0.2857\n",
              "924   0.8353  0.4762\n",
              "925   0.9294  0.6667\n",
              "926   0.9647  0.3333\n",
              "927   0.9765  0.2857\n",
              "928   0.8353  0.3333\n",
              "929   0.9765  0.5714\n",
              "930   0.9882  0.3333\n",
              "931   0.9765  0.3333\n",
              "932   0.8706  0.3333\n",
              "933   0.9412  0.3333\n",
              "934   0.9647  0.4762\n",
              "935   0.9647  0.6667\n",
              "936   0.8941  0.6190\n",
              "937   0.9529  0.2857\n",
              "938   0.9882  0.6667\n",
              "939   0.9059  0.6667\n",
              "940   0.9765  0.2857\n",
              "941   0.8235  0.3333\n",
              "942   0.9882  0.4762\n",
              "943   0.9765  0.6667\n",
              "944   0.7176  0.7143\n",
              "945   0.8118  0.6667\n",
              "946   0.8000  0.6190\n",
              "947   0.9765  0.4762\n",
              "948   0.9765  0.6667\n",
              "949   0.9765  0.2857\n",
              "950   0.8353  0.5714\n",
              "951   0.9765  0.6667\n",
              "952   0.9765  0.2381\n",
              "953   0.9647  0.6667\n",
              "954   0.8353  0.6667\n",
              "955   0.9765  0.6667\n",
              "956   0.9647  0.6667\n",
              "957   0.9882  0.7143\n",
              "958   0.9765  0.3810\n",
              "959   0.8118  0.6190\n",
              "960   0.9647  0.2381\n",
              "961   0.9647  0.6667\n",
              "962   0.9765  0.6667\n",
              "963   0.8824  0.5714\n",
              "964   0.9294  0.6190\n",
              "965   0.9647  0.6667\n",
              "966   0.9882  0.5714\n",
              "967   0.7176  0.6667\n",
              "968   0.9882  0.4286\n",
              "969   0.7176  0.3810\n",
              "970   0.9765  0.2381\n",
              "971   0.9176  0.6667\n",
              "972   0.7176  0.6667\n",
              "973   0.9882  0.4286\n",
              "974   0.9529  0.5714\n",
              "975   0.9647  0.6190\n",
              "976   0.9765  0.3333\n",
              "977   0.8471  0.6667\n",
              "978   0.9882  0.3333\n",
              "979   0.9647  0.1905\n",
              "980   0.9765  0.6667\n",
              "981   0.9765  0.6667\n",
              "982   0.9647  0.2381\n",
              "983   0.9765  0.1905\n",
              "984   0.9882  0.4286\n",
              "985   0.9647  0.6667\n",
              "986   0.8941  0.6667\n",
              "987   0.9765  0.7143\n",
              "988   0.6824  0.3333\n",
              "989   0.8706  0.4286\n",
              "990   0.8235  0.7143\n",
              "991   0.9294  0.3333\n",
              "992   0.9647  0.6667\n",
              "993   0.9882  0.4762\n",
              "994   0.9647  0.2857\n",
              "995   0.9882  0.7143\n",
              "996   0.7059  0.6667\n",
              "997   0.9176  0.6667\n",
              "998   0.9765  0.3333\n",
              "999   0.7176  0.4762\n",
              "1000  0.8471  0.7143\n",
              "1001  0.9647  0.6667\n",
              "1002  0.9882  0.6667\n",
              "1003  0.9647  0.4762\n",
              "1004  0.9882  0.2381\n",
              "1005  0.9294  0.3333\n",
              "1006  0.9765  0.2857\n",
              "1007  0.9529  0.6190\n",
              "1008  0.9294  0.6667\n",
              "1009  0.9765  0.6667\n",
              "1010  0.7059  0.6190\n",
              "1011  0.9882  0.6667\n",
              "1012  0.8000  0.6190\n",
              "1013  0.8353  0.6667\n",
              "1014  0.9412  0.2381\n",
              "1015  0.9529  0.6667\n",
              "1016  0.9765  0.5238\n",
              "1017  0.7176  0.2857\n",
              "1018  0.9765  0.7143\n",
              "1019  0.9176  0.6667\n",
              "1020  0.8588  0.4762\n",
              "1021  0.9765  0.6667\n",
              "1022  0.9882  0.5714\n",
              "1023  0.9882  0.4286\n",
              "1024  0.8353  0.6667\n",
              "1025  0.9647  0.2857\n",
              "1026  0.9882  0.6667\n",
              "1027  0.8824  0.6667\n",
              "1028  0.9882  0.5714\n",
              "1029  0.9765  0.3333\n",
              "1030  0.9882  0.6667\n",
              "1031  0.9882  0.7143\n",
              "1032  0.6706  0.6190\n",
              "1033  0.9882  0.4762\n",
              "1034  0.8118  0.6667\n",
              "1035  0.9882  0.6667\n",
              "1036  0.9059  0.4286\n",
              "1037  0.9765  0.2381\n",
              "1038  0.8471  0.3333\n",
              "1039  0.9765  0.4286\n",
              "1040  0.9647  0.2381\n",
              "1041  0.8824  0.6667\n",
              "1042  0.8235  0.6667\n",
              "1043  0.9882  0.7143\n",
              "1044  0.8471  0.4286\n",
              "1045  0.9412  0.2857\n",
              "1046  0.9647  0.4762\n",
              "1047  0.8941  0.6667\n",
              "1048  0.9882  0.6667\n",
              "1049  0.9882  0.6190\n",
              "1050  0.9412  0.3333\n",
              "1051  0.7882  0.3333\n",
              "1052  0.9882  0.6667\n",
              "1053  0.9647  0.5238\n",
              "1054  0.6941  0.6667\n",
              "1055  0.8824  0.5238\n",
              "1056  0.8353  0.6667\n",
              "1057  0.9882  0.5714\n",
              "1058  0.9647  0.6667\n",
              "1059  0.9647  0.2381\n",
              "1060  0.9882  0.6667\n",
              "1061  0.8353  0.6667\n",
              "1062  0.9412  0.3810\n",
              "1063  0.9765  0.2381\n",
              "1064  0.9882  0.6190\n",
              "1065  0.9765  0.1905\n",
              "1066  0.9647  0.4762\n",
              "1067  0.7412  0.5238\n",
              "1068  0.9765  0.6667\n",
              "1069  0.8706  0.6667\n",
              "1070  0.9647  0.6667\n",
              "1071  0.9529  0.5238\n",
              "1072  0.9882  0.6667\n",
              "1073  0.9647  0.3333\n",
              "1074  0.8588  0.5714\n",
              "1075  0.9647  0.6667\n",
              "1076  0.6941  0.6190\n",
              "1077  0.9647  0.3333\n",
              "1078  0.8000  0.6667\n",
              "1079  0.9765  0.3810\n",
              "1080  0.8353  0.3333\n",
              "1081  0.9647  0.6667\n",
              "1082  0.8000  0.3333\n",
              "1083  0.8471  0.7143\n",
              "1084  0.9765  0.5714\n",
              "1085  0.9882  0.4762\n",
              "1086  0.8588  0.3333\n",
              "1087  0.9882  0.6667\n",
              "1088  0.9529  0.3810\n",
              "1089  0.9765  0.1905\n",
              "1090  0.9765  0.2857\n",
              "1091  0.9529  0.2857\n",
              "1092  0.9765  0.2381\n",
              "1093  0.9647  0.6190\n",
              "1094  0.9647  0.1905\n",
              "1095  0.9647  0.6667\n",
              "1096  0.9765  0.7143\n",
              "1097  0.9882  0.2857\n",
              "1098  0.8588  0.3333\n",
              "1099  0.8471  0.4762\n",
              "1100  0.7176  0.6190\n",
              "1101  0.9765  0.6667\n",
              "1102  0.7294  0.6667\n",
              "1103  0.9882  0.6667\n",
              "1104  0.8353  0.3333\n",
              "1105  0.9765  0.3810\n",
              "1106  0.9882  0.6667\n",
              "1107  0.8000  0.3333\n",
              "1108  0.8353  0.6667\n",
              "1109  0.9412  0.3810\n",
              "1110  0.7765  0.6667\n",
              "1111  0.9647  0.6667\n",
              "1112  0.9294  0.1905\n",
              "1113  0.9882  0.6667\n",
              "1114  0.9765  0.2381\n",
              "1115  0.9529  0.2857\n",
              "1116  0.9882  0.3333\n",
              "1117  0.9765  0.7143\n",
              "1118  0.9765  0.6190\n",
              "1119  0.6941  0.5714\n",
              "1120  0.9882  0.7143\n",
              "1121  0.8235  0.6190\n",
              "1122  0.9647  0.3333\n",
              "1123  0.9647  0.4762\n",
              "1124  0.7882  0.3333\n",
              "1125  0.9647  0.5238\n",
              "1126  0.9765  0.3333\n",
              "1127  0.9647  0.4762\n",
              "1128  0.8824  0.3810\n",
              "1129  0.7529  0.6667\n",
              "1130  0.9647  0.6667\n",
              "1131  0.8824  0.2857\n",
              "1132  0.9765  0.7143\n",
              "1133  0.9765  0.3333\n",
              "1134  0.9647  0.6190\n",
              "1135  0.9765  0.6190\n",
              "1136  0.7882  0.3333\n",
              "1137  0.8000  0.6667\n",
              "1138  0.9647  0.2381\n",
              "1139  0.9765  0.3333\n",
              "1140  0.9765  0.3810\n",
              "1141  0.9647  0.2857\n",
              "1142  0.6824  0.6667\n",
              "1143  0.9765  0.6667\n",
              "1144  0.9647  0.6667\n",
              "1145  0.8118  0.6190\n",
              "1146  0.9765  0.2857\n",
              "1147  0.9765  0.6667\n",
              "1148  0.8353  0.3333\n",
              "1149  0.9647  0.6667\n",
              "1150  0.9647  0.6667\n",
              "1151  0.8353  0.6667\n",
              "1152  0.8471  0.6667\n",
              "1153  0.9765  0.4762\n",
              "1154  0.9882  0.6190\n",
              "1155  0.8824  0.6190\n",
              "1156  0.9059  0.3333\n",
              "1157  0.9765  0.3333\n",
              "1158  0.9882  0.6190\n",
              "1159  0.9882  0.7143\n",
              "1160  0.9765  0.6190\n",
              "1161  0.9882  0.7143\n",
              "1162  0.8706  0.6667\n",
              "1163  0.6941  0.6667\n",
              "1164  0.9765  0.6667\n",
              "1165  0.9765  0.6190\n",
              "1166  0.9647  0.2857\n",
              "1167  0.8118  0.5238\n",
              "1168  0.8118  0.3333\n",
              "1169  0.8471  0.5714\n",
              "1170  0.9176  0.6667\n",
              "1171  0.9882  0.6667\n",
              "1172  0.9647  0.6667\n",
              "1173  0.9647  0.6667\n",
              "1174  0.8706  0.5714\n",
              "1175  0.9882  0.3333\n",
              "1176  0.9647  0.6667\n",
              "1177  0.9882  0.6667\n",
              "1178  0.8353  0.6667\n",
              "1179  0.9529  0.6190\n",
              "1180  0.9647  0.2381\n",
              "1181  0.9647  0.3333\n",
              "1182  0.8706  0.5238\n",
              "1183  0.9647  0.2381\n",
              "1184  0.9529  0.5714\n",
              "1185  0.7765  0.4286\n",
              "1186  0.9647  0.2857\n",
              "1187  0.8235  0.3333\n",
              "1188  0.9647  0.5238\n",
              "1189  0.8824  0.7143\n",
              "1190  0.9765  0.6667\n",
              "1191  0.8706  0.3333\n",
              "1192  0.9765  0.6667\n",
              "1193  0.9294  0.3810\n",
              "1194  0.9529  0.2857\n",
              "1195  0.8471  0.5238\n",
              "1196  0.8235  0.3333\n",
              "1197  0.9765  0.6190\n",
              "1198  0.9647  0.4286\n",
              "1199  0.9882  0.5714\n",
              "1200  0.9412  0.6667\n",
              "1201  0.9882  0.2381\n",
              "1202  0.9765  0.6667\n",
              "1203  0.9765  0.6667\n",
              "1204  0.9765  0.6667\n",
              "1205  0.9647  0.2381\n",
              "1206  0.9647  0.2857\n",
              "1207  0.9529  0.6667\n",
              "1208  0.6471  0.6190\n",
              "1209  0.9529  0.2857\n",
              "1210  0.7294  0.6190\n",
              "1211  0.9882  0.3333\n",
              "1212  0.9059  0.3333\n",
              "1213  0.8471  0.3333\n",
              "1214  0.9647  0.2381\n",
              "1215  0.9765  0.6667\n",
              "1216  0.9765  0.2857\n",
              "1217  0.8471  0.6190\n",
              "1218  0.9412  0.3333\n",
              "1219  0.9765  0.2857\n",
              "1220  0.8941  0.6667\n",
              "1221  0.9765  0.3333\n",
              "1222  0.9765  0.6667\n",
              "1223  0.9176  0.6190\n",
              "1224  0.9647  0.2381\n",
              "1225  0.9882  0.6667\n",
              "1226  0.9765  0.3333\n",
              "1227  0.9765  0.6667\n",
              "1228  0.9765  0.6190\n",
              "1229  0.9882  0.6667\n",
              "1230  0.7059  0.7619\n",
              "1231  0.8118  0.7143\n",
              "1232  0.9765  0.5714\n",
              "1233  0.7882  0.6667\n",
              "1234  0.9647  0.6190\n",
              "1235  0.8588  0.3333\n",
              "1236  0.9882  0.6667\n",
              "1237  0.8706  0.6667\n",
              "1238  0.9765  0.6667\n",
              "1239  0.8824  0.6190\n",
              "1240  0.9294  0.3333\n",
              "1241  0.9765  0.6667\n",
              "1242  0.9647  0.6667\n",
              "1243  0.9882  0.6190\n",
              "1244  0.9412  0.2381\n",
              "1245  0.8235  0.3333\n",
              "1246  0.9765  0.6667\n",
              "1247  0.9647  0.6667\n",
              "1248  0.9765  0.2381\n",
              "1249  0.9059  0.6667\n",
              "1250  0.9294  0.3810\n",
              "1251  0.9882  0.2381\n",
              "1252  0.7059  0.7619\n",
              "1253  0.8471  0.6667\n",
              "1254  0.9529  0.6667\n",
              "1255  0.9765  0.6667\n",
              "1256  0.9882  0.2381\n",
              "1257  0.8353  0.3333\n",
              "1258  0.9647  0.5238\n",
              "1259  0.9882  0.2857\n",
              "1260  0.9529  0.2381\n",
              "1261  0.8353  0.5714\n",
              "1262  0.8706  0.3333\n",
              "1263  0.9529  0.3810\n",
              "1264  0.9647  0.4762\n",
              "1265  0.9882  0.6667\n",
              "1266  0.8941  0.5714\n",
              "1267  0.9412  0.2381\n",
              "1268  0.9882  0.4762\n",
              "1269  0.9882  0.2857\n",
              "1270  0.9529  0.2857\n",
              "1271  0.9647  0.6190\n",
              "1272  0.9765  0.5238\n",
              "1273  0.6471  0.6667\n",
              "1274  0.8118  0.6190\n",
              "1275  0.9765  0.6667\n",
              "1276  0.9882  0.6667\n",
              "1277  0.8588  0.6667\n",
              "1278  0.9647  0.2381\n",
              "1279  0.9647  0.5238\n",
              "1280  0.9765  0.6667\n",
              "1281  0.8588  0.3333\n",
              "1282  0.8235  0.6667\n",
              "1283  0.9529  0.6667\n",
              "1284  0.9529  0.3333\n",
              "1285  0.9882  0.5238\n",
              "1286  0.9765  0.6667\n",
              "1287  0.9882  0.6667\n",
              "1288  0.9882  0.2381\n",
              "1289  0.8471  0.1905\n",
              "1290  0.9765  0.4286\n",
              "1291  0.9765  0.6667\n",
              "1292  0.9412  0.5238\n",
              "1293  0.8471  0.6667\n",
              "1294  0.7059  0.6190\n",
              "1295  0.8588  0.2381\n",
              "1296  0.8353  0.6667\n",
              "1297  0.9647  0.2857\n",
              "1298  0.9647  0.6667\n",
              "1299  0.7882  0.4762\n",
              "1300  0.9647  0.3810\n",
              "1301  0.8353  0.3333\n",
              "1302  0.9765  0.6667\n",
              "1303  0.9882  0.4286\n",
              "1304  0.8824  0.6667\n",
              "1305  0.8824  0.6667\n",
              "1306  0.9529  0.2381\n",
              "1307  0.9882  0.2857\n",
              "1308  0.9882  0.6667\n",
              "1309  0.9882  0.2381\n",
              "1310  0.9882  0.6190\n",
              "1311  0.9529  0.3810\n",
              "1312  0.9882  0.5238\n",
              "1313  0.9882  0.2857\n",
              "1314  0.9647  0.6667\n",
              "1315  0.9765  0.6667\n",
              "1316  0.6824  0.6190\n",
              "1317  0.9647  0.6667\n",
              "1318  0.9765  0.5714\n",
              "1319  0.8235  0.3333\n",
              "1320  0.9059  0.6190\n",
              "1321  0.9647  0.3333\n",
              "1322  0.9765  0.4762\n",
              "1323  0.9765  0.4762\n",
              "1324  0.7176  0.3810\n",
              "1325  0.9765  0.5714\n",
              "1326  0.8588  0.5238\n",
              "1327  0.8353  0.3333\n",
              "1328  0.9647  0.6190\n",
              "1329  0.9765  0.6667\n",
              "1330  0.9059  0.6667\n",
              "1331  0.9765  0.5714\n",
              "1332  0.9647  0.6667\n",
              "1333  0.9882  0.5238\n",
              "1334  0.9765  0.6667\n",
              "1335  0.9765  0.6667\n",
              "1336  0.9412  0.2381\n",
              "1337  0.9765  0.6190\n",
              "1338  0.9647  0.6190\n",
              "1339  0.9765  0.2381\n",
              "1340  0.9882  0.6667\n",
              "1341  0.6824  0.3333\n",
              "1342  0.9882  0.7143\n",
              "1343  0.9882  0.6667\n",
              "1344  0.9059  0.6667\n",
              "1345  0.8588  0.3333\n",
              "1346  0.9647  0.6667\n",
              "1347  0.8471  0.2381\n",
              "1348  0.8588  0.6667\n",
              "1349  0.9765  0.2857\n",
              "1350  0.8118  0.5238\n",
              "1351  0.9765  0.5714\n",
              "1352  0.9882  0.5714\n",
              "1353  0.9765  0.7143\n",
              "1354  0.9882  0.6190\n",
              "1355  0.7765  0.3333\n",
              "1356  0.9647  0.2381\n",
              "1357  0.9647  0.5714\n",
              "1358  0.8353  0.6667\n",
              "1359  0.9765  0.6667\n",
              "1360  0.9647  0.6190\n",
              "1361  0.6941  0.6667\n",
              "1362  0.9529  0.2381\n",
              "1363  0.9882  0.5714\n",
              "1364  0.9765  0.4286\n",
              "1365  0.9647  0.6190\n",
              "1366  0.8353  0.6667\n",
              "1367  0.8706  0.7143\n",
              "1368  0.9647  0.6667\n",
              "1369  0.9294  0.6190\n",
              "1370  0.8588  0.6190\n",
              "1371  0.9765  0.2857\n",
              "1372  0.8471  0.7143\n",
              "1373  0.9882  0.5238\n",
              "1374  0.9765  0.6190\n",
              "1375  0.8471  0.4286\n",
              "1376  0.9294  0.2381\n",
              "1377  0.9647  0.3810\n",
              "1378  0.9412  0.6667\n",
              "1379  0.9529  0.5714\n",
              "1380  0.9765  0.6667\n",
              "1381  0.9529  0.3333\n",
              "1382  0.9647  0.4286\n",
              "1383  0.6471  0.6667\n",
              "1384  0.8235  0.6667\n",
              "1385  0.9059  0.6667\n",
              "1386  0.9647  0.6667\n",
              "1387  0.9647  0.2857\n",
              "1388  0.9765  0.4762\n",
              "1389  0.9647  0.6190\n",
              "1390  0.9765  0.6667\n",
              "1391  0.9647  0.3333\n",
              "1392  0.8235  0.3333\n",
              "1393  0.8588  0.3333\n",
              "1394  0.9882  0.6667\n",
              "1395  0.9765  0.6667\n",
              "1396  0.9882  0.2857\n",
              "1397  0.9647  0.5238\n",
              "1398  0.8706  0.6667\n",
              "1399  0.9882  0.6667\n",
              "1400  0.9412  0.3333\n",
              "1401  0.9529  0.6667\n",
              "1402  0.8824  0.4762\n",
              "1403  0.9882  0.6667\n",
              "1404  0.7529  0.5238\n",
              "1405  0.9647  0.3810\n",
              "1406  0.8235  0.3333\n",
              "1407  0.9647  0.6667\n",
              "1408  0.9647  0.4762\n",
              "1409  0.8118  0.6667\n",
              "1410  0.8471  0.7143\n",
              "1411  0.9882  0.6667\n",
              "1412  0.9765  0.5238\n",
              "1413  0.9765  0.5238\n",
              "1414  0.8235  0.3333\n",
              "1415  0.8588  0.2857\n",
              "1416  0.9647  0.7143\n",
              "1417  0.9765  0.4286\n",
              "1418  0.9176  0.2381\n",
              "1419  0.9882  0.3333\n",
              "1420  0.9647  0.3333\n",
              "1421  0.9765  0.2857\n",
              "1422  0.9765  0.6667\n",
              "1423  0.9647  0.6667\n",
              "1424  0.9529  0.3333\n",
              "1425  0.9765  0.2381\n",
              "1426  0.7059  0.6190\n",
              "1427  0.9647  0.5714\n",
              "1428  0.9765  0.6190\n",
              "1429  0.8118  0.6667\n",
              "1430  0.9882  0.6667\n",
              "1431  0.8706  0.5238\n",
              "1432  0.7882  0.3333\n",
              "1433  0.9294  0.6667\n",
              "1434  0.9765  0.3333\n",
              "1435  0.8588  0.6667\n",
              "1436  0.9882  0.7143\n",
              "1437  0.8118  0.3333\n",
              "1438  0.9647  0.4762\n",
              "1439  0.9647  0.6667\n",
              "1440  0.8588  0.6667\n",
              "1441  0.9765  0.2381\n",
              "1442  0.9412  0.2381\n",
              "1443  0.9647  0.6667\n",
              "1444  0.9882  0.6667\n",
              "1445  0.9882  0.6190\n",
              "1446  0.9765  0.6667\n",
              "1447  0.7059  0.6190\n",
              "1448  0.9647  0.2857\n",
              "1449  0.8118  0.6667\n",
              "1450  0.9882  0.6190\n",
              "1451  0.9294  0.6667\n",
              "1452  0.9412  0.2381\n",
              "1453  0.9647  0.6667\n",
              "1454  0.8471  0.6190\n",
              "1455  0.9882  0.6667\n",
              "1456  0.8353  0.5714\n",
              "1457  0.8588  0.6667\n",
              "1458  0.8471  0.6190\n",
              "1459  0.9176  0.6667\n",
              "1460  0.9765  0.4286\n",
              "1461  0.9882  0.6667\n",
              "1462  0.9529  0.2381\n",
              "1463  0.9647  0.6667\n",
              "1464  0.7765  0.6667\n",
              "1465  0.9647  0.6667\n",
              "1466  0.9647  0.2381\n",
              "1467  0.9882  0.6667\n",
              "1468  0.8941  0.6667\n",
              "1469  0.7059  0.6667\n",
              "1470  0.9882  0.7143\n",
              "1471  0.9765  0.6667\n",
              "1472  0.8353  0.6667\n",
              "1473  0.9647  0.6667\n",
              "1474  0.9882  0.6667\n",
              "1475  0.9647  0.6667\n",
              "1476  0.8353  0.7143\n",
              "1477  0.9882  0.2381\n",
              "1478  0.9294  0.5714\n",
              "1479  0.8353  0.6667\n",
              "1480  0.9882  0.2857\n",
              "1481  0.9765  0.3810\n",
              "1482  0.7882  0.3333\n",
              "1483  0.9647  0.6667\n",
              "1484  0.9765  0.6667\n",
              "1485  0.8824  0.3810\n",
              "1486  0.9529  0.6667\n",
              "1487  0.9412  0.6667\n",
              "1488  0.9765  0.6667\n",
              "1489  0.9647  0.6667\n",
              "1490  0.9647  0.2381\n",
              "1491  0.6941  0.4762\n",
              "1492  0.7765  0.6667\n",
              "1493  0.8353  0.6190\n",
              "1494  0.9529  0.6667\n",
              "1495  0.8471  0.3333\n",
              "1496  0.9647  0.6667\n",
              "1497  0.9765  0.2381\n",
              "1498  0.8471  0.2857\n",
              "1499  0.9765  0.6667\n",
              "1500  0.9765  0.6190\n",
              "1501  0.9647  0.6190\n",
              "1502  0.8588  0.6190\n",
              "1503  0.9294  0.3810\n",
              "1504  0.9765  0.6190\n",
              "1505  0.9647  0.2857\n",
              "1506  0.9882  0.2857\n",
              "1507  0.9765  0.2857\n",
              "1508  0.8706  0.7143\n",
              "1509  0.9882  0.6667\n",
              "1510  0.9647  0.4762\n",
              "1511  0.9412  0.2381\n",
              "1512  0.9529  0.3810\n",
              "1513  0.7647  0.6190\n",
              "1514  0.8706  0.3810\n",
              "1515  0.8353  0.6667\n",
              "1516  0.9882  0.3333\n",
              "1517  0.9529  0.3333\n",
              "1518  0.9647  0.3810\n",
              "1519  0.9765  0.6190\n",
              "1520  0.6824  0.6667\n",
              "1521  0.9647  0.3333\n",
              "1522  0.9529  0.2381\n",
              "1523  0.8471  0.6667\n",
              "1524  0.9882  0.6667\n",
              "1525  0.9765  0.6667\n",
              "1526  0.9882  0.1905\n",
              "1527  0.9882  0.7143\n",
              "1528  0.9412  0.6667\n",
              "1529  0.9176  0.2857\n",
              "1530  0.9294  0.2381\n",
              "1531  0.9529  0.3810\n",
              "1532  0.9882  0.6667\n",
              "1533  0.9647  0.2381\n",
              "1534  0.9882  0.6667\n",
              "1535  0.9765  0.2857\n",
              "1536  0.7176  0.6190\n",
              "1537  0.9765  0.6190\n",
              "1538  0.7294  0.3333\n",
              "1539  0.9647  0.6667\n",
              "1540  0.9647  0.6667\n",
              "1541  0.7176  0.3333\n",
              "1542  0.9529  0.6190\n",
              "1543  0.9176  0.3333\n",
              "1544  0.9412  0.2857\n",
              "1545  0.8588  0.6190\n",
              "1546  0.9882  0.5238\n",
              "1547  0.6588  0.3333\n",
              "1548  0.9647  0.6667\n",
              "1549  0.9294  0.6667\n",
              "1550  0.9882  0.6667\n",
              "1551  0.7647  0.3333\n",
              "1552  0.9765  0.6667\n",
              "1553  0.9765  0.6667\n",
              "1554  0.9765  0.2381\n",
              "1555  0.9294  0.3810\n",
              "1556  0.9882  0.4762\n",
              "1557  0.7176  0.6667\n",
              "1558  0.9765  0.3810\n",
              "1559  0.8235  0.6190\n",
              "1560  0.9765  0.2857\n",
              "1561  0.9647  0.3333\n",
              "1562  0.9647  0.5238\n",
              "1563  0.7412  0.3333\n",
              "1564  0.9765  0.6667\n",
              "1565  0.9882  0.7143\n",
              "1566  0.9882  0.6190\n",
              "1567  0.8824  0.6190\n",
              "1568  0.8824  0.6667\n",
              "1569  0.9765  0.6667\n",
              "1570  0.9765  0.6667\n",
              "1571  0.8471  0.6667\n",
              "1572  0.9647  0.6667\n",
              "1573  0.9765  0.6667\n",
              "1574  0.8235  0.3333\n",
              "1575  0.9647  0.6667\n",
              "1576  0.9647  0.4762\n",
              "1577  0.9882  0.7143\n",
              "1578  0.8824  0.6667\n",
              "1579  0.7176  0.6190\n",
              "1580  0.8235  0.6667\n",
              "1581  0.9765  0.6667\n",
              "1582  0.9294  0.2857\n",
              "1583  0.9765  0.6190\n",
              "1584  0.9882  0.7143\n",
              "1585  0.8471  0.7143\n",
              "1586  0.9647  0.2381\n",
              "1587  0.9647  0.6667\n",
              "1588  0.8706  0.6667\n",
              "1589  0.9647  0.5238\n",
              "1590  0.9765  0.5238\n",
              "1591  0.7882  0.3333\n",
              "1592  0.9765  0.6667\n",
              "1593  0.9765  0.7143\n",
              "1594  0.9647  0.6667\n",
              "1595  0.9529  0.3333\n",
              "1596  0.9765  0.2857\n",
              "1597  0.8941  0.5714\n",
              "1598  0.9647  0.2381\n",
              "1599  0.9176  0.3333\n",
              "1600  0.6941  0.6190\n",
              "1601  0.9765  0.2381\n",
              "1602  0.7176  0.3333\n",
              "1603  0.9647  0.3333\n",
              "1604  0.9647  0.2381\n",
              "1605  0.9647  0.6667\n",
              "1606  0.8471  0.3333\n",
              "1607  0.8353  0.2857\n",
              "1608  0.9647  0.6667\n",
              "1609  0.9882  0.3810\n",
              "1610  0.8706  0.6190\n",
              "1611  0.9647  0.6667\n",
              "1612  0.9647  0.4286\n",
              "1613  0.9882  0.4762\n",
              "1614  0.9765  0.6667\n",
              "1615  0.9765  0.3333\n",
              "1616  0.9529  0.5238\n",
              "1617  0.9647  0.1905\n",
              "1618  0.7882  0.3333\n",
              "1619  0.9765  0.6667\n",
              "1620  0.9882  0.6667\n",
              "1621  0.9412  0.2857\n",
              "1622  0.9765  0.6667\n",
              "1623  0.7647  0.4762\n",
              "1624  0.8000  0.6190\n",
              "1625  0.8824  0.7143\n",
              "1626  0.9882  0.6190\n",
              "1627  0.9529  0.1905\n",
              "1628  0.9882  0.6667\n",
              "1629  0.8706  0.3333\n",
              "1630  0.8941  0.6190\n",
              "1631  0.9647  0.1905\n",
              "1632  0.9882  0.6190\n",
              "1633  0.8235  0.6190\n",
              "1634  0.9529  0.3333\n",
              "1635  0.9765  0.6667\n",
              "1636  0.9765  0.3810\n",
              "1637  0.9882  0.6667\n",
              "1638  0.9294  0.6667\n",
              "1639  0.9882  0.2381\n",
              "1640  0.9765  0.5714\n",
              "1641  0.9529  0.6667\n",
              "1642  0.9765  0.1905\n",
              "1643  0.9647  0.6667\n",
              "1644  0.7059  0.6190\n",
              "1645  0.9647  0.1905\n",
              "1646  0.8235  0.6667\n",
              "1647  0.7294  0.6667\n",
              "1648  0.9765  0.2857\n",
              "1649  0.9882  0.6667\n",
              "1650  0.9765  0.6667\n",
              "1651  0.7176  0.5714\n",
              "1652  0.8941  0.3333\n",
              "1653  0.9647  0.6667\n",
              "1654  0.9647  0.6667\n",
              "1655  0.8706  0.5238\n",
              "1656  0.9765  0.6667\n",
              "1657  0.8941  0.6667\n",
              "1658  0.9882  0.2381\n",
              "1659  0.9294  0.5238\n",
              "1660  0.7176  0.5238\n",
              "1661  0.9647  0.8095\n",
              "1662  0.9294  0.4286\n",
              "1663  0.9765  0.2381\n",
              "1664  0.9647  0.6190\n",
              "1665  0.9647  0.3333\n",
              "1666  0.7176  0.6190\n",
              "1667  0.9647  0.3333\n",
              "1668  0.9882  0.6667\n",
              "1669  0.8235  0.3333\n",
              "1670  0.9765  0.6190\n",
              "1671  0.9529  0.6667\n",
              "1672  0.8235  0.7143\n",
              "1673  0.9765  0.6667\n",
              "1674  0.8353  0.6190\n",
              "1675  0.9529  0.1905\n",
              "1676  0.8353  0.6667\n",
              "1677  0.8588  0.3810\n",
              "1678  0.9294  0.6190\n",
              "1679  0.8588  0.6667\n",
              "1680  0.9882  0.4762\n",
              "1681  0.9882  0.7143\n",
              "1682  0.9765  0.4762\n",
              "1683  0.9765  0.6667\n",
              "1684  0.6706  0.3810\n",
              "1685  0.9765  0.6667\n",
              "1686  0.9765  0.3333\n",
              "1687  0.9882  0.5238\n",
              "1688  0.6941  0.6190\n",
              "1689  0.8588  0.6667\n",
              "1690  0.9294  0.6667\n",
              "1691  0.9882  0.2381\n",
              "1692  0.9765  0.6667\n",
              "1693  0.8588  0.6667\n",
              "1694  0.9882  0.3333\n",
              "1695  0.8235  0.3333\n",
              "1696  0.9529  0.5238\n",
              "1697  0.9765  0.6190\n",
              "1698  0.8353  0.6667\n",
              "1699  0.9882  0.6667\n",
              "1700  0.9882  0.6667\n",
              "1701  0.9882  0.7143\n",
              "1702  0.9647  0.4762\n",
              "1703  0.8941  0.6667\n",
              "1704  0.9647  0.4762\n",
              "1705  0.8706  0.2857\n",
              "1706  0.9882  0.6667\n",
              "1707  0.8824  0.6667\n",
              "1708  0.9765  0.6667\n",
              "1709  0.9529  0.2381\n",
              "1710  0.6824  0.7619\n",
              "1711  0.8235  0.6667\n",
              "1712  0.9882  0.6667\n",
              "1713  0.8824  0.6667\n",
              "1714  0.9647  0.6667\n",
              "1715  0.9647  0.6190\n",
              "1716  0.7176  0.6667\n",
              "1717  0.9529  0.2381\n",
              "1718  0.9882  0.6667\n",
              "1719  0.8588  0.6667\n",
              "1720  0.9882  0.6667\n",
              "1721  0.9765  0.6190\n",
              "1722  0.9176  0.2857\n",
              "1723  0.9647  0.5238\n",
              "1724  0.9529  0.3810\n",
              "1725  0.9765  0.2857\n",
              "1726  0.9647  0.6667\n",
              "1727  0.9176  0.6667\n",
              "1728  0.8941  0.6667\n",
              "1729  0.9647  0.6667\n",
              "1730  0.9882  0.6190\n",
              "1731  0.9647  0.2381\n",
              "1732  0.9882  0.3333\n",
              "1733  0.6471  0.6667\n",
              "1734  0.7765  0.3333\n",
              "1735  0.9882  0.6667\n",
              "1736  0.9647  0.6667\n",
              "1737  0.9765  0.6667\n",
              "1738  0.9412  0.2381\n",
              "1739  0.8471  0.3333\n",
              "1740  0.9765  0.3333\n",
              "1741  0.8588  0.6667\n",
              "1742  0.8353  0.6667\n",
              "1743  0.8471  0.6667\n",
              "1744  0.9647  0.4286\n",
              "1745  0.9882  0.2857\n",
              "1746  0.9765  0.6667\n",
              "1747  0.9765  0.6667\n",
              "1748  0.9765  0.6667\n",
              "1749  0.9882  0.5714\n",
              "1750  0.9647  0.3333\n",
              "1751  0.9529  0.6667\n",
              "1752  0.9294  0.6667\n",
              "1753  0.9765  0.6667\n",
              "1754  0.6824  0.6190\n",
              "1755  0.9059  0.6667\n",
              "1756  0.7294  0.3333\n",
              "1757  0.9882  0.7143\n",
              "1758  0.9882  0.6190\n",
              "1759  0.9647  0.4762\n",
              "1760  0.7176  0.3333\n",
              "1761  0.9882  0.6667\n",
              "1762  0.8941  0.6190\n",
              "1763  0.9647  0.3333\n",
              "1764  0.8353  0.5238\n",
              "1765  0.9529  0.4762\n",
              "1766  0.9882  0.6667\n",
              "1767  0.8353  0.3333\n",
              "1768  0.9647  0.6667\n",
              "1769  0.9647  0.6190\n",
              "1770  0.8235  0.3333\n",
              "1771  0.9882  0.6667\n",
              "1772  0.9647  0.4762\n",
              "1773  0.9882  0.7143\n",
              "1774  0.9765  0.3333\n",
              "1775  0.7059  0.6667\n",
              "1776  0.9765  0.6667\n",
              "1777  0.8235  0.3333\n",
              "1778  0.9647  0.6667\n",
              "1779  0.9529  0.4286\n",
              "1780  0.9765  0.6667\n",
              "1781  0.8353  0.3333\n",
              "1782  0.9765  0.6667\n",
              "1783  0.9765  0.6667\n",
              "1784  0.8471  0.3333\n",
              "1785  0.8706  0.3333\n",
              "1786  0.9647  0.2857\n",
              "1787  0.8824  0.6667\n",
              "1788  0.9765  0.6667\n",
              "1789  0.9529  0.2381\n",
              "1790  0.8824  0.3333\n",
              "1791  0.9529  0.4286\n",
              "1792  0.9882  0.6667\n",
              "1793  0.9529  0.6667\n",
              "1794  0.8824  0.6667\n",
              "1795  0.9294  0.6667\n",
              "1796  0.9765  0.6190\n",
              "1797  0.9647  0.6667\n",
              "1798  0.6471  0.6667\n",
              "1799  0.9882  0.7143\n",
              "1800  0.9882  0.2381\n",
              "1801  0.9647  0.3333\n",
              "1802  0.8118  0.3333\n",
              "1803  0.9647  0.3333\n",
              "1804  0.8471  0.6667\n",
              "1805  0.9882  0.4286\n",
              "1806  0.9882  0.6667\n",
              "1807  0.8706  0.3810\n",
              "1808  0.9765  0.2381\n",
              "1809  0.9647  0.5714\n",
              "1810  0.9882  0.6667\n",
              "1811  0.8353  0.3333\n",
              "1812  0.9882  0.2381\n",
              "1813  0.9647  0.4286\n",
              "1814  0.9647  0.3333\n",
              "1815  0.9529  0.6190\n",
              "1816  0.8941  0.6667\n",
              "1817  0.8353  0.5238\n",
              "1818  0.9529  0.6667\n",
              "1819  0.9529  0.6190\n",
              "1820  0.9765  0.4762\n",
              "1821  0.6941  0.7619\n",
              "1822  0.8353  0.3333\n",
              "1823  0.9882  0.6667\n",
              "1824  0.9529  0.7143\n",
              "1825  0.8353  0.3333\n",
              "1826  0.9882  0.6667\n",
              "1827  0.9529  0.6667\n",
              "1828  0.7294  0.6667\n",
              "1829  0.8353  0.6667\n",
              "1830  0.9765  0.2857\n",
              "1831  0.9647  0.4762\n",
              "1832  0.9647  0.6667\n",
              "1833  0.9765  0.6667\n",
              "1834  0.9882  0.6667\n",
              "1835  0.9765  0.6667\n",
              "1836  0.9647  0.6667\n",
              "1837  0.9765  0.5714\n",
              "1838  0.9882  0.3333\n",
              "1839  0.8471  0.6667\n",
              "1840  0.9647  0.2857\n",
              "1841  0.9765  0.2381\n",
              "1842  0.6824  0.6190\n",
              "1843  0.8824  0.4286\n",
              "1844  0.7647  0.6667\n",
              "1845  0.9765  0.7143\n",
              "1846  0.9765  0.6190\n",
              "1847  0.8588  0.3333\n",
              "1848  0.9412  0.2857\n",
              "1849  0.8941  0.6667\n",
              "1850  0.9882  0.6667\n",
              "1851  0.8706  0.5714\n",
              "1852  0.9882  0.5238\n",
              "1853  0.9882  0.2857\n",
              "1854  0.7294  0.6667\n",
              "1855  0.9765  0.3333\n",
              "1856  0.9294  0.5714\n",
              "1857  0.9412  0.6667\n",
              "1858  0.9882  0.2857\n",
              "1859  0.9647  0.5714\n",
              "1860  0.9647  0.3333\n",
              "1861  0.9882  0.2857\n",
              "1862  0.6824  0.5714\n",
              "1863  0.9412  0.6667\n",
              "1864  0.9765  0.6667\n",
              "1865  0.8235  0.3333\n",
              "1866  0.8000  0.6667\n",
              "1867  0.9647  0.6667\n",
              "1868  0.9529  0.2857\n",
              "1869  0.8471  0.3333\n",
              "1870  0.8588  0.5714\n",
              "1871  0.9647  0.3810\n",
              "1872  0.8706  0.2857\n",
              "1873  0.9412  0.6667\n",
              "1874  0.9765  0.5714\n",
              "1875  0.9765  0.4762\n",
              "1876  0.8471  0.6667\n",
              "1877  0.9765  0.5714\n",
              "1878  0.9412  0.3333\n",
              "1879  0.9765  0.6667\n",
              "1880  0.8235  0.2857\n",
              "1881  0.9294  0.2857\n",
              "1882  0.9882  0.4286\n",
              "1883  0.9765  0.3333\n",
              "1884  0.7176  0.6190\n",
              "1885  0.9882  0.4286\n",
              "1886  0.8235  0.6667\n",
              "1887  0.9529  0.2857\n",
              "1888  0.9765  0.6667\n",
              "1889  0.9765  0.6667"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-68bf44fd-5a60-4dda-ae7f-1813f6787f54\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.7647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.7529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.6471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.6471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.7412</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.7294</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.7647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.7294</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.7765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.7412</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.7529</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.7412</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.6471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.6471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.7412</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>0.7412</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>0.6706</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.7294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>0.7294</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>0.6471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>0.7765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>0.7412</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>0.6824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>236</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>238</th>\n",
              "      <td>0.7412</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>257</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>0.7647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>0.6824</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>306</th>\n",
              "      <td>0.6824</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>316</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>324</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>325</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>326</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>327</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>328</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329</th>\n",
              "      <td>0.6824</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>331</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>332</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333</th>\n",
              "      <td>0.7647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>334</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>335</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>336</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>337</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>338</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>344</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>351</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>352</th>\n",
              "      <td>0.7412</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>353</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>354</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>355</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>358</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>365</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>366</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>368</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>373</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>381</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>384</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>387</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>389</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.8095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>402</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>405</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>406</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>407</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>408</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>409</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>410</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>412</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>415</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>417</th>\n",
              "      <td>0.6824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>418</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>419</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>422</th>\n",
              "      <td>0.7647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>430</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>432</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>442</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>443</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>444</th>\n",
              "      <td>0.7294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>445</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>446</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>447</th>\n",
              "      <td>0.7294</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>448</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>449</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>450</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>451</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>452</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>453</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>455</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>456</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>457</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>458</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>459</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>460</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>461</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>462</th>\n",
              "      <td>0.6706</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>463</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>465</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>466</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>467</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>468</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>469</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>470</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>471</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>472</th>\n",
              "      <td>0.6706</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>473</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>474</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>475</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>476</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>477</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>478</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>479</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>480</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>482</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>483</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>484</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>485</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>487</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>488</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>489</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>490</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>492</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>493</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>0.7882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>500</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>504</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>506</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>507</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>511</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>518</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>519</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>520</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>521</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.1429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>522</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>523</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>524</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>525</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>526</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>527</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>528</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>529</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>530</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>531</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>533</th>\n",
              "      <td>0.7765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>534</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>535</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>536</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>537</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>538</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>540</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>541</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>542</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>543</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>544</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>545</th>\n",
              "      <td>0.7765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>546</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>547</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>548</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>549</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>551</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>552</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>553</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>554</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>555</th>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>556</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>557</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>558</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>559</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>560</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>561</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>562</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>563</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>569</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>570</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>571</th>\n",
              "      <td>0.6824</td>\n",
              "      <td>0.7619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>572</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>573</th>\n",
              "      <td>0.7647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>574</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>575</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>576</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>577</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>578</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>579</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>580</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>582</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>583</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>584</th>\n",
              "      <td>0.7882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>585</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>586</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>587</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>588</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>589</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>590</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>592</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>593</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>594</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>595</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>596</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>597</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>598</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>599</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>600</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>601</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>602</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>603</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>604</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>605</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>606</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>607</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>608</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>609</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>611</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>612</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>613</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>614</th>\n",
              "      <td>0.7882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>615</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>616</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>617</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>618</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>619</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>620</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>621</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>622</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>623</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>624</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>625</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>626</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>627</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>628</th>\n",
              "      <td>0.7647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>629</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>631</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>632</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>633</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>634</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>635</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>636</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>637</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>638</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>639</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>640</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>641</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>642</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>643</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>644</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>645</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>646</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>647</th>\n",
              "      <td>0.7882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>648</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>650</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>651</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>652</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>653</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>654</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>655</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>656</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>657</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>658</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>659</th>\n",
              "      <td>0.7765</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>660</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>661</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>662</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>663</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>664</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>665</th>\n",
              "      <td>0.7647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>666</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>667</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>668</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>669</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>670</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>671</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>672</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>673</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>674</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>675</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>676</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>677</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>678</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>679</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>680</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>681</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>682</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>683</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>684</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>685</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>686</th>\n",
              "      <td>0.6706</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>687</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>688</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>689</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>690</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>691</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>692</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>693</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>694</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>696</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>697</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>698</th>\n",
              "      <td>0.6824</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>700</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>701</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>702</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>703</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>704</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>705</th>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>706</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>707</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>708</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>709</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>710</th>\n",
              "      <td>0.7412</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>711</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>712</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>713</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>714</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>715</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>716</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>717</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>718</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>719</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>720</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>721</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>722</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>723</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>724</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>725</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>726</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>727</th>\n",
              "      <td>0.6824</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>728</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>729</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>730</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>731</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>732</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>733</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>734</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>735</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>736</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>737</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>738</th>\n",
              "      <td>0.7529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>739</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>740</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>741</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>742</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>743</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>744</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>745</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>746</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>747</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>748</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>749</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750</th>\n",
              "      <td>0.7765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751</th>\n",
              "      <td>0.6824</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>752</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>753</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>754</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>755</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>756</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>757</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>758</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>759</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>760</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>761</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>762</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>768</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>769</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>770</th>\n",
              "      <td>0.6706</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>771</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>772</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>773</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>774</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>775</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>776</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>777</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>778</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>779</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>780</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>781</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>782</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>783</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>784</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>785</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>786</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>787</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>788</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>789</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>790</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>791</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792</th>\n",
              "      <td>0.6824</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>793</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>794</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>0.7294</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>801</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>802</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>803</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>804</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>805</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>806</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>807</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>808</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>809</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>810</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>811</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>812</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>813</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>814</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>815</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>816</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>817</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>818</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>819</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>820</th>\n",
              "      <td>0.7412</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>821</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>822</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>823</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>824</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>825</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>826</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>827</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>828</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>829</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>830</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>831</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>832</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>833</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>834</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>835</th>\n",
              "      <td>0.6471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>837</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>838</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>839</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>840</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>841</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>842</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>843</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>844</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>845</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>846</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>847</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>848</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>849</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>850</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>851</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>852</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>853</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>854</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>855</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>856</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>857</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>858</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>859</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>861</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>863</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>864</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>865</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>866</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>867</th>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>868</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>869</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>870</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>871</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>872</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>873</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>874</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>875</th>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>876</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>877</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>878</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>879</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>880</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>881</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>882</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>884</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>885</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>891</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>892</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>893</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>894</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>895</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>896</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>897</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>898</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>899</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>900</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>901</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>902</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>903</th>\n",
              "      <td>0.7294</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>904</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>905</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>906</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>907</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>908</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>909</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>910</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>911</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>912</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>913</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>914</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>915</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>916</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>917</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>918</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>919</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>920</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>921</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>922</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>923</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>924</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>925</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>926</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>927</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>928</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>929</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>930</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>931</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>932</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>933</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>934</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>935</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>936</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>937</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>938</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>939</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>940</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>941</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>942</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>943</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>944</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>945</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>946</th>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>947</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>948</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>949</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>950</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>951</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>952</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>953</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>954</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>955</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>956</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>957</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>958</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>959</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>960</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>961</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>962</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>963</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>964</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>965</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>966</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>967</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>968</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>969</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>970</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>971</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>972</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>973</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>974</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>975</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>976</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>977</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>978</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>979</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>980</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>981</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>983</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>984</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>985</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>986</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>988</th>\n",
              "      <td>0.6824</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>989</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>990</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1001</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1002</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1003</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1004</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1005</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1006</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1007</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1008</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1009</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1010</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1011</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1012</th>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1013</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1014</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1015</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1016</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1017</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1018</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1019</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1020</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1021</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1022</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1023</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1024</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1025</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1026</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1027</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1028</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1029</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1030</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1031</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1032</th>\n",
              "      <td>0.6706</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1033</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1034</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1035</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1036</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1037</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1038</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1039</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1040</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1041</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1042</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1043</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1044</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1045</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1046</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1047</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1049</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1050</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1051</th>\n",
              "      <td>0.7882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1052</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1053</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1054</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1055</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1056</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1057</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1058</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1059</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1060</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1061</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1062</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1063</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1064</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1065</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1066</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1067</th>\n",
              "      <td>0.7412</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1068</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1069</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1070</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1071</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1072</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1073</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1074</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1075</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1076</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1077</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1078</th>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1079</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1080</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1081</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1082</th>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1083</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1084</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1086</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1087</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1088</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1089</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1090</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1091</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1092</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1093</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1094</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1095</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1096</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1097</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1098</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1099</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1100</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1101</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1102</th>\n",
              "      <td>0.7294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1103</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1104</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1105</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1106</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1107</th>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1108</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1109</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1110</th>\n",
              "      <td>0.7765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1111</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1112</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1113</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1114</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1115</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1116</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1117</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1118</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1119</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1120</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1121</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1122</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1123</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1124</th>\n",
              "      <td>0.7882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1125</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1126</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1127</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1128</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1129</th>\n",
              "      <td>0.7529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1130</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1131</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1132</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1133</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1134</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1135</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1136</th>\n",
              "      <td>0.7882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1137</th>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1138</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1139</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1140</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1141</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1142</th>\n",
              "      <td>0.6824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1143</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1144</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1145</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1146</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1147</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1148</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1149</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1150</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1151</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1152</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1153</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1154</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1155</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1156</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1157</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1158</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1159</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1160</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1161</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1162</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1163</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1164</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1165</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1166</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1167</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1168</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1169</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1170</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1171</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1172</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1173</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1174</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1175</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1176</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1177</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1178</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1179</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1180</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1181</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1182</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1183</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1184</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1185</th>\n",
              "      <td>0.7765</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1186</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1187</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1188</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1189</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1190</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1191</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1192</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1193</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1194</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1195</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1196</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1197</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1198</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1199</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1200</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1201</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1202</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1203</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1204</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1205</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1206</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1207</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1208</th>\n",
              "      <td>0.6471</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1209</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1210</th>\n",
              "      <td>0.7294</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1211</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1212</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1213</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1214</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1215</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1216</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1217</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1218</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1219</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1220</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1221</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1222</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1223</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1224</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1225</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1226</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1227</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1228</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1229</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1230</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.7619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1231</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1232</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1233</th>\n",
              "      <td>0.7882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1234</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1235</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1236</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1237</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1238</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1239</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1240</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1241</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1242</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1243</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1244</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1245</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1246</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1247</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1248</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1249</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1250</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1251</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1252</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.7619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1253</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1254</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1255</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1256</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1257</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1258</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1259</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1260</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1261</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1262</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1263</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1264</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1265</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1266</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1267</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1268</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1269</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1270</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1271</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1272</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1273</th>\n",
              "      <td>0.6471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1274</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1275</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1276</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1277</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1278</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1279</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1280</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1281</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1282</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1283</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1284</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1285</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1286</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1287</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1288</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1289</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1290</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1291</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1292</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1293</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1294</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1295</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1296</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1297</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1298</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1299</th>\n",
              "      <td>0.7882</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1300</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1301</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1302</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1303</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1304</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1305</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1306</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1307</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1308</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1309</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1310</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1311</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1312</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1313</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1314</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1316</th>\n",
              "      <td>0.6824</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1317</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1318</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1319</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1320</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1321</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1322</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1323</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1324</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1326</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1327</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1328</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1329</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1330</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1331</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1332</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1333</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1334</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1335</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1336</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1337</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1338</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1339</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1340</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1341</th>\n",
              "      <td>0.6824</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1342</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1343</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1344</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1345</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1346</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1347</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1348</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1349</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1350</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1351</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1352</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1353</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1354</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1355</th>\n",
              "      <td>0.7765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1356</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1357</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1358</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1359</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1360</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1361</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1362</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1363</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1364</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1365</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1366</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1367</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1368</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1369</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1370</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1371</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1372</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1373</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1374</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1375</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1376</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1377</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1378</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1379</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1380</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1381</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1382</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1383</th>\n",
              "      <td>0.6471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1384</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1385</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1386</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1387</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1388</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1389</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1390</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1391</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1392</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1393</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1394</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1395</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1396</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1397</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1398</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1400</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1401</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1402</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1403</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1404</th>\n",
              "      <td>0.7529</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1405</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1406</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1407</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1408</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1409</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1410</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1411</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1412</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1413</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1414</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1415</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1416</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1417</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1418</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1419</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1420</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1421</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1422</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1423</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1424</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1425</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1426</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1427</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1428</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1429</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1430</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1431</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1432</th>\n",
              "      <td>0.7882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1433</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1434</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1435</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1436</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1437</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1438</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1439</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1440</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1441</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1442</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1443</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1444</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1445</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1446</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1447</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1448</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1449</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1450</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1451</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1452</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1453</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1454</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1458</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1459</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1460</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1461</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1462</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1463</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1464</th>\n",
              "      <td>0.7765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1465</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1466</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1467</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1468</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1469</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1470</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1471</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1472</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1473</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1474</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1475</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1476</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1477</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1478</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1479</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1480</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1481</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1482</th>\n",
              "      <td>0.7882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1483</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1484</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1485</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1486</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1487</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1488</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1489</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1490</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1491</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1492</th>\n",
              "      <td>0.7765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1493</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1494</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1495</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1496</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1497</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1498</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1499</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1500</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1501</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1502</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1503</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1504</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1505</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1506</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1507</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1508</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1509</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1510</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1511</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1512</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1513</th>\n",
              "      <td>0.7647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1514</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1515</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1516</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1517</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1518</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1519</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1520</th>\n",
              "      <td>0.6824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1522</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1523</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1524</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1525</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1526</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1527</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1528</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1529</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1530</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1531</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1532</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1533</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1534</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1535</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1536</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1537</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1538</th>\n",
              "      <td>0.7294</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1539</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1540</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1541</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1542</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1543</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1544</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1545</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1546</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1547</th>\n",
              "      <td>0.6588</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1548</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1549</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1550</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1551</th>\n",
              "      <td>0.7647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1552</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1553</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1554</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1555</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1556</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1557</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1558</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1559</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1560</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1561</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1562</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1563</th>\n",
              "      <td>0.7412</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1564</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1565</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1566</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1567</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1568</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1569</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1570</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1571</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1572</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1573</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1574</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1575</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1576</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1577</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1578</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1579</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1580</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1581</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1582</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1583</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1584</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1585</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1586</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1587</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1588</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1589</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1590</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1591</th>\n",
              "      <td>0.7882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1592</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1593</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1594</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1595</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1596</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1597</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1598</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1600</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1601</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1602</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1603</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1604</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1605</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1606</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1607</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1608</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1609</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1610</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1611</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1612</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1613</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1614</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1615</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1616</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1617</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1618</th>\n",
              "      <td>0.7882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1619</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1620</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1621</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1622</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1623</th>\n",
              "      <td>0.7647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1624</th>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1625</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1626</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1627</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1628</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1629</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1630</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1631</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1632</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1633</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1634</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1635</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1636</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1637</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1638</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1639</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1640</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1641</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1642</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1643</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1644</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1645</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1646</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1647</th>\n",
              "      <td>0.7294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1648</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1649</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1650</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1651</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1652</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1653</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1654</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1655</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1656</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1657</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1658</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1659</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1660</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1661</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.8095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1662</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1663</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1664</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1665</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1666</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1667</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1668</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1669</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1670</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1671</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1672</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1673</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1674</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1675</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.1905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1676</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1677</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1678</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1679</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1680</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1681</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1682</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1683</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1684</th>\n",
              "      <td>0.6706</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1685</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1686</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1687</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1688</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1689</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1690</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1691</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1692</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1693</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1694</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1695</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1696</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1697</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1698</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1699</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1700</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1701</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1702</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1703</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1704</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1705</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1706</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1707</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1708</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1709</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1710</th>\n",
              "      <td>0.6824</td>\n",
              "      <td>0.7619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1711</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1712</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1713</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1714</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1715</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1716</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1717</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1718</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1719</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1720</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1721</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1722</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1723</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1724</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1725</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1726</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1727</th>\n",
              "      <td>0.9176</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1728</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1729</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1730</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1731</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1733</th>\n",
              "      <td>0.6471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1734</th>\n",
              "      <td>0.7765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1735</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1736</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1737</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1738</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1739</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1740</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1741</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1742</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1743</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1744</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1745</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1746</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1747</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1748</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1749</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1750</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1751</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1752</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1753</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1754</th>\n",
              "      <td>0.6824</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1755</th>\n",
              "      <td>0.9059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1756</th>\n",
              "      <td>0.7294</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1757</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1758</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1759</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1760</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1761</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1762</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1763</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1764</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1765</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1766</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1767</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1768</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1769</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1770</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1771</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1772</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1773</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1774</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1775</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1776</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1777</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1778</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1779</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1780</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1781</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1782</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1783</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1784</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1785</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1786</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1787</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1788</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1789</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1790</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1791</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1792</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1793</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1794</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1795</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1796</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1797</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1798</th>\n",
              "      <td>0.6471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1799</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1800</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1801</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1802</th>\n",
              "      <td>0.8118</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1803</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1804</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1805</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1806</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1807</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1808</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1809</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1810</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1811</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1812</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1813</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1814</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1815</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1816</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1817</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1818</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1819</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1820</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1821</th>\n",
              "      <td>0.6941</td>\n",
              "      <td>0.7619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1822</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1823</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1824</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1825</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1826</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1827</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1828</th>\n",
              "      <td>0.7294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1829</th>\n",
              "      <td>0.8353</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1830</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1831</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1832</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1833</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1834</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1835</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1836</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1837</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1838</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1839</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1840</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1841</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.2381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1842</th>\n",
              "      <td>0.6824</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1843</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1844</th>\n",
              "      <td>0.7647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1845</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.7143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1846</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1847</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1848</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1849</th>\n",
              "      <td>0.8941</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1850</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1851</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1852</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.5238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1853</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1854</th>\n",
              "      <td>0.7294</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1855</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1856</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1857</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1858</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1859</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1860</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1861</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1862</th>\n",
              "      <td>0.6824</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1863</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1864</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1865</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1866</th>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1867</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1868</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1869</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1870</th>\n",
              "      <td>0.8588</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1871</th>\n",
              "      <td>0.9647</td>\n",
              "      <td>0.3810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1872</th>\n",
              "      <td>0.8706</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1873</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1874</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1875</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1876</th>\n",
              "      <td>0.8471</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1877</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.5714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1878</th>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1879</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1880</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1881</th>\n",
              "      <td>0.9294</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1882</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1883</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1884</th>\n",
              "      <td>0.7176</td>\n",
              "      <td>0.6190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1885</th>\n",
              "      <td>0.9882</td>\n",
              "      <td>0.4286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1886</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1887</th>\n",
              "      <td>0.9529</td>\n",
              "      <td>0.2857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1888</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1889</th>\n",
              "      <td>0.9765</td>\n",
              "      <td>0.6667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68bf44fd-5a60-4dda-ae7f-1813f6787f54')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-68bf44fd-5a60-4dda-ae7f-1813f6787f54 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-68bf44fd-5a60-4dda-ae7f-1813f6787f54');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-40d710d1-3de0-48c8-9a73-abb2034f4612\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-40d710d1-3de0-48c8-9a73-abb2034f4612')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-40d710d1-3de0-48c8-9a73-abb2034f4612 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_825004cb-4359-45f8-9446-7b72878cd010\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('Final_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_825004cb-4359-45f8-9446-7b72878cd010 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('Final_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "Final_results",
              "summary": "{\n  \"name\": \"Final_results\",\n  \"rows\": 1890,\n  \"fields\": [\n    {\n      \"column\": \"train\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08498956549472918,\n        \"min\": 0.6471,\n        \"max\": 0.9882,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          0.6824,\n          0.9176,\n          0.9647\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1685742162378643,\n        \"min\": 0.1429,\n        \"max\": 0.8095,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.2857,\n          0.4762,\n          0.7143\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "Final_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b3e8afb",
      "metadata": {
        "id": "2b3e8afb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53e81c7c",
      "metadata": {
        "id": "53e81c7c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a56bd97",
      "metadata": {
        "id": "0a56bd97"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "993f746b",
      "metadata": {
        "id": "993f746b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "2cc72f14",
      "metadata": {
        "id": "2cc72f14",
        "outputId": "696f5bae-1a68-4c6a-8b6d-27e857c497e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6   \\\n",
              "0   -2.637099 -0.347802 -1.305130  0.477352  0.222379  2.597939  1.865067   \n",
              "1   -2.798193  3.634745  2.086230  3.196251  1.620699 -0.670871  1.726479   \n",
              "2   -1.739392 -1.241732 -1.321686 -1.510875 -0.779627 -1.543128  1.477094   \n",
              "3   -0.994942  1.074568  1.187057  1.138465 -0.079325 -4.044148  1.202367   \n",
              "4   -1.329136  2.676327  0.291800  0.271295 -0.906031 -1.966462  1.868663   \n",
              "5   -1.131488  0.223700  0.368406 -1.841575 -0.366648 -3.507539  0.586460   \n",
              "6   -1.042963  0.784338  0.155310  2.144182 -0.328153 -3.037030  2.740792   \n",
              "7   -2.123212  2.594353  0.784899  2.561225  0.298017 -1.477255  2.404881   \n",
              "8   -2.428108  0.587128  0.336312  1.760723 -0.277718 -1.053064  2.272779   \n",
              "9   -1.996928  0.378025  1.192909  1.650560  0.339481 -1.626476  1.924355   \n",
              "10  -2.397497  2.225219  1.587146  3.089433  0.767364 -2.262547  2.458541   \n",
              "11  -3.121665  3.285395  2.648926  2.631635  2.515994  1.229619  0.031375   \n",
              "12  -3.878436 -1.077783  1.972808  1.255909  4.699007  1.939041 -0.401190   \n",
              "13  -4.014367 -1.252404  1.606471  0.522180  4.165790  1.218129  0.170922   \n",
              "14  -3.816117 -1.324373  1.602825  0.552906  4.097568  0.830133 -0.096257   \n",
              "15  -3.689973 -2.155047  0.552009  0.078458  2.731397  0.792978  0.293722   \n",
              "16  -3.664803 -2.112624  0.709510  0.316549  3.421074  0.577376 -0.058770   \n",
              "17  -3.472888 -2.414408  1.069169  0.544240  3.510205  1.037111 -0.485694   \n",
              "18  -0.154480 -1.546400 -0.879493  1.928970 -1.528069  1.393321  0.603145   \n",
              "19  -0.429591 -1.061395 -1.073781  1.603877 -1.000881  0.137796 -0.474644   \n",
              "20   1.169234  0.563495 -0.556821  2.166775 -1.110425 -1.226026 -0.370970   \n",
              "21   1.997442 -0.566104 -2.878561  2.048987 -2.044952  1.675383 -0.844437   \n",
              "22   0.983302 -1.229718 -1.498552  1.839532 -1.240514  0.511564 -0.776103   \n",
              "23   1.557271  0.050281 -1.899174  1.712817 -0.925847 -1.272008 -0.870842   \n",
              "24   1.250082 -0.475840 -1.731295  1.065621 -1.003865 -0.619206  0.185522   \n",
              "25   0.517654 -0.726796 -0.476584  0.659845 -0.101265 -0.393997 -0.535044   \n",
              "26  -0.252770 -2.423221 -1.737167  1.730142 -0.774093 -0.778728 -0.803385   \n",
              "27   1.164473 -1.224640 -0.914450  2.582735 -1.158372 -0.549832 -0.848184   \n",
              "28   1.911768 -0.915630 -0.944989  2.965577 -1.213963 -0.835333 -1.340682   \n",
              "29  -0.408548 -1.834815 -0.125575  3.054345 -0.877931 -0.548621 -0.374316   \n",
              "30   1.467009 -1.673569 -0.998931  3.212867 -1.358372 -1.950890 -1.870762   \n",
              "31   1.693128 -1.905936 -1.482452  2.580142 -0.631265 -1.065471 -0.645155   \n",
              "32   0.326083 -2.357328 -0.837244  0.915371 -0.114251 -0.320591 -0.543085   \n",
              "33  -3.097785 -1.540616 -1.078272 -1.299055  0.463412  1.260984  0.520225   \n",
              "34  -2.872596 -2.289240 -0.718798 -0.856928  0.755319  0.446068  0.125273   \n",
              "35  -2.642872 -3.093603 -1.580577 -0.983180  0.529433  0.042502  0.219207   \n",
              "36  -2.666563 -2.811451 -1.320080 -0.584977  0.549821  0.310147  0.132449   \n",
              "37  -2.522127 -2.950680 -0.912822 -0.742229  0.636310  0.272814 -0.124880   \n",
              "38  -2.824209 -2.713716 -1.641778 -0.771857  0.380510 -0.132937  0.730788   \n",
              "39  -3.420457 -2.746807 -1.231707 -0.784039  1.139511  1.333265  0.306230   \n",
              "40  -1.099295  0.465213  1.334637 -2.250615 -1.491715 -0.653282 -0.849536   \n",
              "41  -1.169595 -2.462802  1.489594 -3.346595 -0.699219 -2.433538 -1.578992   \n",
              "42  -0.188029 -2.039316  2.035923 -2.630550  0.002784 -0.922663 -1.055911   \n",
              "43  -1.340439 -0.345850  3.167113 -2.315758 -0.589326 -1.100225 -1.353995   \n",
              "44  -0.038033 -1.146338  0.056533 -1.408917 -0.829297 -2.161916  0.448988   \n",
              "45  -0.402297 -2.237480  2.388917 -2.855490  0.062406 -0.589928 -0.938151   \n",
              "46  -1.313723 -2.045631  1.158891 -2.354800 -0.732532 -1.073207 -0.775655   \n",
              "47  -1.048675 -1.715145  2.515703 -2.073400 -0.410095 -0.572684 -1.261211   \n",
              "48  -1.276280  0.422902  2.890507 -0.298685 -0.156793  0.877776  0.280889   \n",
              "49  -1.675160 -0.223162 -0.959539 -1.191272 -1.148406  2.156334  2.685866   \n",
              "50  -0.901350  0.623569 -1.150486 -2.642692 -1.667718 -0.243350  1.203611   \n",
              "51  -0.174547  0.067538 -1.066564 -1.551703 -1.708644  1.853259  2.516707   \n",
              "52  -0.282104 -1.011302 -0.691323 -1.847908 -1.607443 -0.038352  1.813729   \n",
              "53  -0.769889 -1.683174 -1.707398 -1.639610 -1.496469 -0.361207  0.058345   \n",
              "54  -0.081574 -1.092008 -1.654655 -2.202721 -0.719607 -0.060184  1.042880   \n",
              "55  -1.234132 -0.838562 -2.011426 -1.940953 -1.865559  0.548632  1.478590   \n",
              "56  -1.377481 -0.065721 -1.081459 -0.250382 -1.244762  3.282794  1.888986   \n",
              "57  -1.136689  5.804210 -2.158707 -0.711264  2.630094 -0.722201 -2.566602   \n",
              "58  -1.971310  4.386899 -4.043365 -1.361281  1.282604 -0.496142 -1.185623   \n",
              "59  -0.847435  4.087546 -3.075522 -1.403359  1.478408 -0.990298 -1.038978   \n",
              "60  -2.034989  5.777314 -3.917526 -1.192115  0.948309  0.384854 -0.618132   \n",
              "61  -2.899686  3.653677 -4.107776 -0.837462  2.314684 -0.099674 -0.377653   \n",
              "62  -3.044610  4.717416 -2.692057 -0.515452  3.082248 -0.087261 -1.969000   \n",
              "63   7.514398 -0.635832  0.391142 -0.657821  1.896845 -0.123870  0.429798   \n",
              "64   6.081498  0.184675  0.796057  0.446617  0.916233  0.107495  1.029045   \n",
              "65   6.417571 -1.035532  0.593525 -1.207628  1.777169 -0.666690  0.538690   \n",
              "66   4.853528  0.146181  0.280188  0.419659 -0.696790  1.923275  0.624137   \n",
              "67   5.767001 -0.226511  0.799813 -0.561765  1.164284  0.518367  0.365658   \n",
              "68   5.465487 -0.097514  0.327609 -0.011427  1.276314  0.224520  0.800988   \n",
              "69   3.633630  6.278797 -2.002323 -1.931037 -0.796981  0.588673 -0.935282   \n",
              "70   6.400256  0.244836 -0.896523 -0.428111  0.628118  1.700274  0.879492   \n",
              "71   6.061324 -0.621618 -0.241531 -0.590321  1.098110  0.639856  0.818704   \n",
              "72   9.025208 -0.402932 -0.213329 -1.001249  2.936993 -0.683901  0.119570   \n",
              "73   7.222217 -1.011508  0.332970 -0.405879  2.436039 -0.023262  0.352652   \n",
              "74   6.948776 -0.575131  1.190701 -0.513953  1.864702  0.053287  0.334423   \n",
              "75   7.114773  0.999335  0.408795 -0.790729  1.222958  0.188703  0.416799   \n",
              "76   8.566109  0.652017  0.555999 -0.039120  1.827801  0.268175  0.529672   \n",
              "77  -0.645925 -2.615459 -1.874452  0.331043 -0.124215  0.806257 -0.853601   \n",
              "78  -1.407767 -2.200869 -1.877372 -0.870802 -0.213478 -1.136117 -0.138414   \n",
              "79   0.321378 -1.875157 -0.427114  1.717297  0.642612 -0.206885 -1.512476   \n",
              "80   0.253918 -0.777346 -0.444279  2.374200 -0.030165 -0.970837 -0.715522   \n",
              "81   1.031057  0.081395 -0.383579  2.403520 -1.243376  0.543347 -1.806651   \n",
              "82   0.807086 -1.028513 -2.842726  1.866244 -0.376761  0.616221 -0.697409   \n",
              "83  -0.395075 -2.009826 -2.418936  1.200081 -0.904394  0.677463 -1.362367   \n",
              "84  -0.186865  1.001387  3.046893  0.714120 -1.772765  1.926453 -0.771086   \n",
              "85   0.237640 -0.383473  1.286859 -0.997945 -1.344278 -2.151856 -0.595879   \n",
              "86  -1.861955 -0.168720  0.976660 -0.534565  0.455795 -0.776867 -0.452693   \n",
              "87  -1.001201 -0.630144  0.452858 -0.791519 -0.471162 -0.120609 -1.146068   \n",
              "88   0.473754  1.692235  2.473895  0.629114 -1.383129 -0.759267 -1.741127   \n",
              "89  -0.487110  2.589771  3.490235  0.222079 -0.976462  1.375417 -0.971469   \n",
              "90  -0.087586  0.320785  0.233389  0.674989 -1.858322  1.800417 -0.060712   \n",
              "91   0.077380  3.572014  0.896210  0.744622 -3.367878  3.256363  0.249904   \n",
              "92  -0.458112  1.236125  4.384637  0.560432 -2.208922  2.313967 -0.802622   \n",
              "93  -0.673502  0.026571  2.225577  0.275774 -0.205987  0.014778 -0.940846   \n",
              "94   0.058923 -0.221803  0.737666  1.151257 -1.184505  0.206355  0.120637   \n",
              "95  -0.760917  0.429009  0.926646 -0.466413 -1.378488  1.120628 -1.294069   \n",
              "96   0.219442  1.270641  3.100799  0.898660 -1.054064  0.788914 -0.990733   \n",
              "97  -0.441407  0.296999  1.497629  0.187879 -1.194458  1.600436 -1.746629   \n",
              "98  -1.509170  1.832384  0.814599 -0.880316 -0.857682 -0.492055  0.454086   \n",
              "99  -0.155126  1.393178  0.365538 -1.099525 -1.299107 -0.408043  0.279709   \n",
              "100 -1.172911  2.346800  2.090372 -1.631477 -0.952774  0.236958 -0.389693   \n",
              "101 -0.328603  3.061680  2.256522 -0.424378 -1.868915 -0.350414  0.226009   \n",
              "102 -0.718224  0.687967 -1.267417 -0.670412 -1.567853  1.657080  0.818342   \n",
              "103  0.445232  1.184668  0.467780 -2.138310 -1.128685 -0.514149  0.245567   \n",
              "104 -1.120806  0.495716  0.477714 -1.356863 -0.036207  0.809519 -0.160998   \n",
              "105 -1.746276  1.329017  1.284418 -0.877288 -0.101824  0.200079  0.221382   \n",
              "\n",
              "           7         8         9   ...        84        85        86  \\\n",
              "0   -0.494544  0.243539  0.256078  ...  0.245732  0.019818 -0.246347   \n",
              "1   -0.083495  0.892471  0.753701  ... -0.439034 -0.363662  0.125037   \n",
              "2   -1.396603  0.295762 -0.055510  ... -0.171100  0.180284  0.044292   \n",
              "3   -0.249646 -0.755334  1.255697  ...  0.026375  0.007201 -0.165179   \n",
              "4   -1.244299 -0.616884 -0.451323  ... -0.145272  0.047353 -0.150540   \n",
              "5   -1.019767 -0.007901 -0.290260  ...  0.189601 -0.115965 -0.133011   \n",
              "6    0.366178 -0.238992  0.099877  ...  0.050243  0.100481  0.019779   \n",
              "7   -0.012778  0.333973 -0.174705  ...  0.282903  0.042752 -0.064071   \n",
              "8    0.317180 -1.332220  0.513478  ... -0.087155 -0.092904  0.226356   \n",
              "9    0.396526 -0.691455  0.126106  ... -0.414637  0.052247  0.295735   \n",
              "10   0.782024  0.100698  0.812775  ...  0.231648  0.255910 -0.032445   \n",
              "11  -0.082745  1.461712  0.656578  ...  0.247150 -0.025595  0.045100   \n",
              "12  -1.359237  1.269679 -1.087073  ...  0.022675  0.144465 -0.136204   \n",
              "13  -1.910882  0.101441 -0.647171  ...  0.023379 -0.080196  0.042130   \n",
              "14  -1.736858  0.080458 -0.714832  ... -0.397660  0.245568  0.147820   \n",
              "15  -0.278633 -1.153374 -0.412010  ...  0.343332  0.107223 -0.347163   \n",
              "16  -1.129333 -0.513825 -0.480888  ... -0.062118 -0.189135  0.239185   \n",
              "17  -0.891980 -0.098563 -0.805079  ...  0.206288 -0.319841 -0.010459   \n",
              "18   0.218529  0.584960 -0.553539  ... -0.110231 -0.054468 -0.567831   \n",
              "19  -0.926504 -0.011670 -1.288752  ... -0.535468 -0.262623 -0.325703   \n",
              "20  -1.031656 -0.418541 -1.876619  ...  0.189869  0.032763  0.228721   \n",
              "21  -0.803123  0.805781 -0.070944  ... -0.094073 -0.009146 -0.022008   \n",
              "22  -1.325901 -0.012663 -0.640896  ... -0.100999  0.351998 -0.218062   \n",
              "23  -1.380464 -0.368071 -2.021720  ...  0.073958 -0.053034 -0.191372   \n",
              "24  -1.619284 -0.016145 -1.919809  ...  0.279233  0.181656 -0.061430   \n",
              "25  -0.772317 -0.523049 -0.591549  ...  0.000589 -0.182640 -0.225018   \n",
              "26  -1.292481 -0.229035 -1.442089  ...  0.165914  0.036989  0.783753   \n",
              "27  -0.246047 -1.207821 -0.409443  ... -0.609457 -0.247284 -0.058122   \n",
              "28  -0.109988 -0.707614 -0.181341  ...  0.131895  0.306022 -0.058066   \n",
              "29  -0.581437 -0.293733  0.157777  ...  0.567677 -0.090892 -0.030952   \n",
              "30  -0.866857  2.623033  2.573387  ... -0.035778 -0.083382  0.101628   \n",
              "31   1.057783  1.651538  1.695190  ...  0.070735  0.252007 -0.153929   \n",
              "32   0.315936 -1.011740  0.073746  ... -0.289944  0.309996 -0.038902   \n",
              "33   1.642150  0.120016  0.015410  ...  0.235966 -0.282299  0.452872   \n",
              "34   1.672759 -0.015177  0.025464  ... -0.676102  0.357956  0.182697   \n",
              "35   2.124160  0.120424  0.003614  ... -0.268502  0.051885  0.148850   \n",
              "36   1.713566 -0.158619 -0.190742  ...  0.088132  0.001901 -0.181751   \n",
              "37   1.836601  0.079334 -0.030521  ... -0.040810  0.130532 -0.218607   \n",
              "38   2.014792 -0.374588 -0.057618  ...  0.264797 -0.337104 -0.143719   \n",
              "39   1.180762 -0.165200  0.027691  ...  0.161658  0.307317 -0.141023   \n",
              "40  -1.283735  1.205869 -0.368846  ...  0.062673 -0.028989 -0.073108   \n",
              "41  -1.184700  0.438590  0.760127  ... -0.029340  0.150340  0.012598   \n",
              "42  -0.040532 -0.674353 -0.101740  ... -0.018708 -0.172450  0.215015   \n",
              "43  -0.349298 -0.162986  0.367785  ... -0.405684  0.064107 -0.206222   \n",
              "44   0.335673  0.148819 -0.241603  ...  0.011225  0.003135 -0.235874   \n",
              "45  -0.651540 -0.756341  1.189794  ...  0.257882  0.211339 -0.195754   \n",
              "46  -0.032923 -0.081050  0.575991  ... -0.084312 -0.188685 -0.034155   \n",
              "47  -0.960969 -0.815623  0.911458  ...  0.251077 -0.076883  0.164421   \n",
              "48   0.045263 -1.205677  0.486009  ...  0.146607  0.283305 -0.191963   \n",
              "49   0.397253  0.193994  0.309965  ...  0.185404  0.213859 -0.262786   \n",
              "50  -0.993256 -0.761329 -0.080740  ... -0.022582 -0.046744 -0.341623   \n",
              "51   0.415582 -0.010141 -0.189395  ... -0.240947 -0.549458 -0.159876   \n",
              "52  -0.338364 -0.535617 -0.215629  ...  0.070796 -0.140182  0.468011   \n",
              "53  -2.671555  2.745443  0.888032  ... -0.076348 -0.073837  0.271482   \n",
              "54   0.402992  0.703606  0.118129  ... -0.059140 -0.044507 -0.247009   \n",
              "55  -0.938380  0.188878  0.560593  ... -0.038754  0.012199  0.164491   \n",
              "56  -0.334953 -0.230487  0.198537  ... -0.139545  0.036456  0.343505   \n",
              "57   0.452389  0.073112 -0.337740  ... -0.145760 -0.150223 -0.006571   \n",
              "58   0.547070 -0.444554  0.370865  ...  0.111135 -0.015964 -0.099462   \n",
              "59   0.255356 -1.686040  0.209921  ... -0.023100  0.132558  0.012866   \n",
              "60   0.268305 -0.687963  1.258583  ... -0.025951 -0.084966  0.155026   \n",
              "61  -0.632654  0.191662  1.746729  ... -0.031892  0.018460  0.062417   \n",
              "62   0.516689  0.326533  0.505000  ...  0.106817  0.107516 -0.139848   \n",
              "63   0.674668  1.341630  0.361575  ... -0.045240  0.000368 -0.091696   \n",
              "64   0.884821 -0.497122 -0.093180  ...  0.159137  0.025531  0.197970   \n",
              "65   0.050725  0.978742  0.283329  ...  0.064904 -0.130967 -0.095892   \n",
              "66  -0.689111 -1.006972  1.283476  ... -0.119253 -0.194984 -0.072416   \n",
              "67  -0.387542 -0.793323  0.647585  ... -0.029994  0.153425  0.528758   \n",
              "68  -0.654831 -1.101378 -0.044630  ... -0.431430 -0.058752 -0.145380   \n",
              "69  -1.179236 -0.647395 -1.130020  ...  0.033230 -0.016170  0.078437   \n",
              "70  -0.601516  0.319824  0.149369  ...  0.032566 -0.169036 -0.155438   \n",
              "71   0.239169 -0.119760 -0.224642  ...  0.306576  0.747988  0.281006   \n",
              "72   0.352377  1.310474  0.193839  ... -0.425576  0.079482  0.219548   \n",
              "73   1.205280 -0.604935 -0.035388  ...  0.264695 -0.485988 -0.575964   \n",
              "74  -0.239748 -0.058462  0.341092  ...  0.128762  0.111403  0.022093   \n",
              "75  -0.561276 -0.129169 -0.361816  ...  0.298246 -0.120146 -0.243777   \n",
              "76   0.209192 -0.334205 -0.545892  ... -0.106814  0.010436  0.240260   \n",
              "77   0.681877 -0.201955  0.029968  ...  0.084995 -0.950816  0.152380   \n",
              "78  -1.388718  0.304442  0.732854  ...  0.083153  0.141068 -0.196071   \n",
              "79   0.858094 -1.546386  0.572665  ... -0.411556 -0.015249 -0.172982   \n",
              "80   1.849128  1.309309  0.333597  ...  0.076484 -0.439216  0.089563   \n",
              "81   0.558427 -2.667445  1.703981  ...  0.220287  0.151462  0.054684   \n",
              "82   1.367291  1.624622  0.368991  ...  0.229554  0.133209  0.237593   \n",
              "83  -0.608868 -0.813081  0.126835  ... -0.018812  0.267450 -0.128386   \n",
              "84  -0.402627  0.104742  2.651770  ...  0.098177 -0.111877  0.059881   \n",
              "85   1.329868  0.522781 -0.722415  ... -0.013792 -0.225170  0.282117   \n",
              "86   0.807612 -1.241835 -0.856995  ... -0.404808  0.134439 -0.011237   \n",
              "87   0.575198 -0.205599 -1.098592  ...  0.464877  0.212237  0.600586   \n",
              "88   1.266693  0.232469 -1.812457  ...  0.004904 -0.185628 -0.350573   \n",
              "89   1.235543  1.822634 -1.156856  ... -0.133691  0.545830 -0.102365   \n",
              "90   0.065915  0.926925 -0.423506  ...  0.124573 -0.122813  0.115199   \n",
              "91  -0.928727  0.021332 -0.950284  ... -0.111266  0.093473 -0.106342   \n",
              "92  -1.025953 -1.692228  2.282681  ... -0.065489  0.016242 -0.002261   \n",
              "93   1.237993  0.477398 -0.691240  ...  0.348846  0.204388 -0.137155   \n",
              "94   1.580138 -0.950986 -0.260030  ... -0.077231 -0.019061  0.074754   \n",
              "95   0.200679 -0.702918 -0.229318  ...  0.546424 -0.282623  0.020602   \n",
              "96   0.515385 -0.417799 -0.476812  ... -0.172917 -0.390296  0.496663   \n",
              "97   1.181340  0.886013  0.025388  ... -0.500080  0.189998 -0.160553   \n",
              "98   1.153471  1.448765 -1.293766  ... -0.068755 -0.106495  0.170446   \n",
              "99  -0.119767  0.985277 -0.515693  ... -0.150809  0.028025 -0.112792   \n",
              "100  0.292413 -0.029682 -0.423091  ...  0.061643 -0.252224  0.104380   \n",
              "101  1.162425  0.603590 -0.217285  ...  0.227745  0.008060 -0.163510   \n",
              "102  0.515184 -0.144439  0.125603  ... -0.044075  0.357937  0.251036   \n",
              "103  2.021899  0.228138 -0.953547  ... -0.011740  0.419290  0.062435   \n",
              "104 -0.731385  1.327201  0.967042  ... -0.069784  0.047647 -0.162943   \n",
              "105 -0.569326  1.123788 -0.238124  ...  0.070366 -0.196414  0.009736   \n",
              "\n",
              "           87        88        89        90        91        92        93  \n",
              "0   -0.334189  0.001933  0.099690  0.005821  0.098615  0.322466 -0.026514  \n",
              "1   -0.009941  0.000198  0.598846  0.399056 -0.448141 -0.133951  0.163589  \n",
              "2    0.000196  0.003606 -0.028537  0.019995  0.014549 -0.207424 -0.134106  \n",
              "3   -0.136537  0.035306 -0.042689 -0.063572 -0.160413  0.073046 -0.025301  \n",
              "4   -0.009131 -0.106808 -0.137046 -0.175852  0.152502  0.045290 -0.086046  \n",
              "5   -0.063879  0.143104  0.020425 -0.124293 -0.026170  0.228428 -0.088222  \n",
              "6    0.102530  0.022082  0.114263 -0.260516  0.086962  0.043728  0.082299  \n",
              "7    0.104548  0.054169 -0.324595  0.354240  0.060420  0.102943 -0.161668  \n",
              "8   -0.181613 -0.083627  0.256577  0.209390 -0.140765 -0.330571 -0.136723  \n",
              "9    0.499863 -0.019423  0.048918  0.008529  0.233998  0.220361  0.240518  \n",
              "10  -0.373376 -0.155772 -0.378488 -0.422746 -0.007195 -0.081527  0.064764  \n",
              "11   0.256206  0.089645 -0.136949 -0.139985  0.367463  0.123369 -0.021251  \n",
              "12  -0.659289 -0.348931  0.062967 -0.088760 -0.275793 -0.002524  0.053070  \n",
              "13  -0.058174  0.102286 -0.200376  0.157605  0.767559 -0.343533 -0.128184  \n",
              "14   0.036492 -0.018615  0.022910  0.148818 -0.153438 -0.144382  0.505064  \n",
              "15  -0.242998 -0.231267 -0.143570 -0.391664 -0.443641  0.215185 -0.191292  \n",
              "16   0.417584 -0.102288 -0.046368  0.014686  0.194612  0.031704 -0.161622  \n",
              "17   0.574983  0.571460  0.053335 -0.015459 -0.250182  0.438432 -0.287490  \n",
              "18   0.406893 -0.399969  0.101343  0.014383  0.095134  0.098185 -0.259110  \n",
              "19   0.316703 -0.196790  0.313602 -0.153492 -0.250178 -0.260607 -0.131871  \n",
              "20   0.101168  0.307811 -0.011972 -0.280188  0.119275 -0.074072  0.044002  \n",
              "21  -0.162247  0.004115 -0.031810  0.047193  0.030495 -0.001331  0.132705  \n",
              "22   0.053933  0.120976 -0.339442  0.033228  0.098141  0.400648  0.026211  \n",
              "23   0.126416 -0.114568  0.063703 -0.147723  0.231087  0.267255 -0.236652  \n",
              "24   0.089911  0.253002  0.137546 -0.196125 -0.049718 -0.640881  0.196521  \n",
              "25   0.123875 -0.276399 -0.025711  0.131202 -0.241792 -0.199316  0.421149  \n",
              "26  -0.617112  0.081694  0.152925  0.577330 -0.236271  0.359860 -0.102015  \n",
              "27   0.043695  0.375759 -0.444285 -0.061797 -0.089582  0.353245  0.195695  \n",
              "28  -0.026540 -0.734552 -0.085750  0.233943  0.267288 -0.001521 -0.148095  \n",
              "29   0.003469  0.211246 -0.050157 -0.243178 -0.044628 -0.232548  0.123531  \n",
              "30  -0.031518 -0.092311 -0.005899 -0.167378 -0.021581 -0.029826  0.045102  \n",
              "31   0.297099  0.210283 -0.015438  0.036708  0.027371 -0.048235 -0.004286  \n",
              "32  -0.461923  0.331812  0.770746  0.163420 -0.019127 -0.252521 -0.049199  \n",
              "33  -0.069510 -0.150570  0.760613 -0.457456  0.070412  0.253050  0.056535  \n",
              "34   0.031468 -0.266153 -0.694212 -0.289440 -0.311843 -0.538771 -0.691837  \n",
              "35   0.256694 -0.171526 -0.060834 -0.055895  0.610520  0.190081  0.474864  \n",
              "36  -0.153118  0.229762 -0.336681 -0.174751 -0.145790 -0.077123 -0.031268  \n",
              "37  -0.209320  0.143830 -0.222067 -0.031825  0.029729 -0.219203  0.229987  \n",
              "38   0.011564 -0.384079  0.229289  0.537207  0.325409  0.063368 -0.067494  \n",
              "39   0.091703  0.653900  0.250344  0.070657 -0.119526  0.203055  0.214429  \n",
              "40   0.025165 -0.049932 -0.217872 -0.031176 -0.024338  0.110588  0.138984  \n",
              "41   0.166779  0.092546  0.053881  0.066068  0.101522 -0.002252 -0.021419  \n",
              "42   0.214214 -0.064412 -0.084870 -0.137591 -0.312211 -0.063587 -0.424010  \n",
              "43  -0.163489 -0.039891  0.104706 -0.143588  0.139484  0.206617 -0.264348  \n",
              "44  -0.106226 -0.009535  0.056568  0.220914 -0.054410 -0.047803  0.037969  \n",
              "45   0.199907 -0.478144 -0.037040 -0.182755 -0.192339  0.563207  0.281141  \n",
              "46  -0.505823  0.485294 -0.038447 -0.299393  0.173660 -0.024129  0.423081  \n",
              "47  -0.115331 -0.108227  0.118972  0.655574  0.393246 -0.494485 -0.200391  \n",
              "48   0.060690  0.245007 -0.073494  0.235477  0.034611 -0.274523 -0.165675  \n",
              "49  -0.080815  0.011090 -0.314370  0.443712 -0.031642  0.199583  0.252927  \n",
              "50   0.052095  0.141204 -0.017638 -0.106238 -0.123697  0.018175 -0.054416  \n",
              "51  -0.288552  0.168954 -0.066721  0.245986 -0.292556  0.174451 -0.070376  \n",
              "52   0.008004 -0.063999 -0.012369 -0.196740 -0.075268 -0.077160  0.261774  \n",
              "53   0.027421 -0.114554  0.227101  0.036693 -0.046127 -0.009272  0.066155  \n",
              "54  -0.002179 -0.120210  0.042824  0.078140  0.043671  0.092983 -0.159219  \n",
              "55  -0.011457 -0.042707 -0.166679  0.146619  0.150483 -0.066519 -0.057752  \n",
              "56   0.372679 -0.190531  0.167976 -0.521730  0.268769 -0.398607 -0.239639  \n",
              "57  -0.183057 -0.032005  0.013153 -0.105118 -0.073079 -0.020510  0.038878  \n",
              "58   0.128830 -0.107514  0.196250 -0.043452 -0.117866 -0.042734 -0.080784  \n",
              "59  -0.005828  0.104785 -0.066439  0.119514  0.075421  0.109283  0.074889  \n",
              "60  -0.111527  0.031674 -0.120852  0.078288 -0.016775 -0.012838 -0.053098  \n",
              "61   0.041456  0.036796 -0.044589 -0.072414 -0.020983  0.005741  0.027594  \n",
              "62   0.089650 -0.011344 -0.009296  0.033283  0.068578 -0.028233 -0.058885  \n",
              "63  -0.159610 -0.152775  0.171693 -0.169127 -0.140831  0.088552 -0.041704  \n",
              "64   0.218851 -0.195193  0.026675  0.176037 -0.229319  0.020322  0.130651  \n",
              "65   0.006196  0.161780 -0.086680 -0.075419  0.077780 -0.214159  0.096250  \n",
              "66  -0.028816  0.073597  0.157496 -0.049969 -0.048229 -0.082662 -0.055572  \n",
              "67   0.018249 -0.234405 -0.110210  0.099439  0.064447  0.051171  0.056336  \n",
              "68   0.037131 -0.018997 -0.022160  0.122206  0.028687 -0.027614 -0.237752  \n",
              "69   0.061621 -0.066679 -0.065888  0.037886  0.006464 -0.016874  0.071045  \n",
              "70  -0.005980  0.341205  0.016755  0.064121  0.201041 -0.241320  0.066352  \n",
              "71   0.211161 -0.052772  0.271024 -0.101074  0.169923  0.421019 -0.266788  \n",
              "72   0.045769  0.070229 -0.217464  0.153758  0.099874 -0.002688 -0.105864  \n",
              "73  -0.064719  0.069855  0.017468 -0.078100 -0.005051 -0.521922  0.310096  \n",
              "74  -0.161144  0.273438 -0.351178  0.091363  0.147706  0.012927  0.236450  \n",
              "75  -0.193970 -0.187693  0.193393 -0.084965 -0.243552  0.255849 -0.150146  \n",
              "76   0.106577 -0.048935  0.053032 -0.120631 -0.076274  0.216099 -0.193066  \n",
              "77  -0.096333 -0.360521 -0.636963  0.182494  0.116272  0.145837 -0.011910  \n",
              "78  -0.017133  0.093527  0.108013 -0.018965  0.009624 -0.072017 -0.212278  \n",
              "79  -0.080124 -0.420673  0.275791 -0.236614 -0.043578 -0.221336  0.329065  \n",
              "80   0.138923  0.454713 -0.014073  0.436484  0.061091  0.088260 -0.462703  \n",
              "81   0.104405  0.354576 -0.015021  0.062562 -0.013381  0.046793 -0.040719  \n",
              "82  -0.176736 -0.334725  0.028267 -0.065541 -0.124133  0.011299  0.184846  \n",
              "83   0.080484  0.013867  0.082405 -0.019154  0.007168 -0.002415 -0.011750  \n",
              "84   0.004402 -0.130449 -0.039883 -0.035877 -0.000887  0.034442  0.006651  \n",
              "85   0.269362 -0.101066 -0.071562  0.062494 -0.243757  0.005644  0.111357  \n",
              "86   0.114788 -0.153699  0.113997  0.306465  0.153019  0.296461  0.578454  \n",
              "87  -0.221337 -0.022835 -0.319404  0.166073 -0.512772 -0.361424 -0.032517  \n",
              "88  -0.336581 -0.136353 -0.042791  0.136589  0.500420 -0.004391 -0.130279  \n",
              "89  -0.323477 -0.020121  0.130904  0.147145  0.290111 -0.048614 -0.078692  \n",
              "90   0.041773 -0.095950 -0.080089 -0.320160 -0.034541  0.005961  0.263597  \n",
              "91   0.053073 -0.002879 -0.119720  0.188424 -0.140838  0.016097  0.267991  \n",
              "92  -0.017577 -0.059603 -0.027986  0.102249 -0.080871  0.056780  0.008198  \n",
              "93   0.588687 -0.184511 -0.089425  0.280229 -0.663489 -0.166260  0.181005  \n",
              "94  -0.591578  0.095841  0.198789 -0.336367  0.104396  0.270823 -0.588938  \n",
              "95   0.301686  0.072675  0.271945 -0.248259  0.250986 -0.292536 -0.092558  \n",
              "96  -0.263594  0.213383 -0.165381 -0.518336  0.160600  0.145911  0.233921  \n",
              "97   0.169078  0.432112  0.094936  0.095585 -0.312528  0.147194 -0.202735  \n",
              "98   0.169195  0.262094 -0.117901 -0.164800 -0.069491 -0.050880 -0.021687  \n",
              "99  -0.361274 -0.143445 -0.003529  0.029297  0.092871  0.046724  0.017889  \n",
              "100  0.054052 -0.149576  0.054327 -0.085206 -0.020826 -0.022760 -0.051240  \n",
              "101  0.093020 -0.086098  0.027652  0.021020  0.002824 -0.078523 -0.032950  \n",
              "102 -0.005687  0.141986 -0.258837  0.109927 -0.115133 -0.061322 -0.033643  \n",
              "103  0.191918  0.126068  0.287124 -0.040200  0.194610 -0.116119  0.169397  \n",
              "104 -0.053178  0.104861  0.008137 -0.253575 -0.041771 -0.019096  0.024908  \n",
              "105  0.119265  0.025462  0.260401  0.209111 -0.092553  0.108953 -0.142083  \n",
              "\n",
              "[106 rows x 94 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0e83d404-f348-4167-9f41-8cd333a55cd7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-2.637099</td>\n",
              "      <td>-0.347802</td>\n",
              "      <td>-1.305130</td>\n",
              "      <td>0.477352</td>\n",
              "      <td>0.222379</td>\n",
              "      <td>2.597939</td>\n",
              "      <td>1.865067</td>\n",
              "      <td>-0.494544</td>\n",
              "      <td>0.243539</td>\n",
              "      <td>0.256078</td>\n",
              "      <td>...</td>\n",
              "      <td>0.245732</td>\n",
              "      <td>0.019818</td>\n",
              "      <td>-0.246347</td>\n",
              "      <td>-0.334189</td>\n",
              "      <td>0.001933</td>\n",
              "      <td>0.099690</td>\n",
              "      <td>0.005821</td>\n",
              "      <td>0.098615</td>\n",
              "      <td>0.322466</td>\n",
              "      <td>-0.026514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-2.798193</td>\n",
              "      <td>3.634745</td>\n",
              "      <td>2.086230</td>\n",
              "      <td>3.196251</td>\n",
              "      <td>1.620699</td>\n",
              "      <td>-0.670871</td>\n",
              "      <td>1.726479</td>\n",
              "      <td>-0.083495</td>\n",
              "      <td>0.892471</td>\n",
              "      <td>0.753701</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.439034</td>\n",
              "      <td>-0.363662</td>\n",
              "      <td>0.125037</td>\n",
              "      <td>-0.009941</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.598846</td>\n",
              "      <td>0.399056</td>\n",
              "      <td>-0.448141</td>\n",
              "      <td>-0.133951</td>\n",
              "      <td>0.163589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.739392</td>\n",
              "      <td>-1.241732</td>\n",
              "      <td>-1.321686</td>\n",
              "      <td>-1.510875</td>\n",
              "      <td>-0.779627</td>\n",
              "      <td>-1.543128</td>\n",
              "      <td>1.477094</td>\n",
              "      <td>-1.396603</td>\n",
              "      <td>0.295762</td>\n",
              "      <td>-0.055510</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.171100</td>\n",
              "      <td>0.180284</td>\n",
              "      <td>0.044292</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.003606</td>\n",
              "      <td>-0.028537</td>\n",
              "      <td>0.019995</td>\n",
              "      <td>0.014549</td>\n",
              "      <td>-0.207424</td>\n",
              "      <td>-0.134106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.994942</td>\n",
              "      <td>1.074568</td>\n",
              "      <td>1.187057</td>\n",
              "      <td>1.138465</td>\n",
              "      <td>-0.079325</td>\n",
              "      <td>-4.044148</td>\n",
              "      <td>1.202367</td>\n",
              "      <td>-0.249646</td>\n",
              "      <td>-0.755334</td>\n",
              "      <td>1.255697</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026375</td>\n",
              "      <td>0.007201</td>\n",
              "      <td>-0.165179</td>\n",
              "      <td>-0.136537</td>\n",
              "      <td>0.035306</td>\n",
              "      <td>-0.042689</td>\n",
              "      <td>-0.063572</td>\n",
              "      <td>-0.160413</td>\n",
              "      <td>0.073046</td>\n",
              "      <td>-0.025301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.329136</td>\n",
              "      <td>2.676327</td>\n",
              "      <td>0.291800</td>\n",
              "      <td>0.271295</td>\n",
              "      <td>-0.906031</td>\n",
              "      <td>-1.966462</td>\n",
              "      <td>1.868663</td>\n",
              "      <td>-1.244299</td>\n",
              "      <td>-0.616884</td>\n",
              "      <td>-0.451323</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.145272</td>\n",
              "      <td>0.047353</td>\n",
              "      <td>-0.150540</td>\n",
              "      <td>-0.009131</td>\n",
              "      <td>-0.106808</td>\n",
              "      <td>-0.137046</td>\n",
              "      <td>-0.175852</td>\n",
              "      <td>0.152502</td>\n",
              "      <td>0.045290</td>\n",
              "      <td>-0.086046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-1.131488</td>\n",
              "      <td>0.223700</td>\n",
              "      <td>0.368406</td>\n",
              "      <td>-1.841575</td>\n",
              "      <td>-0.366648</td>\n",
              "      <td>-3.507539</td>\n",
              "      <td>0.586460</td>\n",
              "      <td>-1.019767</td>\n",
              "      <td>-0.007901</td>\n",
              "      <td>-0.290260</td>\n",
              "      <td>...</td>\n",
              "      <td>0.189601</td>\n",
              "      <td>-0.115965</td>\n",
              "      <td>-0.133011</td>\n",
              "      <td>-0.063879</td>\n",
              "      <td>0.143104</td>\n",
              "      <td>0.020425</td>\n",
              "      <td>-0.124293</td>\n",
              "      <td>-0.026170</td>\n",
              "      <td>0.228428</td>\n",
              "      <td>-0.088222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-1.042963</td>\n",
              "      <td>0.784338</td>\n",
              "      <td>0.155310</td>\n",
              "      <td>2.144182</td>\n",
              "      <td>-0.328153</td>\n",
              "      <td>-3.037030</td>\n",
              "      <td>2.740792</td>\n",
              "      <td>0.366178</td>\n",
              "      <td>-0.238992</td>\n",
              "      <td>0.099877</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050243</td>\n",
              "      <td>0.100481</td>\n",
              "      <td>0.019779</td>\n",
              "      <td>0.102530</td>\n",
              "      <td>0.022082</td>\n",
              "      <td>0.114263</td>\n",
              "      <td>-0.260516</td>\n",
              "      <td>0.086962</td>\n",
              "      <td>0.043728</td>\n",
              "      <td>0.082299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-2.123212</td>\n",
              "      <td>2.594353</td>\n",
              "      <td>0.784899</td>\n",
              "      <td>2.561225</td>\n",
              "      <td>0.298017</td>\n",
              "      <td>-1.477255</td>\n",
              "      <td>2.404881</td>\n",
              "      <td>-0.012778</td>\n",
              "      <td>0.333973</td>\n",
              "      <td>-0.174705</td>\n",
              "      <td>...</td>\n",
              "      <td>0.282903</td>\n",
              "      <td>0.042752</td>\n",
              "      <td>-0.064071</td>\n",
              "      <td>0.104548</td>\n",
              "      <td>0.054169</td>\n",
              "      <td>-0.324595</td>\n",
              "      <td>0.354240</td>\n",
              "      <td>0.060420</td>\n",
              "      <td>0.102943</td>\n",
              "      <td>-0.161668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-2.428108</td>\n",
              "      <td>0.587128</td>\n",
              "      <td>0.336312</td>\n",
              "      <td>1.760723</td>\n",
              "      <td>-0.277718</td>\n",
              "      <td>-1.053064</td>\n",
              "      <td>2.272779</td>\n",
              "      <td>0.317180</td>\n",
              "      <td>-1.332220</td>\n",
              "      <td>0.513478</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.087155</td>\n",
              "      <td>-0.092904</td>\n",
              "      <td>0.226356</td>\n",
              "      <td>-0.181613</td>\n",
              "      <td>-0.083627</td>\n",
              "      <td>0.256577</td>\n",
              "      <td>0.209390</td>\n",
              "      <td>-0.140765</td>\n",
              "      <td>-0.330571</td>\n",
              "      <td>-0.136723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-1.996928</td>\n",
              "      <td>0.378025</td>\n",
              "      <td>1.192909</td>\n",
              "      <td>1.650560</td>\n",
              "      <td>0.339481</td>\n",
              "      <td>-1.626476</td>\n",
              "      <td>1.924355</td>\n",
              "      <td>0.396526</td>\n",
              "      <td>-0.691455</td>\n",
              "      <td>0.126106</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.414637</td>\n",
              "      <td>0.052247</td>\n",
              "      <td>0.295735</td>\n",
              "      <td>0.499863</td>\n",
              "      <td>-0.019423</td>\n",
              "      <td>0.048918</td>\n",
              "      <td>0.008529</td>\n",
              "      <td>0.233998</td>\n",
              "      <td>0.220361</td>\n",
              "      <td>0.240518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-2.397497</td>\n",
              "      <td>2.225219</td>\n",
              "      <td>1.587146</td>\n",
              "      <td>3.089433</td>\n",
              "      <td>0.767364</td>\n",
              "      <td>-2.262547</td>\n",
              "      <td>2.458541</td>\n",
              "      <td>0.782024</td>\n",
              "      <td>0.100698</td>\n",
              "      <td>0.812775</td>\n",
              "      <td>...</td>\n",
              "      <td>0.231648</td>\n",
              "      <td>0.255910</td>\n",
              "      <td>-0.032445</td>\n",
              "      <td>-0.373376</td>\n",
              "      <td>-0.155772</td>\n",
              "      <td>-0.378488</td>\n",
              "      <td>-0.422746</td>\n",
              "      <td>-0.007195</td>\n",
              "      <td>-0.081527</td>\n",
              "      <td>0.064764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>-3.121665</td>\n",
              "      <td>3.285395</td>\n",
              "      <td>2.648926</td>\n",
              "      <td>2.631635</td>\n",
              "      <td>2.515994</td>\n",
              "      <td>1.229619</td>\n",
              "      <td>0.031375</td>\n",
              "      <td>-0.082745</td>\n",
              "      <td>1.461712</td>\n",
              "      <td>0.656578</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247150</td>\n",
              "      <td>-0.025595</td>\n",
              "      <td>0.045100</td>\n",
              "      <td>0.256206</td>\n",
              "      <td>0.089645</td>\n",
              "      <td>-0.136949</td>\n",
              "      <td>-0.139985</td>\n",
              "      <td>0.367463</td>\n",
              "      <td>0.123369</td>\n",
              "      <td>-0.021251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>-3.878436</td>\n",
              "      <td>-1.077783</td>\n",
              "      <td>1.972808</td>\n",
              "      <td>1.255909</td>\n",
              "      <td>4.699007</td>\n",
              "      <td>1.939041</td>\n",
              "      <td>-0.401190</td>\n",
              "      <td>-1.359237</td>\n",
              "      <td>1.269679</td>\n",
              "      <td>-1.087073</td>\n",
              "      <td>...</td>\n",
              "      <td>0.022675</td>\n",
              "      <td>0.144465</td>\n",
              "      <td>-0.136204</td>\n",
              "      <td>-0.659289</td>\n",
              "      <td>-0.348931</td>\n",
              "      <td>0.062967</td>\n",
              "      <td>-0.088760</td>\n",
              "      <td>-0.275793</td>\n",
              "      <td>-0.002524</td>\n",
              "      <td>0.053070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>-4.014367</td>\n",
              "      <td>-1.252404</td>\n",
              "      <td>1.606471</td>\n",
              "      <td>0.522180</td>\n",
              "      <td>4.165790</td>\n",
              "      <td>1.218129</td>\n",
              "      <td>0.170922</td>\n",
              "      <td>-1.910882</td>\n",
              "      <td>0.101441</td>\n",
              "      <td>-0.647171</td>\n",
              "      <td>...</td>\n",
              "      <td>0.023379</td>\n",
              "      <td>-0.080196</td>\n",
              "      <td>0.042130</td>\n",
              "      <td>-0.058174</td>\n",
              "      <td>0.102286</td>\n",
              "      <td>-0.200376</td>\n",
              "      <td>0.157605</td>\n",
              "      <td>0.767559</td>\n",
              "      <td>-0.343533</td>\n",
              "      <td>-0.128184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-3.816117</td>\n",
              "      <td>-1.324373</td>\n",
              "      <td>1.602825</td>\n",
              "      <td>0.552906</td>\n",
              "      <td>4.097568</td>\n",
              "      <td>0.830133</td>\n",
              "      <td>-0.096257</td>\n",
              "      <td>-1.736858</td>\n",
              "      <td>0.080458</td>\n",
              "      <td>-0.714832</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.397660</td>\n",
              "      <td>0.245568</td>\n",
              "      <td>0.147820</td>\n",
              "      <td>0.036492</td>\n",
              "      <td>-0.018615</td>\n",
              "      <td>0.022910</td>\n",
              "      <td>0.148818</td>\n",
              "      <td>-0.153438</td>\n",
              "      <td>-0.144382</td>\n",
              "      <td>0.505064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-3.689973</td>\n",
              "      <td>-2.155047</td>\n",
              "      <td>0.552009</td>\n",
              "      <td>0.078458</td>\n",
              "      <td>2.731397</td>\n",
              "      <td>0.792978</td>\n",
              "      <td>0.293722</td>\n",
              "      <td>-0.278633</td>\n",
              "      <td>-1.153374</td>\n",
              "      <td>-0.412010</td>\n",
              "      <td>...</td>\n",
              "      <td>0.343332</td>\n",
              "      <td>0.107223</td>\n",
              "      <td>-0.347163</td>\n",
              "      <td>-0.242998</td>\n",
              "      <td>-0.231267</td>\n",
              "      <td>-0.143570</td>\n",
              "      <td>-0.391664</td>\n",
              "      <td>-0.443641</td>\n",
              "      <td>0.215185</td>\n",
              "      <td>-0.191292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>-3.664803</td>\n",
              "      <td>-2.112624</td>\n",
              "      <td>0.709510</td>\n",
              "      <td>0.316549</td>\n",
              "      <td>3.421074</td>\n",
              "      <td>0.577376</td>\n",
              "      <td>-0.058770</td>\n",
              "      <td>-1.129333</td>\n",
              "      <td>-0.513825</td>\n",
              "      <td>-0.480888</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.062118</td>\n",
              "      <td>-0.189135</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.417584</td>\n",
              "      <td>-0.102288</td>\n",
              "      <td>-0.046368</td>\n",
              "      <td>0.014686</td>\n",
              "      <td>0.194612</td>\n",
              "      <td>0.031704</td>\n",
              "      <td>-0.161622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>-3.472888</td>\n",
              "      <td>-2.414408</td>\n",
              "      <td>1.069169</td>\n",
              "      <td>0.544240</td>\n",
              "      <td>3.510205</td>\n",
              "      <td>1.037111</td>\n",
              "      <td>-0.485694</td>\n",
              "      <td>-0.891980</td>\n",
              "      <td>-0.098563</td>\n",
              "      <td>-0.805079</td>\n",
              "      <td>...</td>\n",
              "      <td>0.206288</td>\n",
              "      <td>-0.319841</td>\n",
              "      <td>-0.010459</td>\n",
              "      <td>0.574983</td>\n",
              "      <td>0.571460</td>\n",
              "      <td>0.053335</td>\n",
              "      <td>-0.015459</td>\n",
              "      <td>-0.250182</td>\n",
              "      <td>0.438432</td>\n",
              "      <td>-0.287490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-0.154480</td>\n",
              "      <td>-1.546400</td>\n",
              "      <td>-0.879493</td>\n",
              "      <td>1.928970</td>\n",
              "      <td>-1.528069</td>\n",
              "      <td>1.393321</td>\n",
              "      <td>0.603145</td>\n",
              "      <td>0.218529</td>\n",
              "      <td>0.584960</td>\n",
              "      <td>-0.553539</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.110231</td>\n",
              "      <td>-0.054468</td>\n",
              "      <td>-0.567831</td>\n",
              "      <td>0.406893</td>\n",
              "      <td>-0.399969</td>\n",
              "      <td>0.101343</td>\n",
              "      <td>0.014383</td>\n",
              "      <td>0.095134</td>\n",
              "      <td>0.098185</td>\n",
              "      <td>-0.259110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>-0.429591</td>\n",
              "      <td>-1.061395</td>\n",
              "      <td>-1.073781</td>\n",
              "      <td>1.603877</td>\n",
              "      <td>-1.000881</td>\n",
              "      <td>0.137796</td>\n",
              "      <td>-0.474644</td>\n",
              "      <td>-0.926504</td>\n",
              "      <td>-0.011670</td>\n",
              "      <td>-1.288752</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.535468</td>\n",
              "      <td>-0.262623</td>\n",
              "      <td>-0.325703</td>\n",
              "      <td>0.316703</td>\n",
              "      <td>-0.196790</td>\n",
              "      <td>0.313602</td>\n",
              "      <td>-0.153492</td>\n",
              "      <td>-0.250178</td>\n",
              "      <td>-0.260607</td>\n",
              "      <td>-0.131871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1.169234</td>\n",
              "      <td>0.563495</td>\n",
              "      <td>-0.556821</td>\n",
              "      <td>2.166775</td>\n",
              "      <td>-1.110425</td>\n",
              "      <td>-1.226026</td>\n",
              "      <td>-0.370970</td>\n",
              "      <td>-1.031656</td>\n",
              "      <td>-0.418541</td>\n",
              "      <td>-1.876619</td>\n",
              "      <td>...</td>\n",
              "      <td>0.189869</td>\n",
              "      <td>0.032763</td>\n",
              "      <td>0.228721</td>\n",
              "      <td>0.101168</td>\n",
              "      <td>0.307811</td>\n",
              "      <td>-0.011972</td>\n",
              "      <td>-0.280188</td>\n",
              "      <td>0.119275</td>\n",
              "      <td>-0.074072</td>\n",
              "      <td>0.044002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1.997442</td>\n",
              "      <td>-0.566104</td>\n",
              "      <td>-2.878561</td>\n",
              "      <td>2.048987</td>\n",
              "      <td>-2.044952</td>\n",
              "      <td>1.675383</td>\n",
              "      <td>-0.844437</td>\n",
              "      <td>-0.803123</td>\n",
              "      <td>0.805781</td>\n",
              "      <td>-0.070944</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.094073</td>\n",
              "      <td>-0.009146</td>\n",
              "      <td>-0.022008</td>\n",
              "      <td>-0.162247</td>\n",
              "      <td>0.004115</td>\n",
              "      <td>-0.031810</td>\n",
              "      <td>0.047193</td>\n",
              "      <td>0.030495</td>\n",
              "      <td>-0.001331</td>\n",
              "      <td>0.132705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.983302</td>\n",
              "      <td>-1.229718</td>\n",
              "      <td>-1.498552</td>\n",
              "      <td>1.839532</td>\n",
              "      <td>-1.240514</td>\n",
              "      <td>0.511564</td>\n",
              "      <td>-0.776103</td>\n",
              "      <td>-1.325901</td>\n",
              "      <td>-0.012663</td>\n",
              "      <td>-0.640896</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.100999</td>\n",
              "      <td>0.351998</td>\n",
              "      <td>-0.218062</td>\n",
              "      <td>0.053933</td>\n",
              "      <td>0.120976</td>\n",
              "      <td>-0.339442</td>\n",
              "      <td>0.033228</td>\n",
              "      <td>0.098141</td>\n",
              "      <td>0.400648</td>\n",
              "      <td>0.026211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1.557271</td>\n",
              "      <td>0.050281</td>\n",
              "      <td>-1.899174</td>\n",
              "      <td>1.712817</td>\n",
              "      <td>-0.925847</td>\n",
              "      <td>-1.272008</td>\n",
              "      <td>-0.870842</td>\n",
              "      <td>-1.380464</td>\n",
              "      <td>-0.368071</td>\n",
              "      <td>-2.021720</td>\n",
              "      <td>...</td>\n",
              "      <td>0.073958</td>\n",
              "      <td>-0.053034</td>\n",
              "      <td>-0.191372</td>\n",
              "      <td>0.126416</td>\n",
              "      <td>-0.114568</td>\n",
              "      <td>0.063703</td>\n",
              "      <td>-0.147723</td>\n",
              "      <td>0.231087</td>\n",
              "      <td>0.267255</td>\n",
              "      <td>-0.236652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1.250082</td>\n",
              "      <td>-0.475840</td>\n",
              "      <td>-1.731295</td>\n",
              "      <td>1.065621</td>\n",
              "      <td>-1.003865</td>\n",
              "      <td>-0.619206</td>\n",
              "      <td>0.185522</td>\n",
              "      <td>-1.619284</td>\n",
              "      <td>-0.016145</td>\n",
              "      <td>-1.919809</td>\n",
              "      <td>...</td>\n",
              "      <td>0.279233</td>\n",
              "      <td>0.181656</td>\n",
              "      <td>-0.061430</td>\n",
              "      <td>0.089911</td>\n",
              "      <td>0.253002</td>\n",
              "      <td>0.137546</td>\n",
              "      <td>-0.196125</td>\n",
              "      <td>-0.049718</td>\n",
              "      <td>-0.640881</td>\n",
              "      <td>0.196521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.517654</td>\n",
              "      <td>-0.726796</td>\n",
              "      <td>-0.476584</td>\n",
              "      <td>0.659845</td>\n",
              "      <td>-0.101265</td>\n",
              "      <td>-0.393997</td>\n",
              "      <td>-0.535044</td>\n",
              "      <td>-0.772317</td>\n",
              "      <td>-0.523049</td>\n",
              "      <td>-0.591549</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>-0.182640</td>\n",
              "      <td>-0.225018</td>\n",
              "      <td>0.123875</td>\n",
              "      <td>-0.276399</td>\n",
              "      <td>-0.025711</td>\n",
              "      <td>0.131202</td>\n",
              "      <td>-0.241792</td>\n",
              "      <td>-0.199316</td>\n",
              "      <td>0.421149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>-0.252770</td>\n",
              "      <td>-2.423221</td>\n",
              "      <td>-1.737167</td>\n",
              "      <td>1.730142</td>\n",
              "      <td>-0.774093</td>\n",
              "      <td>-0.778728</td>\n",
              "      <td>-0.803385</td>\n",
              "      <td>-1.292481</td>\n",
              "      <td>-0.229035</td>\n",
              "      <td>-1.442089</td>\n",
              "      <td>...</td>\n",
              "      <td>0.165914</td>\n",
              "      <td>0.036989</td>\n",
              "      <td>0.783753</td>\n",
              "      <td>-0.617112</td>\n",
              "      <td>0.081694</td>\n",
              "      <td>0.152925</td>\n",
              "      <td>0.577330</td>\n",
              "      <td>-0.236271</td>\n",
              "      <td>0.359860</td>\n",
              "      <td>-0.102015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1.164473</td>\n",
              "      <td>-1.224640</td>\n",
              "      <td>-0.914450</td>\n",
              "      <td>2.582735</td>\n",
              "      <td>-1.158372</td>\n",
              "      <td>-0.549832</td>\n",
              "      <td>-0.848184</td>\n",
              "      <td>-0.246047</td>\n",
              "      <td>-1.207821</td>\n",
              "      <td>-0.409443</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.609457</td>\n",
              "      <td>-0.247284</td>\n",
              "      <td>-0.058122</td>\n",
              "      <td>0.043695</td>\n",
              "      <td>0.375759</td>\n",
              "      <td>-0.444285</td>\n",
              "      <td>-0.061797</td>\n",
              "      <td>-0.089582</td>\n",
              "      <td>0.353245</td>\n",
              "      <td>0.195695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1.911768</td>\n",
              "      <td>-0.915630</td>\n",
              "      <td>-0.944989</td>\n",
              "      <td>2.965577</td>\n",
              "      <td>-1.213963</td>\n",
              "      <td>-0.835333</td>\n",
              "      <td>-1.340682</td>\n",
              "      <td>-0.109988</td>\n",
              "      <td>-0.707614</td>\n",
              "      <td>-0.181341</td>\n",
              "      <td>...</td>\n",
              "      <td>0.131895</td>\n",
              "      <td>0.306022</td>\n",
              "      <td>-0.058066</td>\n",
              "      <td>-0.026540</td>\n",
              "      <td>-0.734552</td>\n",
              "      <td>-0.085750</td>\n",
              "      <td>0.233943</td>\n",
              "      <td>0.267288</td>\n",
              "      <td>-0.001521</td>\n",
              "      <td>-0.148095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>-0.408548</td>\n",
              "      <td>-1.834815</td>\n",
              "      <td>-0.125575</td>\n",
              "      <td>3.054345</td>\n",
              "      <td>-0.877931</td>\n",
              "      <td>-0.548621</td>\n",
              "      <td>-0.374316</td>\n",
              "      <td>-0.581437</td>\n",
              "      <td>-0.293733</td>\n",
              "      <td>0.157777</td>\n",
              "      <td>...</td>\n",
              "      <td>0.567677</td>\n",
              "      <td>-0.090892</td>\n",
              "      <td>-0.030952</td>\n",
              "      <td>0.003469</td>\n",
              "      <td>0.211246</td>\n",
              "      <td>-0.050157</td>\n",
              "      <td>-0.243178</td>\n",
              "      <td>-0.044628</td>\n",
              "      <td>-0.232548</td>\n",
              "      <td>0.123531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1.467009</td>\n",
              "      <td>-1.673569</td>\n",
              "      <td>-0.998931</td>\n",
              "      <td>3.212867</td>\n",
              "      <td>-1.358372</td>\n",
              "      <td>-1.950890</td>\n",
              "      <td>-1.870762</td>\n",
              "      <td>-0.866857</td>\n",
              "      <td>2.623033</td>\n",
              "      <td>2.573387</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.035778</td>\n",
              "      <td>-0.083382</td>\n",
              "      <td>0.101628</td>\n",
              "      <td>-0.031518</td>\n",
              "      <td>-0.092311</td>\n",
              "      <td>-0.005899</td>\n",
              "      <td>-0.167378</td>\n",
              "      <td>-0.021581</td>\n",
              "      <td>-0.029826</td>\n",
              "      <td>0.045102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1.693128</td>\n",
              "      <td>-1.905936</td>\n",
              "      <td>-1.482452</td>\n",
              "      <td>2.580142</td>\n",
              "      <td>-0.631265</td>\n",
              "      <td>-1.065471</td>\n",
              "      <td>-0.645155</td>\n",
              "      <td>1.057783</td>\n",
              "      <td>1.651538</td>\n",
              "      <td>1.695190</td>\n",
              "      <td>...</td>\n",
              "      <td>0.070735</td>\n",
              "      <td>0.252007</td>\n",
              "      <td>-0.153929</td>\n",
              "      <td>0.297099</td>\n",
              "      <td>0.210283</td>\n",
              "      <td>-0.015438</td>\n",
              "      <td>0.036708</td>\n",
              "      <td>0.027371</td>\n",
              "      <td>-0.048235</td>\n",
              "      <td>-0.004286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.326083</td>\n",
              "      <td>-2.357328</td>\n",
              "      <td>-0.837244</td>\n",
              "      <td>0.915371</td>\n",
              "      <td>-0.114251</td>\n",
              "      <td>-0.320591</td>\n",
              "      <td>-0.543085</td>\n",
              "      <td>0.315936</td>\n",
              "      <td>-1.011740</td>\n",
              "      <td>0.073746</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.289944</td>\n",
              "      <td>0.309996</td>\n",
              "      <td>-0.038902</td>\n",
              "      <td>-0.461923</td>\n",
              "      <td>0.331812</td>\n",
              "      <td>0.770746</td>\n",
              "      <td>0.163420</td>\n",
              "      <td>-0.019127</td>\n",
              "      <td>-0.252521</td>\n",
              "      <td>-0.049199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>-3.097785</td>\n",
              "      <td>-1.540616</td>\n",
              "      <td>-1.078272</td>\n",
              "      <td>-1.299055</td>\n",
              "      <td>0.463412</td>\n",
              "      <td>1.260984</td>\n",
              "      <td>0.520225</td>\n",
              "      <td>1.642150</td>\n",
              "      <td>0.120016</td>\n",
              "      <td>0.015410</td>\n",
              "      <td>...</td>\n",
              "      <td>0.235966</td>\n",
              "      <td>-0.282299</td>\n",
              "      <td>0.452872</td>\n",
              "      <td>-0.069510</td>\n",
              "      <td>-0.150570</td>\n",
              "      <td>0.760613</td>\n",
              "      <td>-0.457456</td>\n",
              "      <td>0.070412</td>\n",
              "      <td>0.253050</td>\n",
              "      <td>0.056535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>-2.872596</td>\n",
              "      <td>-2.289240</td>\n",
              "      <td>-0.718798</td>\n",
              "      <td>-0.856928</td>\n",
              "      <td>0.755319</td>\n",
              "      <td>0.446068</td>\n",
              "      <td>0.125273</td>\n",
              "      <td>1.672759</td>\n",
              "      <td>-0.015177</td>\n",
              "      <td>0.025464</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.676102</td>\n",
              "      <td>0.357956</td>\n",
              "      <td>0.182697</td>\n",
              "      <td>0.031468</td>\n",
              "      <td>-0.266153</td>\n",
              "      <td>-0.694212</td>\n",
              "      <td>-0.289440</td>\n",
              "      <td>-0.311843</td>\n",
              "      <td>-0.538771</td>\n",
              "      <td>-0.691837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>-2.642872</td>\n",
              "      <td>-3.093603</td>\n",
              "      <td>-1.580577</td>\n",
              "      <td>-0.983180</td>\n",
              "      <td>0.529433</td>\n",
              "      <td>0.042502</td>\n",
              "      <td>0.219207</td>\n",
              "      <td>2.124160</td>\n",
              "      <td>0.120424</td>\n",
              "      <td>0.003614</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.268502</td>\n",
              "      <td>0.051885</td>\n",
              "      <td>0.148850</td>\n",
              "      <td>0.256694</td>\n",
              "      <td>-0.171526</td>\n",
              "      <td>-0.060834</td>\n",
              "      <td>-0.055895</td>\n",
              "      <td>0.610520</td>\n",
              "      <td>0.190081</td>\n",
              "      <td>0.474864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>-2.666563</td>\n",
              "      <td>-2.811451</td>\n",
              "      <td>-1.320080</td>\n",
              "      <td>-0.584977</td>\n",
              "      <td>0.549821</td>\n",
              "      <td>0.310147</td>\n",
              "      <td>0.132449</td>\n",
              "      <td>1.713566</td>\n",
              "      <td>-0.158619</td>\n",
              "      <td>-0.190742</td>\n",
              "      <td>...</td>\n",
              "      <td>0.088132</td>\n",
              "      <td>0.001901</td>\n",
              "      <td>-0.181751</td>\n",
              "      <td>-0.153118</td>\n",
              "      <td>0.229762</td>\n",
              "      <td>-0.336681</td>\n",
              "      <td>-0.174751</td>\n",
              "      <td>-0.145790</td>\n",
              "      <td>-0.077123</td>\n",
              "      <td>-0.031268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>-2.522127</td>\n",
              "      <td>-2.950680</td>\n",
              "      <td>-0.912822</td>\n",
              "      <td>-0.742229</td>\n",
              "      <td>0.636310</td>\n",
              "      <td>0.272814</td>\n",
              "      <td>-0.124880</td>\n",
              "      <td>1.836601</td>\n",
              "      <td>0.079334</td>\n",
              "      <td>-0.030521</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.040810</td>\n",
              "      <td>0.130532</td>\n",
              "      <td>-0.218607</td>\n",
              "      <td>-0.209320</td>\n",
              "      <td>0.143830</td>\n",
              "      <td>-0.222067</td>\n",
              "      <td>-0.031825</td>\n",
              "      <td>0.029729</td>\n",
              "      <td>-0.219203</td>\n",
              "      <td>0.229987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>-2.824209</td>\n",
              "      <td>-2.713716</td>\n",
              "      <td>-1.641778</td>\n",
              "      <td>-0.771857</td>\n",
              "      <td>0.380510</td>\n",
              "      <td>-0.132937</td>\n",
              "      <td>0.730788</td>\n",
              "      <td>2.014792</td>\n",
              "      <td>-0.374588</td>\n",
              "      <td>-0.057618</td>\n",
              "      <td>...</td>\n",
              "      <td>0.264797</td>\n",
              "      <td>-0.337104</td>\n",
              "      <td>-0.143719</td>\n",
              "      <td>0.011564</td>\n",
              "      <td>-0.384079</td>\n",
              "      <td>0.229289</td>\n",
              "      <td>0.537207</td>\n",
              "      <td>0.325409</td>\n",
              "      <td>0.063368</td>\n",
              "      <td>-0.067494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>-3.420457</td>\n",
              "      <td>-2.746807</td>\n",
              "      <td>-1.231707</td>\n",
              "      <td>-0.784039</td>\n",
              "      <td>1.139511</td>\n",
              "      <td>1.333265</td>\n",
              "      <td>0.306230</td>\n",
              "      <td>1.180762</td>\n",
              "      <td>-0.165200</td>\n",
              "      <td>0.027691</td>\n",
              "      <td>...</td>\n",
              "      <td>0.161658</td>\n",
              "      <td>0.307317</td>\n",
              "      <td>-0.141023</td>\n",
              "      <td>0.091703</td>\n",
              "      <td>0.653900</td>\n",
              "      <td>0.250344</td>\n",
              "      <td>0.070657</td>\n",
              "      <td>-0.119526</td>\n",
              "      <td>0.203055</td>\n",
              "      <td>0.214429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>-1.099295</td>\n",
              "      <td>0.465213</td>\n",
              "      <td>1.334637</td>\n",
              "      <td>-2.250615</td>\n",
              "      <td>-1.491715</td>\n",
              "      <td>-0.653282</td>\n",
              "      <td>-0.849536</td>\n",
              "      <td>-1.283735</td>\n",
              "      <td>1.205869</td>\n",
              "      <td>-0.368846</td>\n",
              "      <td>...</td>\n",
              "      <td>0.062673</td>\n",
              "      <td>-0.028989</td>\n",
              "      <td>-0.073108</td>\n",
              "      <td>0.025165</td>\n",
              "      <td>-0.049932</td>\n",
              "      <td>-0.217872</td>\n",
              "      <td>-0.031176</td>\n",
              "      <td>-0.024338</td>\n",
              "      <td>0.110588</td>\n",
              "      <td>0.138984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>-1.169595</td>\n",
              "      <td>-2.462802</td>\n",
              "      <td>1.489594</td>\n",
              "      <td>-3.346595</td>\n",
              "      <td>-0.699219</td>\n",
              "      <td>-2.433538</td>\n",
              "      <td>-1.578992</td>\n",
              "      <td>-1.184700</td>\n",
              "      <td>0.438590</td>\n",
              "      <td>0.760127</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029340</td>\n",
              "      <td>0.150340</td>\n",
              "      <td>0.012598</td>\n",
              "      <td>0.166779</td>\n",
              "      <td>0.092546</td>\n",
              "      <td>0.053881</td>\n",
              "      <td>0.066068</td>\n",
              "      <td>0.101522</td>\n",
              "      <td>-0.002252</td>\n",
              "      <td>-0.021419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>-0.188029</td>\n",
              "      <td>-2.039316</td>\n",
              "      <td>2.035923</td>\n",
              "      <td>-2.630550</td>\n",
              "      <td>0.002784</td>\n",
              "      <td>-0.922663</td>\n",
              "      <td>-1.055911</td>\n",
              "      <td>-0.040532</td>\n",
              "      <td>-0.674353</td>\n",
              "      <td>-0.101740</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018708</td>\n",
              "      <td>-0.172450</td>\n",
              "      <td>0.215015</td>\n",
              "      <td>0.214214</td>\n",
              "      <td>-0.064412</td>\n",
              "      <td>-0.084870</td>\n",
              "      <td>-0.137591</td>\n",
              "      <td>-0.312211</td>\n",
              "      <td>-0.063587</td>\n",
              "      <td>-0.424010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>-1.340439</td>\n",
              "      <td>-0.345850</td>\n",
              "      <td>3.167113</td>\n",
              "      <td>-2.315758</td>\n",
              "      <td>-0.589326</td>\n",
              "      <td>-1.100225</td>\n",
              "      <td>-1.353995</td>\n",
              "      <td>-0.349298</td>\n",
              "      <td>-0.162986</td>\n",
              "      <td>0.367785</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.405684</td>\n",
              "      <td>0.064107</td>\n",
              "      <td>-0.206222</td>\n",
              "      <td>-0.163489</td>\n",
              "      <td>-0.039891</td>\n",
              "      <td>0.104706</td>\n",
              "      <td>-0.143588</td>\n",
              "      <td>0.139484</td>\n",
              "      <td>0.206617</td>\n",
              "      <td>-0.264348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>-0.038033</td>\n",
              "      <td>-1.146338</td>\n",
              "      <td>0.056533</td>\n",
              "      <td>-1.408917</td>\n",
              "      <td>-0.829297</td>\n",
              "      <td>-2.161916</td>\n",
              "      <td>0.448988</td>\n",
              "      <td>0.335673</td>\n",
              "      <td>0.148819</td>\n",
              "      <td>-0.241603</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011225</td>\n",
              "      <td>0.003135</td>\n",
              "      <td>-0.235874</td>\n",
              "      <td>-0.106226</td>\n",
              "      <td>-0.009535</td>\n",
              "      <td>0.056568</td>\n",
              "      <td>0.220914</td>\n",
              "      <td>-0.054410</td>\n",
              "      <td>-0.047803</td>\n",
              "      <td>0.037969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>-0.402297</td>\n",
              "      <td>-2.237480</td>\n",
              "      <td>2.388917</td>\n",
              "      <td>-2.855490</td>\n",
              "      <td>0.062406</td>\n",
              "      <td>-0.589928</td>\n",
              "      <td>-0.938151</td>\n",
              "      <td>-0.651540</td>\n",
              "      <td>-0.756341</td>\n",
              "      <td>1.189794</td>\n",
              "      <td>...</td>\n",
              "      <td>0.257882</td>\n",
              "      <td>0.211339</td>\n",
              "      <td>-0.195754</td>\n",
              "      <td>0.199907</td>\n",
              "      <td>-0.478144</td>\n",
              "      <td>-0.037040</td>\n",
              "      <td>-0.182755</td>\n",
              "      <td>-0.192339</td>\n",
              "      <td>0.563207</td>\n",
              "      <td>0.281141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>-1.313723</td>\n",
              "      <td>-2.045631</td>\n",
              "      <td>1.158891</td>\n",
              "      <td>-2.354800</td>\n",
              "      <td>-0.732532</td>\n",
              "      <td>-1.073207</td>\n",
              "      <td>-0.775655</td>\n",
              "      <td>-0.032923</td>\n",
              "      <td>-0.081050</td>\n",
              "      <td>0.575991</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.084312</td>\n",
              "      <td>-0.188685</td>\n",
              "      <td>-0.034155</td>\n",
              "      <td>-0.505823</td>\n",
              "      <td>0.485294</td>\n",
              "      <td>-0.038447</td>\n",
              "      <td>-0.299393</td>\n",
              "      <td>0.173660</td>\n",
              "      <td>-0.024129</td>\n",
              "      <td>0.423081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>-1.048675</td>\n",
              "      <td>-1.715145</td>\n",
              "      <td>2.515703</td>\n",
              "      <td>-2.073400</td>\n",
              "      <td>-0.410095</td>\n",
              "      <td>-0.572684</td>\n",
              "      <td>-1.261211</td>\n",
              "      <td>-0.960969</td>\n",
              "      <td>-0.815623</td>\n",
              "      <td>0.911458</td>\n",
              "      <td>...</td>\n",
              "      <td>0.251077</td>\n",
              "      <td>-0.076883</td>\n",
              "      <td>0.164421</td>\n",
              "      <td>-0.115331</td>\n",
              "      <td>-0.108227</td>\n",
              "      <td>0.118972</td>\n",
              "      <td>0.655574</td>\n",
              "      <td>0.393246</td>\n",
              "      <td>-0.494485</td>\n",
              "      <td>-0.200391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>-1.276280</td>\n",
              "      <td>0.422902</td>\n",
              "      <td>2.890507</td>\n",
              "      <td>-0.298685</td>\n",
              "      <td>-0.156793</td>\n",
              "      <td>0.877776</td>\n",
              "      <td>0.280889</td>\n",
              "      <td>0.045263</td>\n",
              "      <td>-1.205677</td>\n",
              "      <td>0.486009</td>\n",
              "      <td>...</td>\n",
              "      <td>0.146607</td>\n",
              "      <td>0.283305</td>\n",
              "      <td>-0.191963</td>\n",
              "      <td>0.060690</td>\n",
              "      <td>0.245007</td>\n",
              "      <td>-0.073494</td>\n",
              "      <td>0.235477</td>\n",
              "      <td>0.034611</td>\n",
              "      <td>-0.274523</td>\n",
              "      <td>-0.165675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>-1.675160</td>\n",
              "      <td>-0.223162</td>\n",
              "      <td>-0.959539</td>\n",
              "      <td>-1.191272</td>\n",
              "      <td>-1.148406</td>\n",
              "      <td>2.156334</td>\n",
              "      <td>2.685866</td>\n",
              "      <td>0.397253</td>\n",
              "      <td>0.193994</td>\n",
              "      <td>0.309965</td>\n",
              "      <td>...</td>\n",
              "      <td>0.185404</td>\n",
              "      <td>0.213859</td>\n",
              "      <td>-0.262786</td>\n",
              "      <td>-0.080815</td>\n",
              "      <td>0.011090</td>\n",
              "      <td>-0.314370</td>\n",
              "      <td>0.443712</td>\n",
              "      <td>-0.031642</td>\n",
              "      <td>0.199583</td>\n",
              "      <td>0.252927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>-0.901350</td>\n",
              "      <td>0.623569</td>\n",
              "      <td>-1.150486</td>\n",
              "      <td>-2.642692</td>\n",
              "      <td>-1.667718</td>\n",
              "      <td>-0.243350</td>\n",
              "      <td>1.203611</td>\n",
              "      <td>-0.993256</td>\n",
              "      <td>-0.761329</td>\n",
              "      <td>-0.080740</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.022582</td>\n",
              "      <td>-0.046744</td>\n",
              "      <td>-0.341623</td>\n",
              "      <td>0.052095</td>\n",
              "      <td>0.141204</td>\n",
              "      <td>-0.017638</td>\n",
              "      <td>-0.106238</td>\n",
              "      <td>-0.123697</td>\n",
              "      <td>0.018175</td>\n",
              "      <td>-0.054416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>-0.174547</td>\n",
              "      <td>0.067538</td>\n",
              "      <td>-1.066564</td>\n",
              "      <td>-1.551703</td>\n",
              "      <td>-1.708644</td>\n",
              "      <td>1.853259</td>\n",
              "      <td>2.516707</td>\n",
              "      <td>0.415582</td>\n",
              "      <td>-0.010141</td>\n",
              "      <td>-0.189395</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.240947</td>\n",
              "      <td>-0.549458</td>\n",
              "      <td>-0.159876</td>\n",
              "      <td>-0.288552</td>\n",
              "      <td>0.168954</td>\n",
              "      <td>-0.066721</td>\n",
              "      <td>0.245986</td>\n",
              "      <td>-0.292556</td>\n",
              "      <td>0.174451</td>\n",
              "      <td>-0.070376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>-0.282104</td>\n",
              "      <td>-1.011302</td>\n",
              "      <td>-0.691323</td>\n",
              "      <td>-1.847908</td>\n",
              "      <td>-1.607443</td>\n",
              "      <td>-0.038352</td>\n",
              "      <td>1.813729</td>\n",
              "      <td>-0.338364</td>\n",
              "      <td>-0.535617</td>\n",
              "      <td>-0.215629</td>\n",
              "      <td>...</td>\n",
              "      <td>0.070796</td>\n",
              "      <td>-0.140182</td>\n",
              "      <td>0.468011</td>\n",
              "      <td>0.008004</td>\n",
              "      <td>-0.063999</td>\n",
              "      <td>-0.012369</td>\n",
              "      <td>-0.196740</td>\n",
              "      <td>-0.075268</td>\n",
              "      <td>-0.077160</td>\n",
              "      <td>0.261774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>-0.769889</td>\n",
              "      <td>-1.683174</td>\n",
              "      <td>-1.707398</td>\n",
              "      <td>-1.639610</td>\n",
              "      <td>-1.496469</td>\n",
              "      <td>-0.361207</td>\n",
              "      <td>0.058345</td>\n",
              "      <td>-2.671555</td>\n",
              "      <td>2.745443</td>\n",
              "      <td>0.888032</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.076348</td>\n",
              "      <td>-0.073837</td>\n",
              "      <td>0.271482</td>\n",
              "      <td>0.027421</td>\n",
              "      <td>-0.114554</td>\n",
              "      <td>0.227101</td>\n",
              "      <td>0.036693</td>\n",
              "      <td>-0.046127</td>\n",
              "      <td>-0.009272</td>\n",
              "      <td>0.066155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>-0.081574</td>\n",
              "      <td>-1.092008</td>\n",
              "      <td>-1.654655</td>\n",
              "      <td>-2.202721</td>\n",
              "      <td>-0.719607</td>\n",
              "      <td>-0.060184</td>\n",
              "      <td>1.042880</td>\n",
              "      <td>0.402992</td>\n",
              "      <td>0.703606</td>\n",
              "      <td>0.118129</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.059140</td>\n",
              "      <td>-0.044507</td>\n",
              "      <td>-0.247009</td>\n",
              "      <td>-0.002179</td>\n",
              "      <td>-0.120210</td>\n",
              "      <td>0.042824</td>\n",
              "      <td>0.078140</td>\n",
              "      <td>0.043671</td>\n",
              "      <td>0.092983</td>\n",
              "      <td>-0.159219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>-1.234132</td>\n",
              "      <td>-0.838562</td>\n",
              "      <td>-2.011426</td>\n",
              "      <td>-1.940953</td>\n",
              "      <td>-1.865559</td>\n",
              "      <td>0.548632</td>\n",
              "      <td>1.478590</td>\n",
              "      <td>-0.938380</td>\n",
              "      <td>0.188878</td>\n",
              "      <td>0.560593</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.038754</td>\n",
              "      <td>0.012199</td>\n",
              "      <td>0.164491</td>\n",
              "      <td>-0.011457</td>\n",
              "      <td>-0.042707</td>\n",
              "      <td>-0.166679</td>\n",
              "      <td>0.146619</td>\n",
              "      <td>0.150483</td>\n",
              "      <td>-0.066519</td>\n",
              "      <td>-0.057752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>-1.377481</td>\n",
              "      <td>-0.065721</td>\n",
              "      <td>-1.081459</td>\n",
              "      <td>-0.250382</td>\n",
              "      <td>-1.244762</td>\n",
              "      <td>3.282794</td>\n",
              "      <td>1.888986</td>\n",
              "      <td>-0.334953</td>\n",
              "      <td>-0.230487</td>\n",
              "      <td>0.198537</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.139545</td>\n",
              "      <td>0.036456</td>\n",
              "      <td>0.343505</td>\n",
              "      <td>0.372679</td>\n",
              "      <td>-0.190531</td>\n",
              "      <td>0.167976</td>\n",
              "      <td>-0.521730</td>\n",
              "      <td>0.268769</td>\n",
              "      <td>-0.398607</td>\n",
              "      <td>-0.239639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>-1.136689</td>\n",
              "      <td>5.804210</td>\n",
              "      <td>-2.158707</td>\n",
              "      <td>-0.711264</td>\n",
              "      <td>2.630094</td>\n",
              "      <td>-0.722201</td>\n",
              "      <td>-2.566602</td>\n",
              "      <td>0.452389</td>\n",
              "      <td>0.073112</td>\n",
              "      <td>-0.337740</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.145760</td>\n",
              "      <td>-0.150223</td>\n",
              "      <td>-0.006571</td>\n",
              "      <td>-0.183057</td>\n",
              "      <td>-0.032005</td>\n",
              "      <td>0.013153</td>\n",
              "      <td>-0.105118</td>\n",
              "      <td>-0.073079</td>\n",
              "      <td>-0.020510</td>\n",
              "      <td>0.038878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>-1.971310</td>\n",
              "      <td>4.386899</td>\n",
              "      <td>-4.043365</td>\n",
              "      <td>-1.361281</td>\n",
              "      <td>1.282604</td>\n",
              "      <td>-0.496142</td>\n",
              "      <td>-1.185623</td>\n",
              "      <td>0.547070</td>\n",
              "      <td>-0.444554</td>\n",
              "      <td>0.370865</td>\n",
              "      <td>...</td>\n",
              "      <td>0.111135</td>\n",
              "      <td>-0.015964</td>\n",
              "      <td>-0.099462</td>\n",
              "      <td>0.128830</td>\n",
              "      <td>-0.107514</td>\n",
              "      <td>0.196250</td>\n",
              "      <td>-0.043452</td>\n",
              "      <td>-0.117866</td>\n",
              "      <td>-0.042734</td>\n",
              "      <td>-0.080784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>-0.847435</td>\n",
              "      <td>4.087546</td>\n",
              "      <td>-3.075522</td>\n",
              "      <td>-1.403359</td>\n",
              "      <td>1.478408</td>\n",
              "      <td>-0.990298</td>\n",
              "      <td>-1.038978</td>\n",
              "      <td>0.255356</td>\n",
              "      <td>-1.686040</td>\n",
              "      <td>0.209921</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.023100</td>\n",
              "      <td>0.132558</td>\n",
              "      <td>0.012866</td>\n",
              "      <td>-0.005828</td>\n",
              "      <td>0.104785</td>\n",
              "      <td>-0.066439</td>\n",
              "      <td>0.119514</td>\n",
              "      <td>0.075421</td>\n",
              "      <td>0.109283</td>\n",
              "      <td>0.074889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>-2.034989</td>\n",
              "      <td>5.777314</td>\n",
              "      <td>-3.917526</td>\n",
              "      <td>-1.192115</td>\n",
              "      <td>0.948309</td>\n",
              "      <td>0.384854</td>\n",
              "      <td>-0.618132</td>\n",
              "      <td>0.268305</td>\n",
              "      <td>-0.687963</td>\n",
              "      <td>1.258583</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025951</td>\n",
              "      <td>-0.084966</td>\n",
              "      <td>0.155026</td>\n",
              "      <td>-0.111527</td>\n",
              "      <td>0.031674</td>\n",
              "      <td>-0.120852</td>\n",
              "      <td>0.078288</td>\n",
              "      <td>-0.016775</td>\n",
              "      <td>-0.012838</td>\n",
              "      <td>-0.053098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>-2.899686</td>\n",
              "      <td>3.653677</td>\n",
              "      <td>-4.107776</td>\n",
              "      <td>-0.837462</td>\n",
              "      <td>2.314684</td>\n",
              "      <td>-0.099674</td>\n",
              "      <td>-0.377653</td>\n",
              "      <td>-0.632654</td>\n",
              "      <td>0.191662</td>\n",
              "      <td>1.746729</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.031892</td>\n",
              "      <td>0.018460</td>\n",
              "      <td>0.062417</td>\n",
              "      <td>0.041456</td>\n",
              "      <td>0.036796</td>\n",
              "      <td>-0.044589</td>\n",
              "      <td>-0.072414</td>\n",
              "      <td>-0.020983</td>\n",
              "      <td>0.005741</td>\n",
              "      <td>0.027594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>-3.044610</td>\n",
              "      <td>4.717416</td>\n",
              "      <td>-2.692057</td>\n",
              "      <td>-0.515452</td>\n",
              "      <td>3.082248</td>\n",
              "      <td>-0.087261</td>\n",
              "      <td>-1.969000</td>\n",
              "      <td>0.516689</td>\n",
              "      <td>0.326533</td>\n",
              "      <td>0.505000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.106817</td>\n",
              "      <td>0.107516</td>\n",
              "      <td>-0.139848</td>\n",
              "      <td>0.089650</td>\n",
              "      <td>-0.011344</td>\n",
              "      <td>-0.009296</td>\n",
              "      <td>0.033283</td>\n",
              "      <td>0.068578</td>\n",
              "      <td>-0.028233</td>\n",
              "      <td>-0.058885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>7.514398</td>\n",
              "      <td>-0.635832</td>\n",
              "      <td>0.391142</td>\n",
              "      <td>-0.657821</td>\n",
              "      <td>1.896845</td>\n",
              "      <td>-0.123870</td>\n",
              "      <td>0.429798</td>\n",
              "      <td>0.674668</td>\n",
              "      <td>1.341630</td>\n",
              "      <td>0.361575</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.045240</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>-0.091696</td>\n",
              "      <td>-0.159610</td>\n",
              "      <td>-0.152775</td>\n",
              "      <td>0.171693</td>\n",
              "      <td>-0.169127</td>\n",
              "      <td>-0.140831</td>\n",
              "      <td>0.088552</td>\n",
              "      <td>-0.041704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>6.081498</td>\n",
              "      <td>0.184675</td>\n",
              "      <td>0.796057</td>\n",
              "      <td>0.446617</td>\n",
              "      <td>0.916233</td>\n",
              "      <td>0.107495</td>\n",
              "      <td>1.029045</td>\n",
              "      <td>0.884821</td>\n",
              "      <td>-0.497122</td>\n",
              "      <td>-0.093180</td>\n",
              "      <td>...</td>\n",
              "      <td>0.159137</td>\n",
              "      <td>0.025531</td>\n",
              "      <td>0.197970</td>\n",
              "      <td>0.218851</td>\n",
              "      <td>-0.195193</td>\n",
              "      <td>0.026675</td>\n",
              "      <td>0.176037</td>\n",
              "      <td>-0.229319</td>\n",
              "      <td>0.020322</td>\n",
              "      <td>0.130651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>6.417571</td>\n",
              "      <td>-1.035532</td>\n",
              "      <td>0.593525</td>\n",
              "      <td>-1.207628</td>\n",
              "      <td>1.777169</td>\n",
              "      <td>-0.666690</td>\n",
              "      <td>0.538690</td>\n",
              "      <td>0.050725</td>\n",
              "      <td>0.978742</td>\n",
              "      <td>0.283329</td>\n",
              "      <td>...</td>\n",
              "      <td>0.064904</td>\n",
              "      <td>-0.130967</td>\n",
              "      <td>-0.095892</td>\n",
              "      <td>0.006196</td>\n",
              "      <td>0.161780</td>\n",
              "      <td>-0.086680</td>\n",
              "      <td>-0.075419</td>\n",
              "      <td>0.077780</td>\n",
              "      <td>-0.214159</td>\n",
              "      <td>0.096250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>4.853528</td>\n",
              "      <td>0.146181</td>\n",
              "      <td>0.280188</td>\n",
              "      <td>0.419659</td>\n",
              "      <td>-0.696790</td>\n",
              "      <td>1.923275</td>\n",
              "      <td>0.624137</td>\n",
              "      <td>-0.689111</td>\n",
              "      <td>-1.006972</td>\n",
              "      <td>1.283476</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.119253</td>\n",
              "      <td>-0.194984</td>\n",
              "      <td>-0.072416</td>\n",
              "      <td>-0.028816</td>\n",
              "      <td>0.073597</td>\n",
              "      <td>0.157496</td>\n",
              "      <td>-0.049969</td>\n",
              "      <td>-0.048229</td>\n",
              "      <td>-0.082662</td>\n",
              "      <td>-0.055572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>5.767001</td>\n",
              "      <td>-0.226511</td>\n",
              "      <td>0.799813</td>\n",
              "      <td>-0.561765</td>\n",
              "      <td>1.164284</td>\n",
              "      <td>0.518367</td>\n",
              "      <td>0.365658</td>\n",
              "      <td>-0.387542</td>\n",
              "      <td>-0.793323</td>\n",
              "      <td>0.647585</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029994</td>\n",
              "      <td>0.153425</td>\n",
              "      <td>0.528758</td>\n",
              "      <td>0.018249</td>\n",
              "      <td>-0.234405</td>\n",
              "      <td>-0.110210</td>\n",
              "      <td>0.099439</td>\n",
              "      <td>0.064447</td>\n",
              "      <td>0.051171</td>\n",
              "      <td>0.056336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>5.465487</td>\n",
              "      <td>-0.097514</td>\n",
              "      <td>0.327609</td>\n",
              "      <td>-0.011427</td>\n",
              "      <td>1.276314</td>\n",
              "      <td>0.224520</td>\n",
              "      <td>0.800988</td>\n",
              "      <td>-0.654831</td>\n",
              "      <td>-1.101378</td>\n",
              "      <td>-0.044630</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.431430</td>\n",
              "      <td>-0.058752</td>\n",
              "      <td>-0.145380</td>\n",
              "      <td>0.037131</td>\n",
              "      <td>-0.018997</td>\n",
              "      <td>-0.022160</td>\n",
              "      <td>0.122206</td>\n",
              "      <td>0.028687</td>\n",
              "      <td>-0.027614</td>\n",
              "      <td>-0.237752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>3.633630</td>\n",
              "      <td>6.278797</td>\n",
              "      <td>-2.002323</td>\n",
              "      <td>-1.931037</td>\n",
              "      <td>-0.796981</td>\n",
              "      <td>0.588673</td>\n",
              "      <td>-0.935282</td>\n",
              "      <td>-1.179236</td>\n",
              "      <td>-0.647395</td>\n",
              "      <td>-1.130020</td>\n",
              "      <td>...</td>\n",
              "      <td>0.033230</td>\n",
              "      <td>-0.016170</td>\n",
              "      <td>0.078437</td>\n",
              "      <td>0.061621</td>\n",
              "      <td>-0.066679</td>\n",
              "      <td>-0.065888</td>\n",
              "      <td>0.037886</td>\n",
              "      <td>0.006464</td>\n",
              "      <td>-0.016874</td>\n",
              "      <td>0.071045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>6.400256</td>\n",
              "      <td>0.244836</td>\n",
              "      <td>-0.896523</td>\n",
              "      <td>-0.428111</td>\n",
              "      <td>0.628118</td>\n",
              "      <td>1.700274</td>\n",
              "      <td>0.879492</td>\n",
              "      <td>-0.601516</td>\n",
              "      <td>0.319824</td>\n",
              "      <td>0.149369</td>\n",
              "      <td>...</td>\n",
              "      <td>0.032566</td>\n",
              "      <td>-0.169036</td>\n",
              "      <td>-0.155438</td>\n",
              "      <td>-0.005980</td>\n",
              "      <td>0.341205</td>\n",
              "      <td>0.016755</td>\n",
              "      <td>0.064121</td>\n",
              "      <td>0.201041</td>\n",
              "      <td>-0.241320</td>\n",
              "      <td>0.066352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>6.061324</td>\n",
              "      <td>-0.621618</td>\n",
              "      <td>-0.241531</td>\n",
              "      <td>-0.590321</td>\n",
              "      <td>1.098110</td>\n",
              "      <td>0.639856</td>\n",
              "      <td>0.818704</td>\n",
              "      <td>0.239169</td>\n",
              "      <td>-0.119760</td>\n",
              "      <td>-0.224642</td>\n",
              "      <td>...</td>\n",
              "      <td>0.306576</td>\n",
              "      <td>0.747988</td>\n",
              "      <td>0.281006</td>\n",
              "      <td>0.211161</td>\n",
              "      <td>-0.052772</td>\n",
              "      <td>0.271024</td>\n",
              "      <td>-0.101074</td>\n",
              "      <td>0.169923</td>\n",
              "      <td>0.421019</td>\n",
              "      <td>-0.266788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>9.025208</td>\n",
              "      <td>-0.402932</td>\n",
              "      <td>-0.213329</td>\n",
              "      <td>-1.001249</td>\n",
              "      <td>2.936993</td>\n",
              "      <td>-0.683901</td>\n",
              "      <td>0.119570</td>\n",
              "      <td>0.352377</td>\n",
              "      <td>1.310474</td>\n",
              "      <td>0.193839</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.425576</td>\n",
              "      <td>0.079482</td>\n",
              "      <td>0.219548</td>\n",
              "      <td>0.045769</td>\n",
              "      <td>0.070229</td>\n",
              "      <td>-0.217464</td>\n",
              "      <td>0.153758</td>\n",
              "      <td>0.099874</td>\n",
              "      <td>-0.002688</td>\n",
              "      <td>-0.105864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>7.222217</td>\n",
              "      <td>-1.011508</td>\n",
              "      <td>0.332970</td>\n",
              "      <td>-0.405879</td>\n",
              "      <td>2.436039</td>\n",
              "      <td>-0.023262</td>\n",
              "      <td>0.352652</td>\n",
              "      <td>1.205280</td>\n",
              "      <td>-0.604935</td>\n",
              "      <td>-0.035388</td>\n",
              "      <td>...</td>\n",
              "      <td>0.264695</td>\n",
              "      <td>-0.485988</td>\n",
              "      <td>-0.575964</td>\n",
              "      <td>-0.064719</td>\n",
              "      <td>0.069855</td>\n",
              "      <td>0.017468</td>\n",
              "      <td>-0.078100</td>\n",
              "      <td>-0.005051</td>\n",
              "      <td>-0.521922</td>\n",
              "      <td>0.310096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>6.948776</td>\n",
              "      <td>-0.575131</td>\n",
              "      <td>1.190701</td>\n",
              "      <td>-0.513953</td>\n",
              "      <td>1.864702</td>\n",
              "      <td>0.053287</td>\n",
              "      <td>0.334423</td>\n",
              "      <td>-0.239748</td>\n",
              "      <td>-0.058462</td>\n",
              "      <td>0.341092</td>\n",
              "      <td>...</td>\n",
              "      <td>0.128762</td>\n",
              "      <td>0.111403</td>\n",
              "      <td>0.022093</td>\n",
              "      <td>-0.161144</td>\n",
              "      <td>0.273438</td>\n",
              "      <td>-0.351178</td>\n",
              "      <td>0.091363</td>\n",
              "      <td>0.147706</td>\n",
              "      <td>0.012927</td>\n",
              "      <td>0.236450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>7.114773</td>\n",
              "      <td>0.999335</td>\n",
              "      <td>0.408795</td>\n",
              "      <td>-0.790729</td>\n",
              "      <td>1.222958</td>\n",
              "      <td>0.188703</td>\n",
              "      <td>0.416799</td>\n",
              "      <td>-0.561276</td>\n",
              "      <td>-0.129169</td>\n",
              "      <td>-0.361816</td>\n",
              "      <td>...</td>\n",
              "      <td>0.298246</td>\n",
              "      <td>-0.120146</td>\n",
              "      <td>-0.243777</td>\n",
              "      <td>-0.193970</td>\n",
              "      <td>-0.187693</td>\n",
              "      <td>0.193393</td>\n",
              "      <td>-0.084965</td>\n",
              "      <td>-0.243552</td>\n",
              "      <td>0.255849</td>\n",
              "      <td>-0.150146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>8.566109</td>\n",
              "      <td>0.652017</td>\n",
              "      <td>0.555999</td>\n",
              "      <td>-0.039120</td>\n",
              "      <td>1.827801</td>\n",
              "      <td>0.268175</td>\n",
              "      <td>0.529672</td>\n",
              "      <td>0.209192</td>\n",
              "      <td>-0.334205</td>\n",
              "      <td>-0.545892</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.106814</td>\n",
              "      <td>0.010436</td>\n",
              "      <td>0.240260</td>\n",
              "      <td>0.106577</td>\n",
              "      <td>-0.048935</td>\n",
              "      <td>0.053032</td>\n",
              "      <td>-0.120631</td>\n",
              "      <td>-0.076274</td>\n",
              "      <td>0.216099</td>\n",
              "      <td>-0.193066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>-0.645925</td>\n",
              "      <td>-2.615459</td>\n",
              "      <td>-1.874452</td>\n",
              "      <td>0.331043</td>\n",
              "      <td>-0.124215</td>\n",
              "      <td>0.806257</td>\n",
              "      <td>-0.853601</td>\n",
              "      <td>0.681877</td>\n",
              "      <td>-0.201955</td>\n",
              "      <td>0.029968</td>\n",
              "      <td>...</td>\n",
              "      <td>0.084995</td>\n",
              "      <td>-0.950816</td>\n",
              "      <td>0.152380</td>\n",
              "      <td>-0.096333</td>\n",
              "      <td>-0.360521</td>\n",
              "      <td>-0.636963</td>\n",
              "      <td>0.182494</td>\n",
              "      <td>0.116272</td>\n",
              "      <td>0.145837</td>\n",
              "      <td>-0.011910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>-1.407767</td>\n",
              "      <td>-2.200869</td>\n",
              "      <td>-1.877372</td>\n",
              "      <td>-0.870802</td>\n",
              "      <td>-0.213478</td>\n",
              "      <td>-1.136117</td>\n",
              "      <td>-0.138414</td>\n",
              "      <td>-1.388718</td>\n",
              "      <td>0.304442</td>\n",
              "      <td>0.732854</td>\n",
              "      <td>...</td>\n",
              "      <td>0.083153</td>\n",
              "      <td>0.141068</td>\n",
              "      <td>-0.196071</td>\n",
              "      <td>-0.017133</td>\n",
              "      <td>0.093527</td>\n",
              "      <td>0.108013</td>\n",
              "      <td>-0.018965</td>\n",
              "      <td>0.009624</td>\n",
              "      <td>-0.072017</td>\n",
              "      <td>-0.212278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.321378</td>\n",
              "      <td>-1.875157</td>\n",
              "      <td>-0.427114</td>\n",
              "      <td>1.717297</td>\n",
              "      <td>0.642612</td>\n",
              "      <td>-0.206885</td>\n",
              "      <td>-1.512476</td>\n",
              "      <td>0.858094</td>\n",
              "      <td>-1.546386</td>\n",
              "      <td>0.572665</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.411556</td>\n",
              "      <td>-0.015249</td>\n",
              "      <td>-0.172982</td>\n",
              "      <td>-0.080124</td>\n",
              "      <td>-0.420673</td>\n",
              "      <td>0.275791</td>\n",
              "      <td>-0.236614</td>\n",
              "      <td>-0.043578</td>\n",
              "      <td>-0.221336</td>\n",
              "      <td>0.329065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.253918</td>\n",
              "      <td>-0.777346</td>\n",
              "      <td>-0.444279</td>\n",
              "      <td>2.374200</td>\n",
              "      <td>-0.030165</td>\n",
              "      <td>-0.970837</td>\n",
              "      <td>-0.715522</td>\n",
              "      <td>1.849128</td>\n",
              "      <td>1.309309</td>\n",
              "      <td>0.333597</td>\n",
              "      <td>...</td>\n",
              "      <td>0.076484</td>\n",
              "      <td>-0.439216</td>\n",
              "      <td>0.089563</td>\n",
              "      <td>0.138923</td>\n",
              "      <td>0.454713</td>\n",
              "      <td>-0.014073</td>\n",
              "      <td>0.436484</td>\n",
              "      <td>0.061091</td>\n",
              "      <td>0.088260</td>\n",
              "      <td>-0.462703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>1.031057</td>\n",
              "      <td>0.081395</td>\n",
              "      <td>-0.383579</td>\n",
              "      <td>2.403520</td>\n",
              "      <td>-1.243376</td>\n",
              "      <td>0.543347</td>\n",
              "      <td>-1.806651</td>\n",
              "      <td>0.558427</td>\n",
              "      <td>-2.667445</td>\n",
              "      <td>1.703981</td>\n",
              "      <td>...</td>\n",
              "      <td>0.220287</td>\n",
              "      <td>0.151462</td>\n",
              "      <td>0.054684</td>\n",
              "      <td>0.104405</td>\n",
              "      <td>0.354576</td>\n",
              "      <td>-0.015021</td>\n",
              "      <td>0.062562</td>\n",
              "      <td>-0.013381</td>\n",
              "      <td>0.046793</td>\n",
              "      <td>-0.040719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>0.807086</td>\n",
              "      <td>-1.028513</td>\n",
              "      <td>-2.842726</td>\n",
              "      <td>1.866244</td>\n",
              "      <td>-0.376761</td>\n",
              "      <td>0.616221</td>\n",
              "      <td>-0.697409</td>\n",
              "      <td>1.367291</td>\n",
              "      <td>1.624622</td>\n",
              "      <td>0.368991</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229554</td>\n",
              "      <td>0.133209</td>\n",
              "      <td>0.237593</td>\n",
              "      <td>-0.176736</td>\n",
              "      <td>-0.334725</td>\n",
              "      <td>0.028267</td>\n",
              "      <td>-0.065541</td>\n",
              "      <td>-0.124133</td>\n",
              "      <td>0.011299</td>\n",
              "      <td>0.184846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>-0.395075</td>\n",
              "      <td>-2.009826</td>\n",
              "      <td>-2.418936</td>\n",
              "      <td>1.200081</td>\n",
              "      <td>-0.904394</td>\n",
              "      <td>0.677463</td>\n",
              "      <td>-1.362367</td>\n",
              "      <td>-0.608868</td>\n",
              "      <td>-0.813081</td>\n",
              "      <td>0.126835</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018812</td>\n",
              "      <td>0.267450</td>\n",
              "      <td>-0.128386</td>\n",
              "      <td>0.080484</td>\n",
              "      <td>0.013867</td>\n",
              "      <td>0.082405</td>\n",
              "      <td>-0.019154</td>\n",
              "      <td>0.007168</td>\n",
              "      <td>-0.002415</td>\n",
              "      <td>-0.011750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>-0.186865</td>\n",
              "      <td>1.001387</td>\n",
              "      <td>3.046893</td>\n",
              "      <td>0.714120</td>\n",
              "      <td>-1.772765</td>\n",
              "      <td>1.926453</td>\n",
              "      <td>-0.771086</td>\n",
              "      <td>-0.402627</td>\n",
              "      <td>0.104742</td>\n",
              "      <td>2.651770</td>\n",
              "      <td>...</td>\n",
              "      <td>0.098177</td>\n",
              "      <td>-0.111877</td>\n",
              "      <td>0.059881</td>\n",
              "      <td>0.004402</td>\n",
              "      <td>-0.130449</td>\n",
              "      <td>-0.039883</td>\n",
              "      <td>-0.035877</td>\n",
              "      <td>-0.000887</td>\n",
              "      <td>0.034442</td>\n",
              "      <td>0.006651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>0.237640</td>\n",
              "      <td>-0.383473</td>\n",
              "      <td>1.286859</td>\n",
              "      <td>-0.997945</td>\n",
              "      <td>-1.344278</td>\n",
              "      <td>-2.151856</td>\n",
              "      <td>-0.595879</td>\n",
              "      <td>1.329868</td>\n",
              "      <td>0.522781</td>\n",
              "      <td>-0.722415</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.013792</td>\n",
              "      <td>-0.225170</td>\n",
              "      <td>0.282117</td>\n",
              "      <td>0.269362</td>\n",
              "      <td>-0.101066</td>\n",
              "      <td>-0.071562</td>\n",
              "      <td>0.062494</td>\n",
              "      <td>-0.243757</td>\n",
              "      <td>0.005644</td>\n",
              "      <td>0.111357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>-1.861955</td>\n",
              "      <td>-0.168720</td>\n",
              "      <td>0.976660</td>\n",
              "      <td>-0.534565</td>\n",
              "      <td>0.455795</td>\n",
              "      <td>-0.776867</td>\n",
              "      <td>-0.452693</td>\n",
              "      <td>0.807612</td>\n",
              "      <td>-1.241835</td>\n",
              "      <td>-0.856995</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.404808</td>\n",
              "      <td>0.134439</td>\n",
              "      <td>-0.011237</td>\n",
              "      <td>0.114788</td>\n",
              "      <td>-0.153699</td>\n",
              "      <td>0.113997</td>\n",
              "      <td>0.306465</td>\n",
              "      <td>0.153019</td>\n",
              "      <td>0.296461</td>\n",
              "      <td>0.578454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>-1.001201</td>\n",
              "      <td>-0.630144</td>\n",
              "      <td>0.452858</td>\n",
              "      <td>-0.791519</td>\n",
              "      <td>-0.471162</td>\n",
              "      <td>-0.120609</td>\n",
              "      <td>-1.146068</td>\n",
              "      <td>0.575198</td>\n",
              "      <td>-0.205599</td>\n",
              "      <td>-1.098592</td>\n",
              "      <td>...</td>\n",
              "      <td>0.464877</td>\n",
              "      <td>0.212237</td>\n",
              "      <td>0.600586</td>\n",
              "      <td>-0.221337</td>\n",
              "      <td>-0.022835</td>\n",
              "      <td>-0.319404</td>\n",
              "      <td>0.166073</td>\n",
              "      <td>-0.512772</td>\n",
              "      <td>-0.361424</td>\n",
              "      <td>-0.032517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0.473754</td>\n",
              "      <td>1.692235</td>\n",
              "      <td>2.473895</td>\n",
              "      <td>0.629114</td>\n",
              "      <td>-1.383129</td>\n",
              "      <td>-0.759267</td>\n",
              "      <td>-1.741127</td>\n",
              "      <td>1.266693</td>\n",
              "      <td>0.232469</td>\n",
              "      <td>-1.812457</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004904</td>\n",
              "      <td>-0.185628</td>\n",
              "      <td>-0.350573</td>\n",
              "      <td>-0.336581</td>\n",
              "      <td>-0.136353</td>\n",
              "      <td>-0.042791</td>\n",
              "      <td>0.136589</td>\n",
              "      <td>0.500420</td>\n",
              "      <td>-0.004391</td>\n",
              "      <td>-0.130279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>-0.487110</td>\n",
              "      <td>2.589771</td>\n",
              "      <td>3.490235</td>\n",
              "      <td>0.222079</td>\n",
              "      <td>-0.976462</td>\n",
              "      <td>1.375417</td>\n",
              "      <td>-0.971469</td>\n",
              "      <td>1.235543</td>\n",
              "      <td>1.822634</td>\n",
              "      <td>-1.156856</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.133691</td>\n",
              "      <td>0.545830</td>\n",
              "      <td>-0.102365</td>\n",
              "      <td>-0.323477</td>\n",
              "      <td>-0.020121</td>\n",
              "      <td>0.130904</td>\n",
              "      <td>0.147145</td>\n",
              "      <td>0.290111</td>\n",
              "      <td>-0.048614</td>\n",
              "      <td>-0.078692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>-0.087586</td>\n",
              "      <td>0.320785</td>\n",
              "      <td>0.233389</td>\n",
              "      <td>0.674989</td>\n",
              "      <td>-1.858322</td>\n",
              "      <td>1.800417</td>\n",
              "      <td>-0.060712</td>\n",
              "      <td>0.065915</td>\n",
              "      <td>0.926925</td>\n",
              "      <td>-0.423506</td>\n",
              "      <td>...</td>\n",
              "      <td>0.124573</td>\n",
              "      <td>-0.122813</td>\n",
              "      <td>0.115199</td>\n",
              "      <td>0.041773</td>\n",
              "      <td>-0.095950</td>\n",
              "      <td>-0.080089</td>\n",
              "      <td>-0.320160</td>\n",
              "      <td>-0.034541</td>\n",
              "      <td>0.005961</td>\n",
              "      <td>0.263597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.077380</td>\n",
              "      <td>3.572014</td>\n",
              "      <td>0.896210</td>\n",
              "      <td>0.744622</td>\n",
              "      <td>-3.367878</td>\n",
              "      <td>3.256363</td>\n",
              "      <td>0.249904</td>\n",
              "      <td>-0.928727</td>\n",
              "      <td>0.021332</td>\n",
              "      <td>-0.950284</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.111266</td>\n",
              "      <td>0.093473</td>\n",
              "      <td>-0.106342</td>\n",
              "      <td>0.053073</td>\n",
              "      <td>-0.002879</td>\n",
              "      <td>-0.119720</td>\n",
              "      <td>0.188424</td>\n",
              "      <td>-0.140838</td>\n",
              "      <td>0.016097</td>\n",
              "      <td>0.267991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>-0.458112</td>\n",
              "      <td>1.236125</td>\n",
              "      <td>4.384637</td>\n",
              "      <td>0.560432</td>\n",
              "      <td>-2.208922</td>\n",
              "      <td>2.313967</td>\n",
              "      <td>-0.802622</td>\n",
              "      <td>-1.025953</td>\n",
              "      <td>-1.692228</td>\n",
              "      <td>2.282681</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.065489</td>\n",
              "      <td>0.016242</td>\n",
              "      <td>-0.002261</td>\n",
              "      <td>-0.017577</td>\n",
              "      <td>-0.059603</td>\n",
              "      <td>-0.027986</td>\n",
              "      <td>0.102249</td>\n",
              "      <td>-0.080871</td>\n",
              "      <td>0.056780</td>\n",
              "      <td>0.008198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>-0.673502</td>\n",
              "      <td>0.026571</td>\n",
              "      <td>2.225577</td>\n",
              "      <td>0.275774</td>\n",
              "      <td>-0.205987</td>\n",
              "      <td>0.014778</td>\n",
              "      <td>-0.940846</td>\n",
              "      <td>1.237993</td>\n",
              "      <td>0.477398</td>\n",
              "      <td>-0.691240</td>\n",
              "      <td>...</td>\n",
              "      <td>0.348846</td>\n",
              "      <td>0.204388</td>\n",
              "      <td>-0.137155</td>\n",
              "      <td>0.588687</td>\n",
              "      <td>-0.184511</td>\n",
              "      <td>-0.089425</td>\n",
              "      <td>0.280229</td>\n",
              "      <td>-0.663489</td>\n",
              "      <td>-0.166260</td>\n",
              "      <td>0.181005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>0.058923</td>\n",
              "      <td>-0.221803</td>\n",
              "      <td>0.737666</td>\n",
              "      <td>1.151257</td>\n",
              "      <td>-1.184505</td>\n",
              "      <td>0.206355</td>\n",
              "      <td>0.120637</td>\n",
              "      <td>1.580138</td>\n",
              "      <td>-0.950986</td>\n",
              "      <td>-0.260030</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.077231</td>\n",
              "      <td>-0.019061</td>\n",
              "      <td>0.074754</td>\n",
              "      <td>-0.591578</td>\n",
              "      <td>0.095841</td>\n",
              "      <td>0.198789</td>\n",
              "      <td>-0.336367</td>\n",
              "      <td>0.104396</td>\n",
              "      <td>0.270823</td>\n",
              "      <td>-0.588938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>-0.760917</td>\n",
              "      <td>0.429009</td>\n",
              "      <td>0.926646</td>\n",
              "      <td>-0.466413</td>\n",
              "      <td>-1.378488</td>\n",
              "      <td>1.120628</td>\n",
              "      <td>-1.294069</td>\n",
              "      <td>0.200679</td>\n",
              "      <td>-0.702918</td>\n",
              "      <td>-0.229318</td>\n",
              "      <td>...</td>\n",
              "      <td>0.546424</td>\n",
              "      <td>-0.282623</td>\n",
              "      <td>0.020602</td>\n",
              "      <td>0.301686</td>\n",
              "      <td>0.072675</td>\n",
              "      <td>0.271945</td>\n",
              "      <td>-0.248259</td>\n",
              "      <td>0.250986</td>\n",
              "      <td>-0.292536</td>\n",
              "      <td>-0.092558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.219442</td>\n",
              "      <td>1.270641</td>\n",
              "      <td>3.100799</td>\n",
              "      <td>0.898660</td>\n",
              "      <td>-1.054064</td>\n",
              "      <td>0.788914</td>\n",
              "      <td>-0.990733</td>\n",
              "      <td>0.515385</td>\n",
              "      <td>-0.417799</td>\n",
              "      <td>-0.476812</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.172917</td>\n",
              "      <td>-0.390296</td>\n",
              "      <td>0.496663</td>\n",
              "      <td>-0.263594</td>\n",
              "      <td>0.213383</td>\n",
              "      <td>-0.165381</td>\n",
              "      <td>-0.518336</td>\n",
              "      <td>0.160600</td>\n",
              "      <td>0.145911</td>\n",
              "      <td>0.233921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>-0.441407</td>\n",
              "      <td>0.296999</td>\n",
              "      <td>1.497629</td>\n",
              "      <td>0.187879</td>\n",
              "      <td>-1.194458</td>\n",
              "      <td>1.600436</td>\n",
              "      <td>-1.746629</td>\n",
              "      <td>1.181340</td>\n",
              "      <td>0.886013</td>\n",
              "      <td>0.025388</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.500080</td>\n",
              "      <td>0.189998</td>\n",
              "      <td>-0.160553</td>\n",
              "      <td>0.169078</td>\n",
              "      <td>0.432112</td>\n",
              "      <td>0.094936</td>\n",
              "      <td>0.095585</td>\n",
              "      <td>-0.312528</td>\n",
              "      <td>0.147194</td>\n",
              "      <td>-0.202735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>-1.509170</td>\n",
              "      <td>1.832384</td>\n",
              "      <td>0.814599</td>\n",
              "      <td>-0.880316</td>\n",
              "      <td>-0.857682</td>\n",
              "      <td>-0.492055</td>\n",
              "      <td>0.454086</td>\n",
              "      <td>1.153471</td>\n",
              "      <td>1.448765</td>\n",
              "      <td>-1.293766</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.068755</td>\n",
              "      <td>-0.106495</td>\n",
              "      <td>0.170446</td>\n",
              "      <td>0.169195</td>\n",
              "      <td>0.262094</td>\n",
              "      <td>-0.117901</td>\n",
              "      <td>-0.164800</td>\n",
              "      <td>-0.069491</td>\n",
              "      <td>-0.050880</td>\n",
              "      <td>-0.021687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>-0.155126</td>\n",
              "      <td>1.393178</td>\n",
              "      <td>0.365538</td>\n",
              "      <td>-1.099525</td>\n",
              "      <td>-1.299107</td>\n",
              "      <td>-0.408043</td>\n",
              "      <td>0.279709</td>\n",
              "      <td>-0.119767</td>\n",
              "      <td>0.985277</td>\n",
              "      <td>-0.515693</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.150809</td>\n",
              "      <td>0.028025</td>\n",
              "      <td>-0.112792</td>\n",
              "      <td>-0.361274</td>\n",
              "      <td>-0.143445</td>\n",
              "      <td>-0.003529</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.092871</td>\n",
              "      <td>0.046724</td>\n",
              "      <td>0.017889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>-1.172911</td>\n",
              "      <td>2.346800</td>\n",
              "      <td>2.090372</td>\n",
              "      <td>-1.631477</td>\n",
              "      <td>-0.952774</td>\n",
              "      <td>0.236958</td>\n",
              "      <td>-0.389693</td>\n",
              "      <td>0.292413</td>\n",
              "      <td>-0.029682</td>\n",
              "      <td>-0.423091</td>\n",
              "      <td>...</td>\n",
              "      <td>0.061643</td>\n",
              "      <td>-0.252224</td>\n",
              "      <td>0.104380</td>\n",
              "      <td>0.054052</td>\n",
              "      <td>-0.149576</td>\n",
              "      <td>0.054327</td>\n",
              "      <td>-0.085206</td>\n",
              "      <td>-0.020826</td>\n",
              "      <td>-0.022760</td>\n",
              "      <td>-0.051240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>-0.328603</td>\n",
              "      <td>3.061680</td>\n",
              "      <td>2.256522</td>\n",
              "      <td>-0.424378</td>\n",
              "      <td>-1.868915</td>\n",
              "      <td>-0.350414</td>\n",
              "      <td>0.226009</td>\n",
              "      <td>1.162425</td>\n",
              "      <td>0.603590</td>\n",
              "      <td>-0.217285</td>\n",
              "      <td>...</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.008060</td>\n",
              "      <td>-0.163510</td>\n",
              "      <td>0.093020</td>\n",
              "      <td>-0.086098</td>\n",
              "      <td>0.027652</td>\n",
              "      <td>0.021020</td>\n",
              "      <td>0.002824</td>\n",
              "      <td>-0.078523</td>\n",
              "      <td>-0.032950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>-0.718224</td>\n",
              "      <td>0.687967</td>\n",
              "      <td>-1.267417</td>\n",
              "      <td>-0.670412</td>\n",
              "      <td>-1.567853</td>\n",
              "      <td>1.657080</td>\n",
              "      <td>0.818342</td>\n",
              "      <td>0.515184</td>\n",
              "      <td>-0.144439</td>\n",
              "      <td>0.125603</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.044075</td>\n",
              "      <td>0.357937</td>\n",
              "      <td>0.251036</td>\n",
              "      <td>-0.005687</td>\n",
              "      <td>0.141986</td>\n",
              "      <td>-0.258837</td>\n",
              "      <td>0.109927</td>\n",
              "      <td>-0.115133</td>\n",
              "      <td>-0.061322</td>\n",
              "      <td>-0.033643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>0.445232</td>\n",
              "      <td>1.184668</td>\n",
              "      <td>0.467780</td>\n",
              "      <td>-2.138310</td>\n",
              "      <td>-1.128685</td>\n",
              "      <td>-0.514149</td>\n",
              "      <td>0.245567</td>\n",
              "      <td>2.021899</td>\n",
              "      <td>0.228138</td>\n",
              "      <td>-0.953547</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.011740</td>\n",
              "      <td>0.419290</td>\n",
              "      <td>0.062435</td>\n",
              "      <td>0.191918</td>\n",
              "      <td>0.126068</td>\n",
              "      <td>0.287124</td>\n",
              "      <td>-0.040200</td>\n",
              "      <td>0.194610</td>\n",
              "      <td>-0.116119</td>\n",
              "      <td>0.169397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>-1.120806</td>\n",
              "      <td>0.495716</td>\n",
              "      <td>0.477714</td>\n",
              "      <td>-1.356863</td>\n",
              "      <td>-0.036207</td>\n",
              "      <td>0.809519</td>\n",
              "      <td>-0.160998</td>\n",
              "      <td>-0.731385</td>\n",
              "      <td>1.327201</td>\n",
              "      <td>0.967042</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.069784</td>\n",
              "      <td>0.047647</td>\n",
              "      <td>-0.162943</td>\n",
              "      <td>-0.053178</td>\n",
              "      <td>0.104861</td>\n",
              "      <td>0.008137</td>\n",
              "      <td>-0.253575</td>\n",
              "      <td>-0.041771</td>\n",
              "      <td>-0.019096</td>\n",
              "      <td>0.024908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>-1.746276</td>\n",
              "      <td>1.329017</td>\n",
              "      <td>1.284418</td>\n",
              "      <td>-0.877288</td>\n",
              "      <td>-0.101824</td>\n",
              "      <td>0.200079</td>\n",
              "      <td>0.221382</td>\n",
              "      <td>-0.569326</td>\n",
              "      <td>1.123788</td>\n",
              "      <td>-0.238124</td>\n",
              "      <td>...</td>\n",
              "      <td>0.070366</td>\n",
              "      <td>-0.196414</td>\n",
              "      <td>0.009736</td>\n",
              "      <td>0.119265</td>\n",
              "      <td>0.025462</td>\n",
              "      <td>0.260401</td>\n",
              "      <td>0.209111</td>\n",
              "      <td>-0.092553</td>\n",
              "      <td>0.108953</td>\n",
              "      <td>-0.142083</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>106 rows × 94 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e83d404-f348-4167-9f41-8cd333a55cd7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0e83d404-f348-4167-9f41-8cd333a55cd7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0e83d404-f348-4167-9f41-8cd333a55cd7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2c746ede-431b-4895-b3e6-665df0556f42\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2c746ede-431b-4895-b3e6-665df0556f42')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2c746ede-431b-4895-b3e6-665df0556f42 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_d424acf5-e017-4e05-a671-446c8a478999\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('X_train_df_copy')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d424acf5-e017-4e05-a671-446c8a478999 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('X_train_df_copy');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train_df_copy"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "X_train_df_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "443b582d",
      "metadata": {
        "id": "443b582d",
        "outputId": "b91a823a-a2bc-4e94-dc9b-3fe4133f3668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pandas.core.series.Series</b><br/>def __init__(data=None, index=None, dtype: Dtype | None=None, name=None, copy: bool | None=None, fastpath: bool | lib.NoDefault=lib.no_default) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pandas/core/series.py</a>One-dimensional ndarray with axis labels (including time series).\n",
              "\n",
              "Labels need not be unique but must be a hashable type. The object\n",
              "supports both integer- and label-based indexing and provides a host of\n",
              "methods for performing operations involving the index. Statistical\n",
              "methods from ndarray have been overridden to automatically exclude\n",
              "missing data (currently represented as NaN).\n",
              "\n",
              "Operations between Series (+, -, /, \\*, \\*\\*) align values based on their\n",
              "associated index values-- they need not be the same length. The result\n",
              "index will be the sorted union of the two indexes.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "data : array-like, Iterable, dict, or scalar value\n",
              "    Contains data stored in Series. If data is a dict, argument order is\n",
              "    maintained.\n",
              "index : array-like or Index (1d)\n",
              "    Values must be hashable and have the same length as `data`.\n",
              "    Non-unique index values are allowed. Will default to\n",
              "    RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n",
              "    and index is None, then the keys in the data are used as the index. If the\n",
              "    index is not None, the resulting Series is reindexed with the index values.\n",
              "dtype : str, numpy.dtype, or ExtensionDtype, optional\n",
              "    Data type for the output Series. If not specified, this will be\n",
              "    inferred from `data`.\n",
              "    See the :ref:`user guide &lt;basics.dtypes&gt;` for more usages.\n",
              "name : Hashable, default None\n",
              "    The name to give to the Series.\n",
              "copy : bool, default False\n",
              "    Copy input data. Only affects Series or 1d ndarray input. See examples.\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Please reference the :ref:`User Guide &lt;basics.series&gt;` for more information.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "Constructing Series from a dictionary with an Index specified\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3}\n",
              "&gt;&gt;&gt; ser = pd.Series(data=d, index=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;])\n",
              "&gt;&gt;&gt; ser\n",
              "a   1\n",
              "b   2\n",
              "c   3\n",
              "dtype: int64\n",
              "\n",
              "The keys of the dictionary match with the Index values, hence the Index\n",
              "values have no effect.\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3}\n",
              "&gt;&gt;&gt; ser = pd.Series(data=d, index=[&#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;])\n",
              "&gt;&gt;&gt; ser\n",
              "x   NaN\n",
              "y   NaN\n",
              "z   NaN\n",
              "dtype: float64\n",
              "\n",
              "Note that the Index is first build with the keys from the dictionary.\n",
              "After this the Series is reindexed with the given Index values, hence we\n",
              "get all NaN as a result.\n",
              "\n",
              "Constructing Series from a list with `copy=False`.\n",
              "\n",
              "&gt;&gt;&gt; r = [1, 2]\n",
              "&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n",
              "&gt;&gt;&gt; ser.iloc[0] = 999\n",
              "&gt;&gt;&gt; r\n",
              "[1, 2]\n",
              "&gt;&gt;&gt; ser\n",
              "0    999\n",
              "1      2\n",
              "dtype: int64\n",
              "\n",
              "Due to input data type the Series has a `copy` of\n",
              "the original data even though `copy=False`, so\n",
              "the data is unchanged.\n",
              "\n",
              "Constructing Series from a 1d ndarray with `copy=False`.\n",
              "\n",
              "&gt;&gt;&gt; r = np.array([1, 2])\n",
              "&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n",
              "&gt;&gt;&gt; ser.iloc[0] = 999\n",
              "&gt;&gt;&gt; r\n",
              "array([999,   2])\n",
              "&gt;&gt;&gt; ser\n",
              "0    999\n",
              "1      2\n",
              "dtype: int64\n",
              "\n",
              "Due to input data type the Series has a `view` on\n",
              "the original data, so\n",
              "the data is changed as well.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 263);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "labels = pd.read_csv(\"/AnimalLabels.csv\")\n",
        "labels['majority_vote'] = labels.mode(axis=1, numeric_only=True).astype(int)\n",
        "type(labels['majority_vote'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "2d5eb9c2",
      "metadata": {
        "id": "2d5eb9c2"
      },
      "outputs": [],
      "source": [
        "X_train_df_copy['label']=labels['majority_vote']\n",
        "y=X_train_df_copy['label']\n",
        "X=X_train_df_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "686eb97f",
      "metadata": {
        "id": "686eb97f",
        "outputId": "5c715239-36d5-43e2-f3ca-dc38ebbde559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6   \\\n",
              "0   -2.637099 -0.347802 -1.305130  0.477352  0.222379  2.597939  1.865067   \n",
              "1   -2.798193  3.634745  2.086230  3.196251  1.620699 -0.670871  1.726479   \n",
              "2   -1.739392 -1.241732 -1.321686 -1.510875 -0.779627 -1.543128  1.477094   \n",
              "3   -0.994942  1.074568  1.187057  1.138465 -0.079325 -4.044148  1.202367   \n",
              "4   -1.329136  2.676327  0.291800  0.271295 -0.906031 -1.966462  1.868663   \n",
              "5   -1.131488  0.223700  0.368406 -1.841575 -0.366648 -3.507539  0.586460   \n",
              "6   -1.042963  0.784338  0.155310  2.144182 -0.328153 -3.037030  2.740792   \n",
              "7   -2.123212  2.594353  0.784899  2.561225  0.298017 -1.477255  2.404881   \n",
              "8   -2.428108  0.587128  0.336312  1.760723 -0.277718 -1.053064  2.272779   \n",
              "9   -1.996928  0.378025  1.192909  1.650560  0.339481 -1.626476  1.924355   \n",
              "10  -2.397497  2.225219  1.587146  3.089433  0.767364 -2.262547  2.458541   \n",
              "11  -3.121665  3.285395  2.648926  2.631635  2.515994  1.229619  0.031375   \n",
              "12  -3.878436 -1.077783  1.972808  1.255909  4.699007  1.939041 -0.401190   \n",
              "13  -4.014367 -1.252404  1.606471  0.522180  4.165790  1.218129  0.170922   \n",
              "14  -3.816117 -1.324373  1.602825  0.552906  4.097568  0.830133 -0.096257   \n",
              "15  -3.689973 -2.155047  0.552009  0.078458  2.731397  0.792978  0.293722   \n",
              "16  -3.664803 -2.112624  0.709510  0.316549  3.421074  0.577376 -0.058770   \n",
              "17  -3.472888 -2.414408  1.069169  0.544240  3.510205  1.037111 -0.485694   \n",
              "18  -0.154480 -1.546400 -0.879493  1.928970 -1.528069  1.393321  0.603145   \n",
              "19  -0.429591 -1.061395 -1.073781  1.603877 -1.000881  0.137796 -0.474644   \n",
              "20   1.169234  0.563495 -0.556821  2.166775 -1.110425 -1.226026 -0.370970   \n",
              "21   1.997442 -0.566104 -2.878561  2.048987 -2.044952  1.675383 -0.844437   \n",
              "22   0.983302 -1.229718 -1.498552  1.839532 -1.240514  0.511564 -0.776103   \n",
              "23   1.557271  0.050281 -1.899174  1.712817 -0.925847 -1.272008 -0.870842   \n",
              "24   1.250082 -0.475840 -1.731295  1.065621 -1.003865 -0.619206  0.185522   \n",
              "25   0.517654 -0.726796 -0.476584  0.659845 -0.101265 -0.393997 -0.535044   \n",
              "26  -0.252770 -2.423221 -1.737167  1.730142 -0.774093 -0.778728 -0.803385   \n",
              "27   1.164473 -1.224640 -0.914450  2.582735 -1.158372 -0.549832 -0.848184   \n",
              "28   1.911768 -0.915630 -0.944989  2.965577 -1.213963 -0.835333 -1.340682   \n",
              "29  -0.408548 -1.834815 -0.125575  3.054345 -0.877931 -0.548621 -0.374316   \n",
              "30   1.467009 -1.673569 -0.998931  3.212867 -1.358372 -1.950890 -1.870762   \n",
              "31   1.693128 -1.905936 -1.482452  2.580142 -0.631265 -1.065471 -0.645155   \n",
              "32   0.326083 -2.357328 -0.837244  0.915371 -0.114251 -0.320591 -0.543085   \n",
              "33  -3.097785 -1.540616 -1.078272 -1.299055  0.463412  1.260984  0.520225   \n",
              "34  -2.872596 -2.289240 -0.718798 -0.856928  0.755319  0.446068  0.125273   \n",
              "35  -2.642872 -3.093603 -1.580577 -0.983180  0.529433  0.042502  0.219207   \n",
              "36  -2.666563 -2.811451 -1.320080 -0.584977  0.549821  0.310147  0.132449   \n",
              "37  -2.522127 -2.950680 -0.912822 -0.742229  0.636310  0.272814 -0.124880   \n",
              "38  -2.824209 -2.713716 -1.641778 -0.771857  0.380510 -0.132937  0.730788   \n",
              "39  -3.420457 -2.746807 -1.231707 -0.784039  1.139511  1.333265  0.306230   \n",
              "40  -1.099295  0.465213  1.334637 -2.250615 -1.491715 -0.653282 -0.849536   \n",
              "41  -1.169595 -2.462802  1.489594 -3.346595 -0.699219 -2.433538 -1.578992   \n",
              "42  -0.188029 -2.039316  2.035923 -2.630550  0.002784 -0.922663 -1.055911   \n",
              "43  -1.340439 -0.345850  3.167113 -2.315758 -0.589326 -1.100225 -1.353995   \n",
              "44  -0.038033 -1.146338  0.056533 -1.408917 -0.829297 -2.161916  0.448988   \n",
              "45  -0.402297 -2.237480  2.388917 -2.855490  0.062406 -0.589928 -0.938151   \n",
              "46  -1.313723 -2.045631  1.158891 -2.354800 -0.732532 -1.073207 -0.775655   \n",
              "47  -1.048675 -1.715145  2.515703 -2.073400 -0.410095 -0.572684 -1.261211   \n",
              "48  -1.276280  0.422902  2.890507 -0.298685 -0.156793  0.877776  0.280889   \n",
              "49  -1.675160 -0.223162 -0.959539 -1.191272 -1.148406  2.156334  2.685866   \n",
              "50  -0.901350  0.623569 -1.150486 -2.642692 -1.667718 -0.243350  1.203611   \n",
              "51  -0.174547  0.067538 -1.066564 -1.551703 -1.708644  1.853259  2.516707   \n",
              "52  -0.282104 -1.011302 -0.691323 -1.847908 -1.607443 -0.038352  1.813729   \n",
              "53  -0.769889 -1.683174 -1.707398 -1.639610 -1.496469 -0.361207  0.058345   \n",
              "54  -0.081574 -1.092008 -1.654655 -2.202721 -0.719607 -0.060184  1.042880   \n",
              "55  -1.234132 -0.838562 -2.011426 -1.940953 -1.865559  0.548632  1.478590   \n",
              "56  -1.377481 -0.065721 -1.081459 -0.250382 -1.244762  3.282794  1.888986   \n",
              "57  -1.136689  5.804210 -2.158707 -0.711264  2.630094 -0.722201 -2.566602   \n",
              "58  -1.971310  4.386899 -4.043365 -1.361281  1.282604 -0.496142 -1.185623   \n",
              "59  -0.847435  4.087546 -3.075522 -1.403359  1.478408 -0.990298 -1.038978   \n",
              "60  -2.034989  5.777314 -3.917526 -1.192115  0.948309  0.384854 -0.618132   \n",
              "61  -2.899686  3.653677 -4.107776 -0.837462  2.314684 -0.099674 -0.377653   \n",
              "62  -3.044610  4.717416 -2.692057 -0.515452  3.082248 -0.087261 -1.969000   \n",
              "63   7.514398 -0.635832  0.391142 -0.657821  1.896845 -0.123870  0.429798   \n",
              "64   6.081498  0.184675  0.796057  0.446617  0.916233  0.107495  1.029045   \n",
              "65   6.417571 -1.035532  0.593525 -1.207628  1.777169 -0.666690  0.538690   \n",
              "66   4.853528  0.146181  0.280188  0.419659 -0.696790  1.923275  0.624137   \n",
              "67   5.767001 -0.226511  0.799813 -0.561765  1.164284  0.518367  0.365658   \n",
              "68   5.465487 -0.097514  0.327609 -0.011427  1.276314  0.224520  0.800988   \n",
              "69   3.633630  6.278797 -2.002323 -1.931037 -0.796981  0.588673 -0.935282   \n",
              "70   6.400256  0.244836 -0.896523 -0.428111  0.628118  1.700274  0.879492   \n",
              "71   6.061324 -0.621618 -0.241531 -0.590321  1.098110  0.639856  0.818704   \n",
              "72   9.025208 -0.402932 -0.213329 -1.001249  2.936993 -0.683901  0.119570   \n",
              "73   7.222217 -1.011508  0.332970 -0.405879  2.436039 -0.023262  0.352652   \n",
              "74   6.948776 -0.575131  1.190701 -0.513953  1.864702  0.053287  0.334423   \n",
              "75   7.114773  0.999335  0.408795 -0.790729  1.222958  0.188703  0.416799   \n",
              "76   8.566109  0.652017  0.555999 -0.039120  1.827801  0.268175  0.529672   \n",
              "77  -0.645925 -2.615459 -1.874452  0.331043 -0.124215  0.806257 -0.853601   \n",
              "78  -1.407767 -2.200869 -1.877372 -0.870802 -0.213478 -1.136117 -0.138414   \n",
              "79   0.321378 -1.875157 -0.427114  1.717297  0.642612 -0.206885 -1.512476   \n",
              "80   0.253918 -0.777346 -0.444279  2.374200 -0.030165 -0.970837 -0.715522   \n",
              "81   1.031057  0.081395 -0.383579  2.403520 -1.243376  0.543347 -1.806651   \n",
              "82   0.807086 -1.028513 -2.842726  1.866244 -0.376761  0.616221 -0.697409   \n",
              "83  -0.395075 -2.009826 -2.418936  1.200081 -0.904394  0.677463 -1.362367   \n",
              "84  -0.186865  1.001387  3.046893  0.714120 -1.772765  1.926453 -0.771086   \n",
              "85   0.237640 -0.383473  1.286859 -0.997945 -1.344278 -2.151856 -0.595879   \n",
              "86  -1.861955 -0.168720  0.976660 -0.534565  0.455795 -0.776867 -0.452693   \n",
              "87  -1.001201 -0.630144  0.452858 -0.791519 -0.471162 -0.120609 -1.146068   \n",
              "88   0.473754  1.692235  2.473895  0.629114 -1.383129 -0.759267 -1.741127   \n",
              "89  -0.487110  2.589771  3.490235  0.222079 -0.976462  1.375417 -0.971469   \n",
              "90  -0.087586  0.320785  0.233389  0.674989 -1.858322  1.800417 -0.060712   \n",
              "91   0.077380  3.572014  0.896210  0.744622 -3.367878  3.256363  0.249904   \n",
              "92  -0.458112  1.236125  4.384637  0.560432 -2.208922  2.313967 -0.802622   \n",
              "93  -0.673502  0.026571  2.225577  0.275774 -0.205987  0.014778 -0.940846   \n",
              "94   0.058923 -0.221803  0.737666  1.151257 -1.184505  0.206355  0.120637   \n",
              "95  -0.760917  0.429009  0.926646 -0.466413 -1.378488  1.120628 -1.294069   \n",
              "96   0.219442  1.270641  3.100799  0.898660 -1.054064  0.788914 -0.990733   \n",
              "97  -0.441407  0.296999  1.497629  0.187879 -1.194458  1.600436 -1.746629   \n",
              "98  -1.509170  1.832384  0.814599 -0.880316 -0.857682 -0.492055  0.454086   \n",
              "99  -0.155126  1.393178  0.365538 -1.099525 -1.299107 -0.408043  0.279709   \n",
              "100 -1.172911  2.346800  2.090372 -1.631477 -0.952774  0.236958 -0.389693   \n",
              "101 -0.328603  3.061680  2.256522 -0.424378 -1.868915 -0.350414  0.226009   \n",
              "102 -0.718224  0.687967 -1.267417 -0.670412 -1.567853  1.657080  0.818342   \n",
              "103  0.445232  1.184668  0.467780 -2.138310 -1.128685 -0.514149  0.245567   \n",
              "104 -1.120806  0.495716  0.477714 -1.356863 -0.036207  0.809519 -0.160998   \n",
              "105 -1.746276  1.329017  1.284418 -0.877288 -0.101824  0.200079  0.221382   \n",
              "\n",
              "           7         8         9   ...        84        85        86  \\\n",
              "0   -0.494544  0.243539  0.256078  ...  0.245732  0.019818 -0.246347   \n",
              "1   -0.083495  0.892471  0.753701  ... -0.439034 -0.363662  0.125037   \n",
              "2   -1.396603  0.295762 -0.055510  ... -0.171100  0.180284  0.044292   \n",
              "3   -0.249646 -0.755334  1.255697  ...  0.026375  0.007201 -0.165179   \n",
              "4   -1.244299 -0.616884 -0.451323  ... -0.145272  0.047353 -0.150540   \n",
              "5   -1.019767 -0.007901 -0.290260  ...  0.189601 -0.115965 -0.133011   \n",
              "6    0.366178 -0.238992  0.099877  ...  0.050243  0.100481  0.019779   \n",
              "7   -0.012778  0.333973 -0.174705  ...  0.282903  0.042752 -0.064071   \n",
              "8    0.317180 -1.332220  0.513478  ... -0.087155 -0.092904  0.226356   \n",
              "9    0.396526 -0.691455  0.126106  ... -0.414637  0.052247  0.295735   \n",
              "10   0.782024  0.100698  0.812775  ...  0.231648  0.255910 -0.032445   \n",
              "11  -0.082745  1.461712  0.656578  ...  0.247150 -0.025595  0.045100   \n",
              "12  -1.359237  1.269679 -1.087073  ...  0.022675  0.144465 -0.136204   \n",
              "13  -1.910882  0.101441 -0.647171  ...  0.023379 -0.080196  0.042130   \n",
              "14  -1.736858  0.080458 -0.714832  ... -0.397660  0.245568  0.147820   \n",
              "15  -0.278633 -1.153374 -0.412010  ...  0.343332  0.107223 -0.347163   \n",
              "16  -1.129333 -0.513825 -0.480888  ... -0.062118 -0.189135  0.239185   \n",
              "17  -0.891980 -0.098563 -0.805079  ...  0.206288 -0.319841 -0.010459   \n",
              "18   0.218529  0.584960 -0.553539  ... -0.110231 -0.054468 -0.567831   \n",
              "19  -0.926504 -0.011670 -1.288752  ... -0.535468 -0.262623 -0.325703   \n",
              "20  -1.031656 -0.418541 -1.876619  ...  0.189869  0.032763  0.228721   \n",
              "21  -0.803123  0.805781 -0.070944  ... -0.094073 -0.009146 -0.022008   \n",
              "22  -1.325901 -0.012663 -0.640896  ... -0.100999  0.351998 -0.218062   \n",
              "23  -1.380464 -0.368071 -2.021720  ...  0.073958 -0.053034 -0.191372   \n",
              "24  -1.619284 -0.016145 -1.919809  ...  0.279233  0.181656 -0.061430   \n",
              "25  -0.772317 -0.523049 -0.591549  ...  0.000589 -0.182640 -0.225018   \n",
              "26  -1.292481 -0.229035 -1.442089  ...  0.165914  0.036989  0.783753   \n",
              "27  -0.246047 -1.207821 -0.409443  ... -0.609457 -0.247284 -0.058122   \n",
              "28  -0.109988 -0.707614 -0.181341  ...  0.131895  0.306022 -0.058066   \n",
              "29  -0.581437 -0.293733  0.157777  ...  0.567677 -0.090892 -0.030952   \n",
              "30  -0.866857  2.623033  2.573387  ... -0.035778 -0.083382  0.101628   \n",
              "31   1.057783  1.651538  1.695190  ...  0.070735  0.252007 -0.153929   \n",
              "32   0.315936 -1.011740  0.073746  ... -0.289944  0.309996 -0.038902   \n",
              "33   1.642150  0.120016  0.015410  ...  0.235966 -0.282299  0.452872   \n",
              "34   1.672759 -0.015177  0.025464  ... -0.676102  0.357956  0.182697   \n",
              "35   2.124160  0.120424  0.003614  ... -0.268502  0.051885  0.148850   \n",
              "36   1.713566 -0.158619 -0.190742  ...  0.088132  0.001901 -0.181751   \n",
              "37   1.836601  0.079334 -0.030521  ... -0.040810  0.130532 -0.218607   \n",
              "38   2.014792 -0.374588 -0.057618  ...  0.264797 -0.337104 -0.143719   \n",
              "39   1.180762 -0.165200  0.027691  ...  0.161658  0.307317 -0.141023   \n",
              "40  -1.283735  1.205869 -0.368846  ...  0.062673 -0.028989 -0.073108   \n",
              "41  -1.184700  0.438590  0.760127  ... -0.029340  0.150340  0.012598   \n",
              "42  -0.040532 -0.674353 -0.101740  ... -0.018708 -0.172450  0.215015   \n",
              "43  -0.349298 -0.162986  0.367785  ... -0.405684  0.064107 -0.206222   \n",
              "44   0.335673  0.148819 -0.241603  ...  0.011225  0.003135 -0.235874   \n",
              "45  -0.651540 -0.756341  1.189794  ...  0.257882  0.211339 -0.195754   \n",
              "46  -0.032923 -0.081050  0.575991  ... -0.084312 -0.188685 -0.034155   \n",
              "47  -0.960969 -0.815623  0.911458  ...  0.251077 -0.076883  0.164421   \n",
              "48   0.045263 -1.205677  0.486009  ...  0.146607  0.283305 -0.191963   \n",
              "49   0.397253  0.193994  0.309965  ...  0.185404  0.213859 -0.262786   \n",
              "50  -0.993256 -0.761329 -0.080740  ... -0.022582 -0.046744 -0.341623   \n",
              "51   0.415582 -0.010141 -0.189395  ... -0.240947 -0.549458 -0.159876   \n",
              "52  -0.338364 -0.535617 -0.215629  ...  0.070796 -0.140182  0.468011   \n",
              "53  -2.671555  2.745443  0.888032  ... -0.076348 -0.073837  0.271482   \n",
              "54   0.402992  0.703606  0.118129  ... -0.059140 -0.044507 -0.247009   \n",
              "55  -0.938380  0.188878  0.560593  ... -0.038754  0.012199  0.164491   \n",
              "56  -0.334953 -0.230487  0.198537  ... -0.139545  0.036456  0.343505   \n",
              "57   0.452389  0.073112 -0.337740  ... -0.145760 -0.150223 -0.006571   \n",
              "58   0.547070 -0.444554  0.370865  ...  0.111135 -0.015964 -0.099462   \n",
              "59   0.255356 -1.686040  0.209921  ... -0.023100  0.132558  0.012866   \n",
              "60   0.268305 -0.687963  1.258583  ... -0.025951 -0.084966  0.155026   \n",
              "61  -0.632654  0.191662  1.746729  ... -0.031892  0.018460  0.062417   \n",
              "62   0.516689  0.326533  0.505000  ...  0.106817  0.107516 -0.139848   \n",
              "63   0.674668  1.341630  0.361575  ... -0.045240  0.000368 -0.091696   \n",
              "64   0.884821 -0.497122 -0.093180  ...  0.159137  0.025531  0.197970   \n",
              "65   0.050725  0.978742  0.283329  ...  0.064904 -0.130967 -0.095892   \n",
              "66  -0.689111 -1.006972  1.283476  ... -0.119253 -0.194984 -0.072416   \n",
              "67  -0.387542 -0.793323  0.647585  ... -0.029994  0.153425  0.528758   \n",
              "68  -0.654831 -1.101378 -0.044630  ... -0.431430 -0.058752 -0.145380   \n",
              "69  -1.179236 -0.647395 -1.130020  ...  0.033230 -0.016170  0.078437   \n",
              "70  -0.601516  0.319824  0.149369  ...  0.032566 -0.169036 -0.155438   \n",
              "71   0.239169 -0.119760 -0.224642  ...  0.306576  0.747988  0.281006   \n",
              "72   0.352377  1.310474  0.193839  ... -0.425576  0.079482  0.219548   \n",
              "73   1.205280 -0.604935 -0.035388  ...  0.264695 -0.485988 -0.575964   \n",
              "74  -0.239748 -0.058462  0.341092  ...  0.128762  0.111403  0.022093   \n",
              "75  -0.561276 -0.129169 -0.361816  ...  0.298246 -0.120146 -0.243777   \n",
              "76   0.209192 -0.334205 -0.545892  ... -0.106814  0.010436  0.240260   \n",
              "77   0.681877 -0.201955  0.029968  ...  0.084995 -0.950816  0.152380   \n",
              "78  -1.388718  0.304442  0.732854  ...  0.083153  0.141068 -0.196071   \n",
              "79   0.858094 -1.546386  0.572665  ... -0.411556 -0.015249 -0.172982   \n",
              "80   1.849128  1.309309  0.333597  ...  0.076484 -0.439216  0.089563   \n",
              "81   0.558427 -2.667445  1.703981  ...  0.220287  0.151462  0.054684   \n",
              "82   1.367291  1.624622  0.368991  ...  0.229554  0.133209  0.237593   \n",
              "83  -0.608868 -0.813081  0.126835  ... -0.018812  0.267450 -0.128386   \n",
              "84  -0.402627  0.104742  2.651770  ...  0.098177 -0.111877  0.059881   \n",
              "85   1.329868  0.522781 -0.722415  ... -0.013792 -0.225170  0.282117   \n",
              "86   0.807612 -1.241835 -0.856995  ... -0.404808  0.134439 -0.011237   \n",
              "87   0.575198 -0.205599 -1.098592  ...  0.464877  0.212237  0.600586   \n",
              "88   1.266693  0.232469 -1.812457  ...  0.004904 -0.185628 -0.350573   \n",
              "89   1.235543  1.822634 -1.156856  ... -0.133691  0.545830 -0.102365   \n",
              "90   0.065915  0.926925 -0.423506  ...  0.124573 -0.122813  0.115199   \n",
              "91  -0.928727  0.021332 -0.950284  ... -0.111266  0.093473 -0.106342   \n",
              "92  -1.025953 -1.692228  2.282681  ... -0.065489  0.016242 -0.002261   \n",
              "93   1.237993  0.477398 -0.691240  ...  0.348846  0.204388 -0.137155   \n",
              "94   1.580138 -0.950986 -0.260030  ... -0.077231 -0.019061  0.074754   \n",
              "95   0.200679 -0.702918 -0.229318  ...  0.546424 -0.282623  0.020602   \n",
              "96   0.515385 -0.417799 -0.476812  ... -0.172917 -0.390296  0.496663   \n",
              "97   1.181340  0.886013  0.025388  ... -0.500080  0.189998 -0.160553   \n",
              "98   1.153471  1.448765 -1.293766  ... -0.068755 -0.106495  0.170446   \n",
              "99  -0.119767  0.985277 -0.515693  ... -0.150809  0.028025 -0.112792   \n",
              "100  0.292413 -0.029682 -0.423091  ...  0.061643 -0.252224  0.104380   \n",
              "101  1.162425  0.603590 -0.217285  ...  0.227745  0.008060 -0.163510   \n",
              "102  0.515184 -0.144439  0.125603  ... -0.044075  0.357937  0.251036   \n",
              "103  2.021899  0.228138 -0.953547  ... -0.011740  0.419290  0.062435   \n",
              "104 -0.731385  1.327201  0.967042  ... -0.069784  0.047647 -0.162943   \n",
              "105 -0.569326  1.123788 -0.238124  ...  0.070366 -0.196414  0.009736   \n",
              "\n",
              "           87        88        89        90        91        92        93  \n",
              "0   -0.334189  0.001933  0.099690  0.005821  0.098615  0.322466 -0.026514  \n",
              "1   -0.009941  0.000198  0.598846  0.399056 -0.448141 -0.133951  0.163589  \n",
              "2    0.000196  0.003606 -0.028537  0.019995  0.014549 -0.207424 -0.134106  \n",
              "3   -0.136537  0.035306 -0.042689 -0.063572 -0.160413  0.073046 -0.025301  \n",
              "4   -0.009131 -0.106808 -0.137046 -0.175852  0.152502  0.045290 -0.086046  \n",
              "5   -0.063879  0.143104  0.020425 -0.124293 -0.026170  0.228428 -0.088222  \n",
              "6    0.102530  0.022082  0.114263 -0.260516  0.086962  0.043728  0.082299  \n",
              "7    0.104548  0.054169 -0.324595  0.354240  0.060420  0.102943 -0.161668  \n",
              "8   -0.181613 -0.083627  0.256577  0.209390 -0.140765 -0.330571 -0.136723  \n",
              "9    0.499863 -0.019423  0.048918  0.008529  0.233998  0.220361  0.240518  \n",
              "10  -0.373376 -0.155772 -0.378488 -0.422746 -0.007195 -0.081527  0.064764  \n",
              "11   0.256206  0.089645 -0.136949 -0.139985  0.367463  0.123369 -0.021251  \n",
              "12  -0.659289 -0.348931  0.062967 -0.088760 -0.275793 -0.002524  0.053070  \n",
              "13  -0.058174  0.102286 -0.200376  0.157605  0.767559 -0.343533 -0.128184  \n",
              "14   0.036492 -0.018615  0.022910  0.148818 -0.153438 -0.144382  0.505064  \n",
              "15  -0.242998 -0.231267 -0.143570 -0.391664 -0.443641  0.215185 -0.191292  \n",
              "16   0.417584 -0.102288 -0.046368  0.014686  0.194612  0.031704 -0.161622  \n",
              "17   0.574983  0.571460  0.053335 -0.015459 -0.250182  0.438432 -0.287490  \n",
              "18   0.406893 -0.399969  0.101343  0.014383  0.095134  0.098185 -0.259110  \n",
              "19   0.316703 -0.196790  0.313602 -0.153492 -0.250178 -0.260607 -0.131871  \n",
              "20   0.101168  0.307811 -0.011972 -0.280188  0.119275 -0.074072  0.044002  \n",
              "21  -0.162247  0.004115 -0.031810  0.047193  0.030495 -0.001331  0.132705  \n",
              "22   0.053933  0.120976 -0.339442  0.033228  0.098141  0.400648  0.026211  \n",
              "23   0.126416 -0.114568  0.063703 -0.147723  0.231087  0.267255 -0.236652  \n",
              "24   0.089911  0.253002  0.137546 -0.196125 -0.049718 -0.640881  0.196521  \n",
              "25   0.123875 -0.276399 -0.025711  0.131202 -0.241792 -0.199316  0.421149  \n",
              "26  -0.617112  0.081694  0.152925  0.577330 -0.236271  0.359860 -0.102015  \n",
              "27   0.043695  0.375759 -0.444285 -0.061797 -0.089582  0.353245  0.195695  \n",
              "28  -0.026540 -0.734552 -0.085750  0.233943  0.267288 -0.001521 -0.148095  \n",
              "29   0.003469  0.211246 -0.050157 -0.243178 -0.044628 -0.232548  0.123531  \n",
              "30  -0.031518 -0.092311 -0.005899 -0.167378 -0.021581 -0.029826  0.045102  \n",
              "31   0.297099  0.210283 -0.015438  0.036708  0.027371 -0.048235 -0.004286  \n",
              "32  -0.461923  0.331812  0.770746  0.163420 -0.019127 -0.252521 -0.049199  \n",
              "33  -0.069510 -0.150570  0.760613 -0.457456  0.070412  0.253050  0.056535  \n",
              "34   0.031468 -0.266153 -0.694212 -0.289440 -0.311843 -0.538771 -0.691837  \n",
              "35   0.256694 -0.171526 -0.060834 -0.055895  0.610520  0.190081  0.474864  \n",
              "36  -0.153118  0.229762 -0.336681 -0.174751 -0.145790 -0.077123 -0.031268  \n",
              "37  -0.209320  0.143830 -0.222067 -0.031825  0.029729 -0.219203  0.229987  \n",
              "38   0.011564 -0.384079  0.229289  0.537207  0.325409  0.063368 -0.067494  \n",
              "39   0.091703  0.653900  0.250344  0.070657 -0.119526  0.203055  0.214429  \n",
              "40   0.025165 -0.049932 -0.217872 -0.031176 -0.024338  0.110588  0.138984  \n",
              "41   0.166779  0.092546  0.053881  0.066068  0.101522 -0.002252 -0.021419  \n",
              "42   0.214214 -0.064412 -0.084870 -0.137591 -0.312211 -0.063587 -0.424010  \n",
              "43  -0.163489 -0.039891  0.104706 -0.143588  0.139484  0.206617 -0.264348  \n",
              "44  -0.106226 -0.009535  0.056568  0.220914 -0.054410 -0.047803  0.037969  \n",
              "45   0.199907 -0.478144 -0.037040 -0.182755 -0.192339  0.563207  0.281141  \n",
              "46  -0.505823  0.485294 -0.038447 -0.299393  0.173660 -0.024129  0.423081  \n",
              "47  -0.115331 -0.108227  0.118972  0.655574  0.393246 -0.494485 -0.200391  \n",
              "48   0.060690  0.245007 -0.073494  0.235477  0.034611 -0.274523 -0.165675  \n",
              "49  -0.080815  0.011090 -0.314370  0.443712 -0.031642  0.199583  0.252927  \n",
              "50   0.052095  0.141204 -0.017638 -0.106238 -0.123697  0.018175 -0.054416  \n",
              "51  -0.288552  0.168954 -0.066721  0.245986 -0.292556  0.174451 -0.070376  \n",
              "52   0.008004 -0.063999 -0.012369 -0.196740 -0.075268 -0.077160  0.261774  \n",
              "53   0.027421 -0.114554  0.227101  0.036693 -0.046127 -0.009272  0.066155  \n",
              "54  -0.002179 -0.120210  0.042824  0.078140  0.043671  0.092983 -0.159219  \n",
              "55  -0.011457 -0.042707 -0.166679  0.146619  0.150483 -0.066519 -0.057752  \n",
              "56   0.372679 -0.190531  0.167976 -0.521730  0.268769 -0.398607 -0.239639  \n",
              "57  -0.183057 -0.032005  0.013153 -0.105118 -0.073079 -0.020510  0.038878  \n",
              "58   0.128830 -0.107514  0.196250 -0.043452 -0.117866 -0.042734 -0.080784  \n",
              "59  -0.005828  0.104785 -0.066439  0.119514  0.075421  0.109283  0.074889  \n",
              "60  -0.111527  0.031674 -0.120852  0.078288 -0.016775 -0.012838 -0.053098  \n",
              "61   0.041456  0.036796 -0.044589 -0.072414 -0.020983  0.005741  0.027594  \n",
              "62   0.089650 -0.011344 -0.009296  0.033283  0.068578 -0.028233 -0.058885  \n",
              "63  -0.159610 -0.152775  0.171693 -0.169127 -0.140831  0.088552 -0.041704  \n",
              "64   0.218851 -0.195193  0.026675  0.176037 -0.229319  0.020322  0.130651  \n",
              "65   0.006196  0.161780 -0.086680 -0.075419  0.077780 -0.214159  0.096250  \n",
              "66  -0.028816  0.073597  0.157496 -0.049969 -0.048229 -0.082662 -0.055572  \n",
              "67   0.018249 -0.234405 -0.110210  0.099439  0.064447  0.051171  0.056336  \n",
              "68   0.037131 -0.018997 -0.022160  0.122206  0.028687 -0.027614 -0.237752  \n",
              "69   0.061621 -0.066679 -0.065888  0.037886  0.006464 -0.016874  0.071045  \n",
              "70  -0.005980  0.341205  0.016755  0.064121  0.201041 -0.241320  0.066352  \n",
              "71   0.211161 -0.052772  0.271024 -0.101074  0.169923  0.421019 -0.266788  \n",
              "72   0.045769  0.070229 -0.217464  0.153758  0.099874 -0.002688 -0.105864  \n",
              "73  -0.064719  0.069855  0.017468 -0.078100 -0.005051 -0.521922  0.310096  \n",
              "74  -0.161144  0.273438 -0.351178  0.091363  0.147706  0.012927  0.236450  \n",
              "75  -0.193970 -0.187693  0.193393 -0.084965 -0.243552  0.255849 -0.150146  \n",
              "76   0.106577 -0.048935  0.053032 -0.120631 -0.076274  0.216099 -0.193066  \n",
              "77  -0.096333 -0.360521 -0.636963  0.182494  0.116272  0.145837 -0.011910  \n",
              "78  -0.017133  0.093527  0.108013 -0.018965  0.009624 -0.072017 -0.212278  \n",
              "79  -0.080124 -0.420673  0.275791 -0.236614 -0.043578 -0.221336  0.329065  \n",
              "80   0.138923  0.454713 -0.014073  0.436484  0.061091  0.088260 -0.462703  \n",
              "81   0.104405  0.354576 -0.015021  0.062562 -0.013381  0.046793 -0.040719  \n",
              "82  -0.176736 -0.334725  0.028267 -0.065541 -0.124133  0.011299  0.184846  \n",
              "83   0.080484  0.013867  0.082405 -0.019154  0.007168 -0.002415 -0.011750  \n",
              "84   0.004402 -0.130449 -0.039883 -0.035877 -0.000887  0.034442  0.006651  \n",
              "85   0.269362 -0.101066 -0.071562  0.062494 -0.243757  0.005644  0.111357  \n",
              "86   0.114788 -0.153699  0.113997  0.306465  0.153019  0.296461  0.578454  \n",
              "87  -0.221337 -0.022835 -0.319404  0.166073 -0.512772 -0.361424 -0.032517  \n",
              "88  -0.336581 -0.136353 -0.042791  0.136589  0.500420 -0.004391 -0.130279  \n",
              "89  -0.323477 -0.020121  0.130904  0.147145  0.290111 -0.048614 -0.078692  \n",
              "90   0.041773 -0.095950 -0.080089 -0.320160 -0.034541  0.005961  0.263597  \n",
              "91   0.053073 -0.002879 -0.119720  0.188424 -0.140838  0.016097  0.267991  \n",
              "92  -0.017577 -0.059603 -0.027986  0.102249 -0.080871  0.056780  0.008198  \n",
              "93   0.588687 -0.184511 -0.089425  0.280229 -0.663489 -0.166260  0.181005  \n",
              "94  -0.591578  0.095841  0.198789 -0.336367  0.104396  0.270823 -0.588938  \n",
              "95   0.301686  0.072675  0.271945 -0.248259  0.250986 -0.292536 -0.092558  \n",
              "96  -0.263594  0.213383 -0.165381 -0.518336  0.160600  0.145911  0.233921  \n",
              "97   0.169078  0.432112  0.094936  0.095585 -0.312528  0.147194 -0.202735  \n",
              "98   0.169195  0.262094 -0.117901 -0.164800 -0.069491 -0.050880 -0.021687  \n",
              "99  -0.361274 -0.143445 -0.003529  0.029297  0.092871  0.046724  0.017889  \n",
              "100  0.054052 -0.149576  0.054327 -0.085206 -0.020826 -0.022760 -0.051240  \n",
              "101  0.093020 -0.086098  0.027652  0.021020  0.002824 -0.078523 -0.032950  \n",
              "102 -0.005687  0.141986 -0.258837  0.109927 -0.115133 -0.061322 -0.033643  \n",
              "103  0.191918  0.126068  0.287124 -0.040200  0.194610 -0.116119  0.169397  \n",
              "104 -0.053178  0.104861  0.008137 -0.253575 -0.041771 -0.019096  0.024908  \n",
              "105  0.119265  0.025462  0.260401  0.209111 -0.092553  0.108953 -0.142083  \n",
              "\n",
              "[106 rows x 94 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c00092b0-ab7b-4de3-9614-ceacbc582024\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-2.637099</td>\n",
              "      <td>-0.347802</td>\n",
              "      <td>-1.305130</td>\n",
              "      <td>0.477352</td>\n",
              "      <td>0.222379</td>\n",
              "      <td>2.597939</td>\n",
              "      <td>1.865067</td>\n",
              "      <td>-0.494544</td>\n",
              "      <td>0.243539</td>\n",
              "      <td>0.256078</td>\n",
              "      <td>...</td>\n",
              "      <td>0.245732</td>\n",
              "      <td>0.019818</td>\n",
              "      <td>-0.246347</td>\n",
              "      <td>-0.334189</td>\n",
              "      <td>0.001933</td>\n",
              "      <td>0.099690</td>\n",
              "      <td>0.005821</td>\n",
              "      <td>0.098615</td>\n",
              "      <td>0.322466</td>\n",
              "      <td>-0.026514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-2.798193</td>\n",
              "      <td>3.634745</td>\n",
              "      <td>2.086230</td>\n",
              "      <td>3.196251</td>\n",
              "      <td>1.620699</td>\n",
              "      <td>-0.670871</td>\n",
              "      <td>1.726479</td>\n",
              "      <td>-0.083495</td>\n",
              "      <td>0.892471</td>\n",
              "      <td>0.753701</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.439034</td>\n",
              "      <td>-0.363662</td>\n",
              "      <td>0.125037</td>\n",
              "      <td>-0.009941</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.598846</td>\n",
              "      <td>0.399056</td>\n",
              "      <td>-0.448141</td>\n",
              "      <td>-0.133951</td>\n",
              "      <td>0.163589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.739392</td>\n",
              "      <td>-1.241732</td>\n",
              "      <td>-1.321686</td>\n",
              "      <td>-1.510875</td>\n",
              "      <td>-0.779627</td>\n",
              "      <td>-1.543128</td>\n",
              "      <td>1.477094</td>\n",
              "      <td>-1.396603</td>\n",
              "      <td>0.295762</td>\n",
              "      <td>-0.055510</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.171100</td>\n",
              "      <td>0.180284</td>\n",
              "      <td>0.044292</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.003606</td>\n",
              "      <td>-0.028537</td>\n",
              "      <td>0.019995</td>\n",
              "      <td>0.014549</td>\n",
              "      <td>-0.207424</td>\n",
              "      <td>-0.134106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.994942</td>\n",
              "      <td>1.074568</td>\n",
              "      <td>1.187057</td>\n",
              "      <td>1.138465</td>\n",
              "      <td>-0.079325</td>\n",
              "      <td>-4.044148</td>\n",
              "      <td>1.202367</td>\n",
              "      <td>-0.249646</td>\n",
              "      <td>-0.755334</td>\n",
              "      <td>1.255697</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026375</td>\n",
              "      <td>0.007201</td>\n",
              "      <td>-0.165179</td>\n",
              "      <td>-0.136537</td>\n",
              "      <td>0.035306</td>\n",
              "      <td>-0.042689</td>\n",
              "      <td>-0.063572</td>\n",
              "      <td>-0.160413</td>\n",
              "      <td>0.073046</td>\n",
              "      <td>-0.025301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.329136</td>\n",
              "      <td>2.676327</td>\n",
              "      <td>0.291800</td>\n",
              "      <td>0.271295</td>\n",
              "      <td>-0.906031</td>\n",
              "      <td>-1.966462</td>\n",
              "      <td>1.868663</td>\n",
              "      <td>-1.244299</td>\n",
              "      <td>-0.616884</td>\n",
              "      <td>-0.451323</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.145272</td>\n",
              "      <td>0.047353</td>\n",
              "      <td>-0.150540</td>\n",
              "      <td>-0.009131</td>\n",
              "      <td>-0.106808</td>\n",
              "      <td>-0.137046</td>\n",
              "      <td>-0.175852</td>\n",
              "      <td>0.152502</td>\n",
              "      <td>0.045290</td>\n",
              "      <td>-0.086046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-1.131488</td>\n",
              "      <td>0.223700</td>\n",
              "      <td>0.368406</td>\n",
              "      <td>-1.841575</td>\n",
              "      <td>-0.366648</td>\n",
              "      <td>-3.507539</td>\n",
              "      <td>0.586460</td>\n",
              "      <td>-1.019767</td>\n",
              "      <td>-0.007901</td>\n",
              "      <td>-0.290260</td>\n",
              "      <td>...</td>\n",
              "      <td>0.189601</td>\n",
              "      <td>-0.115965</td>\n",
              "      <td>-0.133011</td>\n",
              "      <td>-0.063879</td>\n",
              "      <td>0.143104</td>\n",
              "      <td>0.020425</td>\n",
              "      <td>-0.124293</td>\n",
              "      <td>-0.026170</td>\n",
              "      <td>0.228428</td>\n",
              "      <td>-0.088222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-1.042963</td>\n",
              "      <td>0.784338</td>\n",
              "      <td>0.155310</td>\n",
              "      <td>2.144182</td>\n",
              "      <td>-0.328153</td>\n",
              "      <td>-3.037030</td>\n",
              "      <td>2.740792</td>\n",
              "      <td>0.366178</td>\n",
              "      <td>-0.238992</td>\n",
              "      <td>0.099877</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050243</td>\n",
              "      <td>0.100481</td>\n",
              "      <td>0.019779</td>\n",
              "      <td>0.102530</td>\n",
              "      <td>0.022082</td>\n",
              "      <td>0.114263</td>\n",
              "      <td>-0.260516</td>\n",
              "      <td>0.086962</td>\n",
              "      <td>0.043728</td>\n",
              "      <td>0.082299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-2.123212</td>\n",
              "      <td>2.594353</td>\n",
              "      <td>0.784899</td>\n",
              "      <td>2.561225</td>\n",
              "      <td>0.298017</td>\n",
              "      <td>-1.477255</td>\n",
              "      <td>2.404881</td>\n",
              "      <td>-0.012778</td>\n",
              "      <td>0.333973</td>\n",
              "      <td>-0.174705</td>\n",
              "      <td>...</td>\n",
              "      <td>0.282903</td>\n",
              "      <td>0.042752</td>\n",
              "      <td>-0.064071</td>\n",
              "      <td>0.104548</td>\n",
              "      <td>0.054169</td>\n",
              "      <td>-0.324595</td>\n",
              "      <td>0.354240</td>\n",
              "      <td>0.060420</td>\n",
              "      <td>0.102943</td>\n",
              "      <td>-0.161668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-2.428108</td>\n",
              "      <td>0.587128</td>\n",
              "      <td>0.336312</td>\n",
              "      <td>1.760723</td>\n",
              "      <td>-0.277718</td>\n",
              "      <td>-1.053064</td>\n",
              "      <td>2.272779</td>\n",
              "      <td>0.317180</td>\n",
              "      <td>-1.332220</td>\n",
              "      <td>0.513478</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.087155</td>\n",
              "      <td>-0.092904</td>\n",
              "      <td>0.226356</td>\n",
              "      <td>-0.181613</td>\n",
              "      <td>-0.083627</td>\n",
              "      <td>0.256577</td>\n",
              "      <td>0.209390</td>\n",
              "      <td>-0.140765</td>\n",
              "      <td>-0.330571</td>\n",
              "      <td>-0.136723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-1.996928</td>\n",
              "      <td>0.378025</td>\n",
              "      <td>1.192909</td>\n",
              "      <td>1.650560</td>\n",
              "      <td>0.339481</td>\n",
              "      <td>-1.626476</td>\n",
              "      <td>1.924355</td>\n",
              "      <td>0.396526</td>\n",
              "      <td>-0.691455</td>\n",
              "      <td>0.126106</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.414637</td>\n",
              "      <td>0.052247</td>\n",
              "      <td>0.295735</td>\n",
              "      <td>0.499863</td>\n",
              "      <td>-0.019423</td>\n",
              "      <td>0.048918</td>\n",
              "      <td>0.008529</td>\n",
              "      <td>0.233998</td>\n",
              "      <td>0.220361</td>\n",
              "      <td>0.240518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-2.397497</td>\n",
              "      <td>2.225219</td>\n",
              "      <td>1.587146</td>\n",
              "      <td>3.089433</td>\n",
              "      <td>0.767364</td>\n",
              "      <td>-2.262547</td>\n",
              "      <td>2.458541</td>\n",
              "      <td>0.782024</td>\n",
              "      <td>0.100698</td>\n",
              "      <td>0.812775</td>\n",
              "      <td>...</td>\n",
              "      <td>0.231648</td>\n",
              "      <td>0.255910</td>\n",
              "      <td>-0.032445</td>\n",
              "      <td>-0.373376</td>\n",
              "      <td>-0.155772</td>\n",
              "      <td>-0.378488</td>\n",
              "      <td>-0.422746</td>\n",
              "      <td>-0.007195</td>\n",
              "      <td>-0.081527</td>\n",
              "      <td>0.064764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>-3.121665</td>\n",
              "      <td>3.285395</td>\n",
              "      <td>2.648926</td>\n",
              "      <td>2.631635</td>\n",
              "      <td>2.515994</td>\n",
              "      <td>1.229619</td>\n",
              "      <td>0.031375</td>\n",
              "      <td>-0.082745</td>\n",
              "      <td>1.461712</td>\n",
              "      <td>0.656578</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247150</td>\n",
              "      <td>-0.025595</td>\n",
              "      <td>0.045100</td>\n",
              "      <td>0.256206</td>\n",
              "      <td>0.089645</td>\n",
              "      <td>-0.136949</td>\n",
              "      <td>-0.139985</td>\n",
              "      <td>0.367463</td>\n",
              "      <td>0.123369</td>\n",
              "      <td>-0.021251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>-3.878436</td>\n",
              "      <td>-1.077783</td>\n",
              "      <td>1.972808</td>\n",
              "      <td>1.255909</td>\n",
              "      <td>4.699007</td>\n",
              "      <td>1.939041</td>\n",
              "      <td>-0.401190</td>\n",
              "      <td>-1.359237</td>\n",
              "      <td>1.269679</td>\n",
              "      <td>-1.087073</td>\n",
              "      <td>...</td>\n",
              "      <td>0.022675</td>\n",
              "      <td>0.144465</td>\n",
              "      <td>-0.136204</td>\n",
              "      <td>-0.659289</td>\n",
              "      <td>-0.348931</td>\n",
              "      <td>0.062967</td>\n",
              "      <td>-0.088760</td>\n",
              "      <td>-0.275793</td>\n",
              "      <td>-0.002524</td>\n",
              "      <td>0.053070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>-4.014367</td>\n",
              "      <td>-1.252404</td>\n",
              "      <td>1.606471</td>\n",
              "      <td>0.522180</td>\n",
              "      <td>4.165790</td>\n",
              "      <td>1.218129</td>\n",
              "      <td>0.170922</td>\n",
              "      <td>-1.910882</td>\n",
              "      <td>0.101441</td>\n",
              "      <td>-0.647171</td>\n",
              "      <td>...</td>\n",
              "      <td>0.023379</td>\n",
              "      <td>-0.080196</td>\n",
              "      <td>0.042130</td>\n",
              "      <td>-0.058174</td>\n",
              "      <td>0.102286</td>\n",
              "      <td>-0.200376</td>\n",
              "      <td>0.157605</td>\n",
              "      <td>0.767559</td>\n",
              "      <td>-0.343533</td>\n",
              "      <td>-0.128184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-3.816117</td>\n",
              "      <td>-1.324373</td>\n",
              "      <td>1.602825</td>\n",
              "      <td>0.552906</td>\n",
              "      <td>4.097568</td>\n",
              "      <td>0.830133</td>\n",
              "      <td>-0.096257</td>\n",
              "      <td>-1.736858</td>\n",
              "      <td>0.080458</td>\n",
              "      <td>-0.714832</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.397660</td>\n",
              "      <td>0.245568</td>\n",
              "      <td>0.147820</td>\n",
              "      <td>0.036492</td>\n",
              "      <td>-0.018615</td>\n",
              "      <td>0.022910</td>\n",
              "      <td>0.148818</td>\n",
              "      <td>-0.153438</td>\n",
              "      <td>-0.144382</td>\n",
              "      <td>0.505064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-3.689973</td>\n",
              "      <td>-2.155047</td>\n",
              "      <td>0.552009</td>\n",
              "      <td>0.078458</td>\n",
              "      <td>2.731397</td>\n",
              "      <td>0.792978</td>\n",
              "      <td>0.293722</td>\n",
              "      <td>-0.278633</td>\n",
              "      <td>-1.153374</td>\n",
              "      <td>-0.412010</td>\n",
              "      <td>...</td>\n",
              "      <td>0.343332</td>\n",
              "      <td>0.107223</td>\n",
              "      <td>-0.347163</td>\n",
              "      <td>-0.242998</td>\n",
              "      <td>-0.231267</td>\n",
              "      <td>-0.143570</td>\n",
              "      <td>-0.391664</td>\n",
              "      <td>-0.443641</td>\n",
              "      <td>0.215185</td>\n",
              "      <td>-0.191292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>-3.664803</td>\n",
              "      <td>-2.112624</td>\n",
              "      <td>0.709510</td>\n",
              "      <td>0.316549</td>\n",
              "      <td>3.421074</td>\n",
              "      <td>0.577376</td>\n",
              "      <td>-0.058770</td>\n",
              "      <td>-1.129333</td>\n",
              "      <td>-0.513825</td>\n",
              "      <td>-0.480888</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.062118</td>\n",
              "      <td>-0.189135</td>\n",
              "      <td>0.239185</td>\n",
              "      <td>0.417584</td>\n",
              "      <td>-0.102288</td>\n",
              "      <td>-0.046368</td>\n",
              "      <td>0.014686</td>\n",
              "      <td>0.194612</td>\n",
              "      <td>0.031704</td>\n",
              "      <td>-0.161622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>-3.472888</td>\n",
              "      <td>-2.414408</td>\n",
              "      <td>1.069169</td>\n",
              "      <td>0.544240</td>\n",
              "      <td>3.510205</td>\n",
              "      <td>1.037111</td>\n",
              "      <td>-0.485694</td>\n",
              "      <td>-0.891980</td>\n",
              "      <td>-0.098563</td>\n",
              "      <td>-0.805079</td>\n",
              "      <td>...</td>\n",
              "      <td>0.206288</td>\n",
              "      <td>-0.319841</td>\n",
              "      <td>-0.010459</td>\n",
              "      <td>0.574983</td>\n",
              "      <td>0.571460</td>\n",
              "      <td>0.053335</td>\n",
              "      <td>-0.015459</td>\n",
              "      <td>-0.250182</td>\n",
              "      <td>0.438432</td>\n",
              "      <td>-0.287490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-0.154480</td>\n",
              "      <td>-1.546400</td>\n",
              "      <td>-0.879493</td>\n",
              "      <td>1.928970</td>\n",
              "      <td>-1.528069</td>\n",
              "      <td>1.393321</td>\n",
              "      <td>0.603145</td>\n",
              "      <td>0.218529</td>\n",
              "      <td>0.584960</td>\n",
              "      <td>-0.553539</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.110231</td>\n",
              "      <td>-0.054468</td>\n",
              "      <td>-0.567831</td>\n",
              "      <td>0.406893</td>\n",
              "      <td>-0.399969</td>\n",
              "      <td>0.101343</td>\n",
              "      <td>0.014383</td>\n",
              "      <td>0.095134</td>\n",
              "      <td>0.098185</td>\n",
              "      <td>-0.259110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>-0.429591</td>\n",
              "      <td>-1.061395</td>\n",
              "      <td>-1.073781</td>\n",
              "      <td>1.603877</td>\n",
              "      <td>-1.000881</td>\n",
              "      <td>0.137796</td>\n",
              "      <td>-0.474644</td>\n",
              "      <td>-0.926504</td>\n",
              "      <td>-0.011670</td>\n",
              "      <td>-1.288752</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.535468</td>\n",
              "      <td>-0.262623</td>\n",
              "      <td>-0.325703</td>\n",
              "      <td>0.316703</td>\n",
              "      <td>-0.196790</td>\n",
              "      <td>0.313602</td>\n",
              "      <td>-0.153492</td>\n",
              "      <td>-0.250178</td>\n",
              "      <td>-0.260607</td>\n",
              "      <td>-0.131871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1.169234</td>\n",
              "      <td>0.563495</td>\n",
              "      <td>-0.556821</td>\n",
              "      <td>2.166775</td>\n",
              "      <td>-1.110425</td>\n",
              "      <td>-1.226026</td>\n",
              "      <td>-0.370970</td>\n",
              "      <td>-1.031656</td>\n",
              "      <td>-0.418541</td>\n",
              "      <td>-1.876619</td>\n",
              "      <td>...</td>\n",
              "      <td>0.189869</td>\n",
              "      <td>0.032763</td>\n",
              "      <td>0.228721</td>\n",
              "      <td>0.101168</td>\n",
              "      <td>0.307811</td>\n",
              "      <td>-0.011972</td>\n",
              "      <td>-0.280188</td>\n",
              "      <td>0.119275</td>\n",
              "      <td>-0.074072</td>\n",
              "      <td>0.044002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1.997442</td>\n",
              "      <td>-0.566104</td>\n",
              "      <td>-2.878561</td>\n",
              "      <td>2.048987</td>\n",
              "      <td>-2.044952</td>\n",
              "      <td>1.675383</td>\n",
              "      <td>-0.844437</td>\n",
              "      <td>-0.803123</td>\n",
              "      <td>0.805781</td>\n",
              "      <td>-0.070944</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.094073</td>\n",
              "      <td>-0.009146</td>\n",
              "      <td>-0.022008</td>\n",
              "      <td>-0.162247</td>\n",
              "      <td>0.004115</td>\n",
              "      <td>-0.031810</td>\n",
              "      <td>0.047193</td>\n",
              "      <td>0.030495</td>\n",
              "      <td>-0.001331</td>\n",
              "      <td>0.132705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.983302</td>\n",
              "      <td>-1.229718</td>\n",
              "      <td>-1.498552</td>\n",
              "      <td>1.839532</td>\n",
              "      <td>-1.240514</td>\n",
              "      <td>0.511564</td>\n",
              "      <td>-0.776103</td>\n",
              "      <td>-1.325901</td>\n",
              "      <td>-0.012663</td>\n",
              "      <td>-0.640896</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.100999</td>\n",
              "      <td>0.351998</td>\n",
              "      <td>-0.218062</td>\n",
              "      <td>0.053933</td>\n",
              "      <td>0.120976</td>\n",
              "      <td>-0.339442</td>\n",
              "      <td>0.033228</td>\n",
              "      <td>0.098141</td>\n",
              "      <td>0.400648</td>\n",
              "      <td>0.026211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1.557271</td>\n",
              "      <td>0.050281</td>\n",
              "      <td>-1.899174</td>\n",
              "      <td>1.712817</td>\n",
              "      <td>-0.925847</td>\n",
              "      <td>-1.272008</td>\n",
              "      <td>-0.870842</td>\n",
              "      <td>-1.380464</td>\n",
              "      <td>-0.368071</td>\n",
              "      <td>-2.021720</td>\n",
              "      <td>...</td>\n",
              "      <td>0.073958</td>\n",
              "      <td>-0.053034</td>\n",
              "      <td>-0.191372</td>\n",
              "      <td>0.126416</td>\n",
              "      <td>-0.114568</td>\n",
              "      <td>0.063703</td>\n",
              "      <td>-0.147723</td>\n",
              "      <td>0.231087</td>\n",
              "      <td>0.267255</td>\n",
              "      <td>-0.236652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1.250082</td>\n",
              "      <td>-0.475840</td>\n",
              "      <td>-1.731295</td>\n",
              "      <td>1.065621</td>\n",
              "      <td>-1.003865</td>\n",
              "      <td>-0.619206</td>\n",
              "      <td>0.185522</td>\n",
              "      <td>-1.619284</td>\n",
              "      <td>-0.016145</td>\n",
              "      <td>-1.919809</td>\n",
              "      <td>...</td>\n",
              "      <td>0.279233</td>\n",
              "      <td>0.181656</td>\n",
              "      <td>-0.061430</td>\n",
              "      <td>0.089911</td>\n",
              "      <td>0.253002</td>\n",
              "      <td>0.137546</td>\n",
              "      <td>-0.196125</td>\n",
              "      <td>-0.049718</td>\n",
              "      <td>-0.640881</td>\n",
              "      <td>0.196521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.517654</td>\n",
              "      <td>-0.726796</td>\n",
              "      <td>-0.476584</td>\n",
              "      <td>0.659845</td>\n",
              "      <td>-0.101265</td>\n",
              "      <td>-0.393997</td>\n",
              "      <td>-0.535044</td>\n",
              "      <td>-0.772317</td>\n",
              "      <td>-0.523049</td>\n",
              "      <td>-0.591549</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>-0.182640</td>\n",
              "      <td>-0.225018</td>\n",
              "      <td>0.123875</td>\n",
              "      <td>-0.276399</td>\n",
              "      <td>-0.025711</td>\n",
              "      <td>0.131202</td>\n",
              "      <td>-0.241792</td>\n",
              "      <td>-0.199316</td>\n",
              "      <td>0.421149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>-0.252770</td>\n",
              "      <td>-2.423221</td>\n",
              "      <td>-1.737167</td>\n",
              "      <td>1.730142</td>\n",
              "      <td>-0.774093</td>\n",
              "      <td>-0.778728</td>\n",
              "      <td>-0.803385</td>\n",
              "      <td>-1.292481</td>\n",
              "      <td>-0.229035</td>\n",
              "      <td>-1.442089</td>\n",
              "      <td>...</td>\n",
              "      <td>0.165914</td>\n",
              "      <td>0.036989</td>\n",
              "      <td>0.783753</td>\n",
              "      <td>-0.617112</td>\n",
              "      <td>0.081694</td>\n",
              "      <td>0.152925</td>\n",
              "      <td>0.577330</td>\n",
              "      <td>-0.236271</td>\n",
              "      <td>0.359860</td>\n",
              "      <td>-0.102015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1.164473</td>\n",
              "      <td>-1.224640</td>\n",
              "      <td>-0.914450</td>\n",
              "      <td>2.582735</td>\n",
              "      <td>-1.158372</td>\n",
              "      <td>-0.549832</td>\n",
              "      <td>-0.848184</td>\n",
              "      <td>-0.246047</td>\n",
              "      <td>-1.207821</td>\n",
              "      <td>-0.409443</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.609457</td>\n",
              "      <td>-0.247284</td>\n",
              "      <td>-0.058122</td>\n",
              "      <td>0.043695</td>\n",
              "      <td>0.375759</td>\n",
              "      <td>-0.444285</td>\n",
              "      <td>-0.061797</td>\n",
              "      <td>-0.089582</td>\n",
              "      <td>0.353245</td>\n",
              "      <td>0.195695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1.911768</td>\n",
              "      <td>-0.915630</td>\n",
              "      <td>-0.944989</td>\n",
              "      <td>2.965577</td>\n",
              "      <td>-1.213963</td>\n",
              "      <td>-0.835333</td>\n",
              "      <td>-1.340682</td>\n",
              "      <td>-0.109988</td>\n",
              "      <td>-0.707614</td>\n",
              "      <td>-0.181341</td>\n",
              "      <td>...</td>\n",
              "      <td>0.131895</td>\n",
              "      <td>0.306022</td>\n",
              "      <td>-0.058066</td>\n",
              "      <td>-0.026540</td>\n",
              "      <td>-0.734552</td>\n",
              "      <td>-0.085750</td>\n",
              "      <td>0.233943</td>\n",
              "      <td>0.267288</td>\n",
              "      <td>-0.001521</td>\n",
              "      <td>-0.148095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>-0.408548</td>\n",
              "      <td>-1.834815</td>\n",
              "      <td>-0.125575</td>\n",
              "      <td>3.054345</td>\n",
              "      <td>-0.877931</td>\n",
              "      <td>-0.548621</td>\n",
              "      <td>-0.374316</td>\n",
              "      <td>-0.581437</td>\n",
              "      <td>-0.293733</td>\n",
              "      <td>0.157777</td>\n",
              "      <td>...</td>\n",
              "      <td>0.567677</td>\n",
              "      <td>-0.090892</td>\n",
              "      <td>-0.030952</td>\n",
              "      <td>0.003469</td>\n",
              "      <td>0.211246</td>\n",
              "      <td>-0.050157</td>\n",
              "      <td>-0.243178</td>\n",
              "      <td>-0.044628</td>\n",
              "      <td>-0.232548</td>\n",
              "      <td>0.123531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1.467009</td>\n",
              "      <td>-1.673569</td>\n",
              "      <td>-0.998931</td>\n",
              "      <td>3.212867</td>\n",
              "      <td>-1.358372</td>\n",
              "      <td>-1.950890</td>\n",
              "      <td>-1.870762</td>\n",
              "      <td>-0.866857</td>\n",
              "      <td>2.623033</td>\n",
              "      <td>2.573387</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.035778</td>\n",
              "      <td>-0.083382</td>\n",
              "      <td>0.101628</td>\n",
              "      <td>-0.031518</td>\n",
              "      <td>-0.092311</td>\n",
              "      <td>-0.005899</td>\n",
              "      <td>-0.167378</td>\n",
              "      <td>-0.021581</td>\n",
              "      <td>-0.029826</td>\n",
              "      <td>0.045102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1.693128</td>\n",
              "      <td>-1.905936</td>\n",
              "      <td>-1.482452</td>\n",
              "      <td>2.580142</td>\n",
              "      <td>-0.631265</td>\n",
              "      <td>-1.065471</td>\n",
              "      <td>-0.645155</td>\n",
              "      <td>1.057783</td>\n",
              "      <td>1.651538</td>\n",
              "      <td>1.695190</td>\n",
              "      <td>...</td>\n",
              "      <td>0.070735</td>\n",
              "      <td>0.252007</td>\n",
              "      <td>-0.153929</td>\n",
              "      <td>0.297099</td>\n",
              "      <td>0.210283</td>\n",
              "      <td>-0.015438</td>\n",
              "      <td>0.036708</td>\n",
              "      <td>0.027371</td>\n",
              "      <td>-0.048235</td>\n",
              "      <td>-0.004286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.326083</td>\n",
              "      <td>-2.357328</td>\n",
              "      <td>-0.837244</td>\n",
              "      <td>0.915371</td>\n",
              "      <td>-0.114251</td>\n",
              "      <td>-0.320591</td>\n",
              "      <td>-0.543085</td>\n",
              "      <td>0.315936</td>\n",
              "      <td>-1.011740</td>\n",
              "      <td>0.073746</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.289944</td>\n",
              "      <td>0.309996</td>\n",
              "      <td>-0.038902</td>\n",
              "      <td>-0.461923</td>\n",
              "      <td>0.331812</td>\n",
              "      <td>0.770746</td>\n",
              "      <td>0.163420</td>\n",
              "      <td>-0.019127</td>\n",
              "      <td>-0.252521</td>\n",
              "      <td>-0.049199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>-3.097785</td>\n",
              "      <td>-1.540616</td>\n",
              "      <td>-1.078272</td>\n",
              "      <td>-1.299055</td>\n",
              "      <td>0.463412</td>\n",
              "      <td>1.260984</td>\n",
              "      <td>0.520225</td>\n",
              "      <td>1.642150</td>\n",
              "      <td>0.120016</td>\n",
              "      <td>0.015410</td>\n",
              "      <td>...</td>\n",
              "      <td>0.235966</td>\n",
              "      <td>-0.282299</td>\n",
              "      <td>0.452872</td>\n",
              "      <td>-0.069510</td>\n",
              "      <td>-0.150570</td>\n",
              "      <td>0.760613</td>\n",
              "      <td>-0.457456</td>\n",
              "      <td>0.070412</td>\n",
              "      <td>0.253050</td>\n",
              "      <td>0.056535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>-2.872596</td>\n",
              "      <td>-2.289240</td>\n",
              "      <td>-0.718798</td>\n",
              "      <td>-0.856928</td>\n",
              "      <td>0.755319</td>\n",
              "      <td>0.446068</td>\n",
              "      <td>0.125273</td>\n",
              "      <td>1.672759</td>\n",
              "      <td>-0.015177</td>\n",
              "      <td>0.025464</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.676102</td>\n",
              "      <td>0.357956</td>\n",
              "      <td>0.182697</td>\n",
              "      <td>0.031468</td>\n",
              "      <td>-0.266153</td>\n",
              "      <td>-0.694212</td>\n",
              "      <td>-0.289440</td>\n",
              "      <td>-0.311843</td>\n",
              "      <td>-0.538771</td>\n",
              "      <td>-0.691837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>-2.642872</td>\n",
              "      <td>-3.093603</td>\n",
              "      <td>-1.580577</td>\n",
              "      <td>-0.983180</td>\n",
              "      <td>0.529433</td>\n",
              "      <td>0.042502</td>\n",
              "      <td>0.219207</td>\n",
              "      <td>2.124160</td>\n",
              "      <td>0.120424</td>\n",
              "      <td>0.003614</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.268502</td>\n",
              "      <td>0.051885</td>\n",
              "      <td>0.148850</td>\n",
              "      <td>0.256694</td>\n",
              "      <td>-0.171526</td>\n",
              "      <td>-0.060834</td>\n",
              "      <td>-0.055895</td>\n",
              "      <td>0.610520</td>\n",
              "      <td>0.190081</td>\n",
              "      <td>0.474864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>-2.666563</td>\n",
              "      <td>-2.811451</td>\n",
              "      <td>-1.320080</td>\n",
              "      <td>-0.584977</td>\n",
              "      <td>0.549821</td>\n",
              "      <td>0.310147</td>\n",
              "      <td>0.132449</td>\n",
              "      <td>1.713566</td>\n",
              "      <td>-0.158619</td>\n",
              "      <td>-0.190742</td>\n",
              "      <td>...</td>\n",
              "      <td>0.088132</td>\n",
              "      <td>0.001901</td>\n",
              "      <td>-0.181751</td>\n",
              "      <td>-0.153118</td>\n",
              "      <td>0.229762</td>\n",
              "      <td>-0.336681</td>\n",
              "      <td>-0.174751</td>\n",
              "      <td>-0.145790</td>\n",
              "      <td>-0.077123</td>\n",
              "      <td>-0.031268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>-2.522127</td>\n",
              "      <td>-2.950680</td>\n",
              "      <td>-0.912822</td>\n",
              "      <td>-0.742229</td>\n",
              "      <td>0.636310</td>\n",
              "      <td>0.272814</td>\n",
              "      <td>-0.124880</td>\n",
              "      <td>1.836601</td>\n",
              "      <td>0.079334</td>\n",
              "      <td>-0.030521</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.040810</td>\n",
              "      <td>0.130532</td>\n",
              "      <td>-0.218607</td>\n",
              "      <td>-0.209320</td>\n",
              "      <td>0.143830</td>\n",
              "      <td>-0.222067</td>\n",
              "      <td>-0.031825</td>\n",
              "      <td>0.029729</td>\n",
              "      <td>-0.219203</td>\n",
              "      <td>0.229987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>-2.824209</td>\n",
              "      <td>-2.713716</td>\n",
              "      <td>-1.641778</td>\n",
              "      <td>-0.771857</td>\n",
              "      <td>0.380510</td>\n",
              "      <td>-0.132937</td>\n",
              "      <td>0.730788</td>\n",
              "      <td>2.014792</td>\n",
              "      <td>-0.374588</td>\n",
              "      <td>-0.057618</td>\n",
              "      <td>...</td>\n",
              "      <td>0.264797</td>\n",
              "      <td>-0.337104</td>\n",
              "      <td>-0.143719</td>\n",
              "      <td>0.011564</td>\n",
              "      <td>-0.384079</td>\n",
              "      <td>0.229289</td>\n",
              "      <td>0.537207</td>\n",
              "      <td>0.325409</td>\n",
              "      <td>0.063368</td>\n",
              "      <td>-0.067494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>-3.420457</td>\n",
              "      <td>-2.746807</td>\n",
              "      <td>-1.231707</td>\n",
              "      <td>-0.784039</td>\n",
              "      <td>1.139511</td>\n",
              "      <td>1.333265</td>\n",
              "      <td>0.306230</td>\n",
              "      <td>1.180762</td>\n",
              "      <td>-0.165200</td>\n",
              "      <td>0.027691</td>\n",
              "      <td>...</td>\n",
              "      <td>0.161658</td>\n",
              "      <td>0.307317</td>\n",
              "      <td>-0.141023</td>\n",
              "      <td>0.091703</td>\n",
              "      <td>0.653900</td>\n",
              "      <td>0.250344</td>\n",
              "      <td>0.070657</td>\n",
              "      <td>-0.119526</td>\n",
              "      <td>0.203055</td>\n",
              "      <td>0.214429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>-1.099295</td>\n",
              "      <td>0.465213</td>\n",
              "      <td>1.334637</td>\n",
              "      <td>-2.250615</td>\n",
              "      <td>-1.491715</td>\n",
              "      <td>-0.653282</td>\n",
              "      <td>-0.849536</td>\n",
              "      <td>-1.283735</td>\n",
              "      <td>1.205869</td>\n",
              "      <td>-0.368846</td>\n",
              "      <td>...</td>\n",
              "      <td>0.062673</td>\n",
              "      <td>-0.028989</td>\n",
              "      <td>-0.073108</td>\n",
              "      <td>0.025165</td>\n",
              "      <td>-0.049932</td>\n",
              "      <td>-0.217872</td>\n",
              "      <td>-0.031176</td>\n",
              "      <td>-0.024338</td>\n",
              "      <td>0.110588</td>\n",
              "      <td>0.138984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>-1.169595</td>\n",
              "      <td>-2.462802</td>\n",
              "      <td>1.489594</td>\n",
              "      <td>-3.346595</td>\n",
              "      <td>-0.699219</td>\n",
              "      <td>-2.433538</td>\n",
              "      <td>-1.578992</td>\n",
              "      <td>-1.184700</td>\n",
              "      <td>0.438590</td>\n",
              "      <td>0.760127</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029340</td>\n",
              "      <td>0.150340</td>\n",
              "      <td>0.012598</td>\n",
              "      <td>0.166779</td>\n",
              "      <td>0.092546</td>\n",
              "      <td>0.053881</td>\n",
              "      <td>0.066068</td>\n",
              "      <td>0.101522</td>\n",
              "      <td>-0.002252</td>\n",
              "      <td>-0.021419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>-0.188029</td>\n",
              "      <td>-2.039316</td>\n",
              "      <td>2.035923</td>\n",
              "      <td>-2.630550</td>\n",
              "      <td>0.002784</td>\n",
              "      <td>-0.922663</td>\n",
              "      <td>-1.055911</td>\n",
              "      <td>-0.040532</td>\n",
              "      <td>-0.674353</td>\n",
              "      <td>-0.101740</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018708</td>\n",
              "      <td>-0.172450</td>\n",
              "      <td>0.215015</td>\n",
              "      <td>0.214214</td>\n",
              "      <td>-0.064412</td>\n",
              "      <td>-0.084870</td>\n",
              "      <td>-0.137591</td>\n",
              "      <td>-0.312211</td>\n",
              "      <td>-0.063587</td>\n",
              "      <td>-0.424010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>-1.340439</td>\n",
              "      <td>-0.345850</td>\n",
              "      <td>3.167113</td>\n",
              "      <td>-2.315758</td>\n",
              "      <td>-0.589326</td>\n",
              "      <td>-1.100225</td>\n",
              "      <td>-1.353995</td>\n",
              "      <td>-0.349298</td>\n",
              "      <td>-0.162986</td>\n",
              "      <td>0.367785</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.405684</td>\n",
              "      <td>0.064107</td>\n",
              "      <td>-0.206222</td>\n",
              "      <td>-0.163489</td>\n",
              "      <td>-0.039891</td>\n",
              "      <td>0.104706</td>\n",
              "      <td>-0.143588</td>\n",
              "      <td>0.139484</td>\n",
              "      <td>0.206617</td>\n",
              "      <td>-0.264348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>-0.038033</td>\n",
              "      <td>-1.146338</td>\n",
              "      <td>0.056533</td>\n",
              "      <td>-1.408917</td>\n",
              "      <td>-0.829297</td>\n",
              "      <td>-2.161916</td>\n",
              "      <td>0.448988</td>\n",
              "      <td>0.335673</td>\n",
              "      <td>0.148819</td>\n",
              "      <td>-0.241603</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011225</td>\n",
              "      <td>0.003135</td>\n",
              "      <td>-0.235874</td>\n",
              "      <td>-0.106226</td>\n",
              "      <td>-0.009535</td>\n",
              "      <td>0.056568</td>\n",
              "      <td>0.220914</td>\n",
              "      <td>-0.054410</td>\n",
              "      <td>-0.047803</td>\n",
              "      <td>0.037969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>-0.402297</td>\n",
              "      <td>-2.237480</td>\n",
              "      <td>2.388917</td>\n",
              "      <td>-2.855490</td>\n",
              "      <td>0.062406</td>\n",
              "      <td>-0.589928</td>\n",
              "      <td>-0.938151</td>\n",
              "      <td>-0.651540</td>\n",
              "      <td>-0.756341</td>\n",
              "      <td>1.189794</td>\n",
              "      <td>...</td>\n",
              "      <td>0.257882</td>\n",
              "      <td>0.211339</td>\n",
              "      <td>-0.195754</td>\n",
              "      <td>0.199907</td>\n",
              "      <td>-0.478144</td>\n",
              "      <td>-0.037040</td>\n",
              "      <td>-0.182755</td>\n",
              "      <td>-0.192339</td>\n",
              "      <td>0.563207</td>\n",
              "      <td>0.281141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>-1.313723</td>\n",
              "      <td>-2.045631</td>\n",
              "      <td>1.158891</td>\n",
              "      <td>-2.354800</td>\n",
              "      <td>-0.732532</td>\n",
              "      <td>-1.073207</td>\n",
              "      <td>-0.775655</td>\n",
              "      <td>-0.032923</td>\n",
              "      <td>-0.081050</td>\n",
              "      <td>0.575991</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.084312</td>\n",
              "      <td>-0.188685</td>\n",
              "      <td>-0.034155</td>\n",
              "      <td>-0.505823</td>\n",
              "      <td>0.485294</td>\n",
              "      <td>-0.038447</td>\n",
              "      <td>-0.299393</td>\n",
              "      <td>0.173660</td>\n",
              "      <td>-0.024129</td>\n",
              "      <td>0.423081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>-1.048675</td>\n",
              "      <td>-1.715145</td>\n",
              "      <td>2.515703</td>\n",
              "      <td>-2.073400</td>\n",
              "      <td>-0.410095</td>\n",
              "      <td>-0.572684</td>\n",
              "      <td>-1.261211</td>\n",
              "      <td>-0.960969</td>\n",
              "      <td>-0.815623</td>\n",
              "      <td>0.911458</td>\n",
              "      <td>...</td>\n",
              "      <td>0.251077</td>\n",
              "      <td>-0.076883</td>\n",
              "      <td>0.164421</td>\n",
              "      <td>-0.115331</td>\n",
              "      <td>-0.108227</td>\n",
              "      <td>0.118972</td>\n",
              "      <td>0.655574</td>\n",
              "      <td>0.393246</td>\n",
              "      <td>-0.494485</td>\n",
              "      <td>-0.200391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>-1.276280</td>\n",
              "      <td>0.422902</td>\n",
              "      <td>2.890507</td>\n",
              "      <td>-0.298685</td>\n",
              "      <td>-0.156793</td>\n",
              "      <td>0.877776</td>\n",
              "      <td>0.280889</td>\n",
              "      <td>0.045263</td>\n",
              "      <td>-1.205677</td>\n",
              "      <td>0.486009</td>\n",
              "      <td>...</td>\n",
              "      <td>0.146607</td>\n",
              "      <td>0.283305</td>\n",
              "      <td>-0.191963</td>\n",
              "      <td>0.060690</td>\n",
              "      <td>0.245007</td>\n",
              "      <td>-0.073494</td>\n",
              "      <td>0.235477</td>\n",
              "      <td>0.034611</td>\n",
              "      <td>-0.274523</td>\n",
              "      <td>-0.165675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>-1.675160</td>\n",
              "      <td>-0.223162</td>\n",
              "      <td>-0.959539</td>\n",
              "      <td>-1.191272</td>\n",
              "      <td>-1.148406</td>\n",
              "      <td>2.156334</td>\n",
              "      <td>2.685866</td>\n",
              "      <td>0.397253</td>\n",
              "      <td>0.193994</td>\n",
              "      <td>0.309965</td>\n",
              "      <td>...</td>\n",
              "      <td>0.185404</td>\n",
              "      <td>0.213859</td>\n",
              "      <td>-0.262786</td>\n",
              "      <td>-0.080815</td>\n",
              "      <td>0.011090</td>\n",
              "      <td>-0.314370</td>\n",
              "      <td>0.443712</td>\n",
              "      <td>-0.031642</td>\n",
              "      <td>0.199583</td>\n",
              "      <td>0.252927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>-0.901350</td>\n",
              "      <td>0.623569</td>\n",
              "      <td>-1.150486</td>\n",
              "      <td>-2.642692</td>\n",
              "      <td>-1.667718</td>\n",
              "      <td>-0.243350</td>\n",
              "      <td>1.203611</td>\n",
              "      <td>-0.993256</td>\n",
              "      <td>-0.761329</td>\n",
              "      <td>-0.080740</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.022582</td>\n",
              "      <td>-0.046744</td>\n",
              "      <td>-0.341623</td>\n",
              "      <td>0.052095</td>\n",
              "      <td>0.141204</td>\n",
              "      <td>-0.017638</td>\n",
              "      <td>-0.106238</td>\n",
              "      <td>-0.123697</td>\n",
              "      <td>0.018175</td>\n",
              "      <td>-0.054416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>-0.174547</td>\n",
              "      <td>0.067538</td>\n",
              "      <td>-1.066564</td>\n",
              "      <td>-1.551703</td>\n",
              "      <td>-1.708644</td>\n",
              "      <td>1.853259</td>\n",
              "      <td>2.516707</td>\n",
              "      <td>0.415582</td>\n",
              "      <td>-0.010141</td>\n",
              "      <td>-0.189395</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.240947</td>\n",
              "      <td>-0.549458</td>\n",
              "      <td>-0.159876</td>\n",
              "      <td>-0.288552</td>\n",
              "      <td>0.168954</td>\n",
              "      <td>-0.066721</td>\n",
              "      <td>0.245986</td>\n",
              "      <td>-0.292556</td>\n",
              "      <td>0.174451</td>\n",
              "      <td>-0.070376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>-0.282104</td>\n",
              "      <td>-1.011302</td>\n",
              "      <td>-0.691323</td>\n",
              "      <td>-1.847908</td>\n",
              "      <td>-1.607443</td>\n",
              "      <td>-0.038352</td>\n",
              "      <td>1.813729</td>\n",
              "      <td>-0.338364</td>\n",
              "      <td>-0.535617</td>\n",
              "      <td>-0.215629</td>\n",
              "      <td>...</td>\n",
              "      <td>0.070796</td>\n",
              "      <td>-0.140182</td>\n",
              "      <td>0.468011</td>\n",
              "      <td>0.008004</td>\n",
              "      <td>-0.063999</td>\n",
              "      <td>-0.012369</td>\n",
              "      <td>-0.196740</td>\n",
              "      <td>-0.075268</td>\n",
              "      <td>-0.077160</td>\n",
              "      <td>0.261774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>-0.769889</td>\n",
              "      <td>-1.683174</td>\n",
              "      <td>-1.707398</td>\n",
              "      <td>-1.639610</td>\n",
              "      <td>-1.496469</td>\n",
              "      <td>-0.361207</td>\n",
              "      <td>0.058345</td>\n",
              "      <td>-2.671555</td>\n",
              "      <td>2.745443</td>\n",
              "      <td>0.888032</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.076348</td>\n",
              "      <td>-0.073837</td>\n",
              "      <td>0.271482</td>\n",
              "      <td>0.027421</td>\n",
              "      <td>-0.114554</td>\n",
              "      <td>0.227101</td>\n",
              "      <td>0.036693</td>\n",
              "      <td>-0.046127</td>\n",
              "      <td>-0.009272</td>\n",
              "      <td>0.066155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>-0.081574</td>\n",
              "      <td>-1.092008</td>\n",
              "      <td>-1.654655</td>\n",
              "      <td>-2.202721</td>\n",
              "      <td>-0.719607</td>\n",
              "      <td>-0.060184</td>\n",
              "      <td>1.042880</td>\n",
              "      <td>0.402992</td>\n",
              "      <td>0.703606</td>\n",
              "      <td>0.118129</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.059140</td>\n",
              "      <td>-0.044507</td>\n",
              "      <td>-0.247009</td>\n",
              "      <td>-0.002179</td>\n",
              "      <td>-0.120210</td>\n",
              "      <td>0.042824</td>\n",
              "      <td>0.078140</td>\n",
              "      <td>0.043671</td>\n",
              "      <td>0.092983</td>\n",
              "      <td>-0.159219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>-1.234132</td>\n",
              "      <td>-0.838562</td>\n",
              "      <td>-2.011426</td>\n",
              "      <td>-1.940953</td>\n",
              "      <td>-1.865559</td>\n",
              "      <td>0.548632</td>\n",
              "      <td>1.478590</td>\n",
              "      <td>-0.938380</td>\n",
              "      <td>0.188878</td>\n",
              "      <td>0.560593</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.038754</td>\n",
              "      <td>0.012199</td>\n",
              "      <td>0.164491</td>\n",
              "      <td>-0.011457</td>\n",
              "      <td>-0.042707</td>\n",
              "      <td>-0.166679</td>\n",
              "      <td>0.146619</td>\n",
              "      <td>0.150483</td>\n",
              "      <td>-0.066519</td>\n",
              "      <td>-0.057752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>-1.377481</td>\n",
              "      <td>-0.065721</td>\n",
              "      <td>-1.081459</td>\n",
              "      <td>-0.250382</td>\n",
              "      <td>-1.244762</td>\n",
              "      <td>3.282794</td>\n",
              "      <td>1.888986</td>\n",
              "      <td>-0.334953</td>\n",
              "      <td>-0.230487</td>\n",
              "      <td>0.198537</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.139545</td>\n",
              "      <td>0.036456</td>\n",
              "      <td>0.343505</td>\n",
              "      <td>0.372679</td>\n",
              "      <td>-0.190531</td>\n",
              "      <td>0.167976</td>\n",
              "      <td>-0.521730</td>\n",
              "      <td>0.268769</td>\n",
              "      <td>-0.398607</td>\n",
              "      <td>-0.239639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>-1.136689</td>\n",
              "      <td>5.804210</td>\n",
              "      <td>-2.158707</td>\n",
              "      <td>-0.711264</td>\n",
              "      <td>2.630094</td>\n",
              "      <td>-0.722201</td>\n",
              "      <td>-2.566602</td>\n",
              "      <td>0.452389</td>\n",
              "      <td>0.073112</td>\n",
              "      <td>-0.337740</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.145760</td>\n",
              "      <td>-0.150223</td>\n",
              "      <td>-0.006571</td>\n",
              "      <td>-0.183057</td>\n",
              "      <td>-0.032005</td>\n",
              "      <td>0.013153</td>\n",
              "      <td>-0.105118</td>\n",
              "      <td>-0.073079</td>\n",
              "      <td>-0.020510</td>\n",
              "      <td>0.038878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>-1.971310</td>\n",
              "      <td>4.386899</td>\n",
              "      <td>-4.043365</td>\n",
              "      <td>-1.361281</td>\n",
              "      <td>1.282604</td>\n",
              "      <td>-0.496142</td>\n",
              "      <td>-1.185623</td>\n",
              "      <td>0.547070</td>\n",
              "      <td>-0.444554</td>\n",
              "      <td>0.370865</td>\n",
              "      <td>...</td>\n",
              "      <td>0.111135</td>\n",
              "      <td>-0.015964</td>\n",
              "      <td>-0.099462</td>\n",
              "      <td>0.128830</td>\n",
              "      <td>-0.107514</td>\n",
              "      <td>0.196250</td>\n",
              "      <td>-0.043452</td>\n",
              "      <td>-0.117866</td>\n",
              "      <td>-0.042734</td>\n",
              "      <td>-0.080784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>-0.847435</td>\n",
              "      <td>4.087546</td>\n",
              "      <td>-3.075522</td>\n",
              "      <td>-1.403359</td>\n",
              "      <td>1.478408</td>\n",
              "      <td>-0.990298</td>\n",
              "      <td>-1.038978</td>\n",
              "      <td>0.255356</td>\n",
              "      <td>-1.686040</td>\n",
              "      <td>0.209921</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.023100</td>\n",
              "      <td>0.132558</td>\n",
              "      <td>0.012866</td>\n",
              "      <td>-0.005828</td>\n",
              "      <td>0.104785</td>\n",
              "      <td>-0.066439</td>\n",
              "      <td>0.119514</td>\n",
              "      <td>0.075421</td>\n",
              "      <td>0.109283</td>\n",
              "      <td>0.074889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>-2.034989</td>\n",
              "      <td>5.777314</td>\n",
              "      <td>-3.917526</td>\n",
              "      <td>-1.192115</td>\n",
              "      <td>0.948309</td>\n",
              "      <td>0.384854</td>\n",
              "      <td>-0.618132</td>\n",
              "      <td>0.268305</td>\n",
              "      <td>-0.687963</td>\n",
              "      <td>1.258583</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025951</td>\n",
              "      <td>-0.084966</td>\n",
              "      <td>0.155026</td>\n",
              "      <td>-0.111527</td>\n",
              "      <td>0.031674</td>\n",
              "      <td>-0.120852</td>\n",
              "      <td>0.078288</td>\n",
              "      <td>-0.016775</td>\n",
              "      <td>-0.012838</td>\n",
              "      <td>-0.053098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>-2.899686</td>\n",
              "      <td>3.653677</td>\n",
              "      <td>-4.107776</td>\n",
              "      <td>-0.837462</td>\n",
              "      <td>2.314684</td>\n",
              "      <td>-0.099674</td>\n",
              "      <td>-0.377653</td>\n",
              "      <td>-0.632654</td>\n",
              "      <td>0.191662</td>\n",
              "      <td>1.746729</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.031892</td>\n",
              "      <td>0.018460</td>\n",
              "      <td>0.062417</td>\n",
              "      <td>0.041456</td>\n",
              "      <td>0.036796</td>\n",
              "      <td>-0.044589</td>\n",
              "      <td>-0.072414</td>\n",
              "      <td>-0.020983</td>\n",
              "      <td>0.005741</td>\n",
              "      <td>0.027594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>-3.044610</td>\n",
              "      <td>4.717416</td>\n",
              "      <td>-2.692057</td>\n",
              "      <td>-0.515452</td>\n",
              "      <td>3.082248</td>\n",
              "      <td>-0.087261</td>\n",
              "      <td>-1.969000</td>\n",
              "      <td>0.516689</td>\n",
              "      <td>0.326533</td>\n",
              "      <td>0.505000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.106817</td>\n",
              "      <td>0.107516</td>\n",
              "      <td>-0.139848</td>\n",
              "      <td>0.089650</td>\n",
              "      <td>-0.011344</td>\n",
              "      <td>-0.009296</td>\n",
              "      <td>0.033283</td>\n",
              "      <td>0.068578</td>\n",
              "      <td>-0.028233</td>\n",
              "      <td>-0.058885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>7.514398</td>\n",
              "      <td>-0.635832</td>\n",
              "      <td>0.391142</td>\n",
              "      <td>-0.657821</td>\n",
              "      <td>1.896845</td>\n",
              "      <td>-0.123870</td>\n",
              "      <td>0.429798</td>\n",
              "      <td>0.674668</td>\n",
              "      <td>1.341630</td>\n",
              "      <td>0.361575</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.045240</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>-0.091696</td>\n",
              "      <td>-0.159610</td>\n",
              "      <td>-0.152775</td>\n",
              "      <td>0.171693</td>\n",
              "      <td>-0.169127</td>\n",
              "      <td>-0.140831</td>\n",
              "      <td>0.088552</td>\n",
              "      <td>-0.041704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>6.081498</td>\n",
              "      <td>0.184675</td>\n",
              "      <td>0.796057</td>\n",
              "      <td>0.446617</td>\n",
              "      <td>0.916233</td>\n",
              "      <td>0.107495</td>\n",
              "      <td>1.029045</td>\n",
              "      <td>0.884821</td>\n",
              "      <td>-0.497122</td>\n",
              "      <td>-0.093180</td>\n",
              "      <td>...</td>\n",
              "      <td>0.159137</td>\n",
              "      <td>0.025531</td>\n",
              "      <td>0.197970</td>\n",
              "      <td>0.218851</td>\n",
              "      <td>-0.195193</td>\n",
              "      <td>0.026675</td>\n",
              "      <td>0.176037</td>\n",
              "      <td>-0.229319</td>\n",
              "      <td>0.020322</td>\n",
              "      <td>0.130651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>6.417571</td>\n",
              "      <td>-1.035532</td>\n",
              "      <td>0.593525</td>\n",
              "      <td>-1.207628</td>\n",
              "      <td>1.777169</td>\n",
              "      <td>-0.666690</td>\n",
              "      <td>0.538690</td>\n",
              "      <td>0.050725</td>\n",
              "      <td>0.978742</td>\n",
              "      <td>0.283329</td>\n",
              "      <td>...</td>\n",
              "      <td>0.064904</td>\n",
              "      <td>-0.130967</td>\n",
              "      <td>-0.095892</td>\n",
              "      <td>0.006196</td>\n",
              "      <td>0.161780</td>\n",
              "      <td>-0.086680</td>\n",
              "      <td>-0.075419</td>\n",
              "      <td>0.077780</td>\n",
              "      <td>-0.214159</td>\n",
              "      <td>0.096250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>4.853528</td>\n",
              "      <td>0.146181</td>\n",
              "      <td>0.280188</td>\n",
              "      <td>0.419659</td>\n",
              "      <td>-0.696790</td>\n",
              "      <td>1.923275</td>\n",
              "      <td>0.624137</td>\n",
              "      <td>-0.689111</td>\n",
              "      <td>-1.006972</td>\n",
              "      <td>1.283476</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.119253</td>\n",
              "      <td>-0.194984</td>\n",
              "      <td>-0.072416</td>\n",
              "      <td>-0.028816</td>\n",
              "      <td>0.073597</td>\n",
              "      <td>0.157496</td>\n",
              "      <td>-0.049969</td>\n",
              "      <td>-0.048229</td>\n",
              "      <td>-0.082662</td>\n",
              "      <td>-0.055572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>5.767001</td>\n",
              "      <td>-0.226511</td>\n",
              "      <td>0.799813</td>\n",
              "      <td>-0.561765</td>\n",
              "      <td>1.164284</td>\n",
              "      <td>0.518367</td>\n",
              "      <td>0.365658</td>\n",
              "      <td>-0.387542</td>\n",
              "      <td>-0.793323</td>\n",
              "      <td>0.647585</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029994</td>\n",
              "      <td>0.153425</td>\n",
              "      <td>0.528758</td>\n",
              "      <td>0.018249</td>\n",
              "      <td>-0.234405</td>\n",
              "      <td>-0.110210</td>\n",
              "      <td>0.099439</td>\n",
              "      <td>0.064447</td>\n",
              "      <td>0.051171</td>\n",
              "      <td>0.056336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>5.465487</td>\n",
              "      <td>-0.097514</td>\n",
              "      <td>0.327609</td>\n",
              "      <td>-0.011427</td>\n",
              "      <td>1.276314</td>\n",
              "      <td>0.224520</td>\n",
              "      <td>0.800988</td>\n",
              "      <td>-0.654831</td>\n",
              "      <td>-1.101378</td>\n",
              "      <td>-0.044630</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.431430</td>\n",
              "      <td>-0.058752</td>\n",
              "      <td>-0.145380</td>\n",
              "      <td>0.037131</td>\n",
              "      <td>-0.018997</td>\n",
              "      <td>-0.022160</td>\n",
              "      <td>0.122206</td>\n",
              "      <td>0.028687</td>\n",
              "      <td>-0.027614</td>\n",
              "      <td>-0.237752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>3.633630</td>\n",
              "      <td>6.278797</td>\n",
              "      <td>-2.002323</td>\n",
              "      <td>-1.931037</td>\n",
              "      <td>-0.796981</td>\n",
              "      <td>0.588673</td>\n",
              "      <td>-0.935282</td>\n",
              "      <td>-1.179236</td>\n",
              "      <td>-0.647395</td>\n",
              "      <td>-1.130020</td>\n",
              "      <td>...</td>\n",
              "      <td>0.033230</td>\n",
              "      <td>-0.016170</td>\n",
              "      <td>0.078437</td>\n",
              "      <td>0.061621</td>\n",
              "      <td>-0.066679</td>\n",
              "      <td>-0.065888</td>\n",
              "      <td>0.037886</td>\n",
              "      <td>0.006464</td>\n",
              "      <td>-0.016874</td>\n",
              "      <td>0.071045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>6.400256</td>\n",
              "      <td>0.244836</td>\n",
              "      <td>-0.896523</td>\n",
              "      <td>-0.428111</td>\n",
              "      <td>0.628118</td>\n",
              "      <td>1.700274</td>\n",
              "      <td>0.879492</td>\n",
              "      <td>-0.601516</td>\n",
              "      <td>0.319824</td>\n",
              "      <td>0.149369</td>\n",
              "      <td>...</td>\n",
              "      <td>0.032566</td>\n",
              "      <td>-0.169036</td>\n",
              "      <td>-0.155438</td>\n",
              "      <td>-0.005980</td>\n",
              "      <td>0.341205</td>\n",
              "      <td>0.016755</td>\n",
              "      <td>0.064121</td>\n",
              "      <td>0.201041</td>\n",
              "      <td>-0.241320</td>\n",
              "      <td>0.066352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>6.061324</td>\n",
              "      <td>-0.621618</td>\n",
              "      <td>-0.241531</td>\n",
              "      <td>-0.590321</td>\n",
              "      <td>1.098110</td>\n",
              "      <td>0.639856</td>\n",
              "      <td>0.818704</td>\n",
              "      <td>0.239169</td>\n",
              "      <td>-0.119760</td>\n",
              "      <td>-0.224642</td>\n",
              "      <td>...</td>\n",
              "      <td>0.306576</td>\n",
              "      <td>0.747988</td>\n",
              "      <td>0.281006</td>\n",
              "      <td>0.211161</td>\n",
              "      <td>-0.052772</td>\n",
              "      <td>0.271024</td>\n",
              "      <td>-0.101074</td>\n",
              "      <td>0.169923</td>\n",
              "      <td>0.421019</td>\n",
              "      <td>-0.266788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>9.025208</td>\n",
              "      <td>-0.402932</td>\n",
              "      <td>-0.213329</td>\n",
              "      <td>-1.001249</td>\n",
              "      <td>2.936993</td>\n",
              "      <td>-0.683901</td>\n",
              "      <td>0.119570</td>\n",
              "      <td>0.352377</td>\n",
              "      <td>1.310474</td>\n",
              "      <td>0.193839</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.425576</td>\n",
              "      <td>0.079482</td>\n",
              "      <td>0.219548</td>\n",
              "      <td>0.045769</td>\n",
              "      <td>0.070229</td>\n",
              "      <td>-0.217464</td>\n",
              "      <td>0.153758</td>\n",
              "      <td>0.099874</td>\n",
              "      <td>-0.002688</td>\n",
              "      <td>-0.105864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>7.222217</td>\n",
              "      <td>-1.011508</td>\n",
              "      <td>0.332970</td>\n",
              "      <td>-0.405879</td>\n",
              "      <td>2.436039</td>\n",
              "      <td>-0.023262</td>\n",
              "      <td>0.352652</td>\n",
              "      <td>1.205280</td>\n",
              "      <td>-0.604935</td>\n",
              "      <td>-0.035388</td>\n",
              "      <td>...</td>\n",
              "      <td>0.264695</td>\n",
              "      <td>-0.485988</td>\n",
              "      <td>-0.575964</td>\n",
              "      <td>-0.064719</td>\n",
              "      <td>0.069855</td>\n",
              "      <td>0.017468</td>\n",
              "      <td>-0.078100</td>\n",
              "      <td>-0.005051</td>\n",
              "      <td>-0.521922</td>\n",
              "      <td>0.310096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>6.948776</td>\n",
              "      <td>-0.575131</td>\n",
              "      <td>1.190701</td>\n",
              "      <td>-0.513953</td>\n",
              "      <td>1.864702</td>\n",
              "      <td>0.053287</td>\n",
              "      <td>0.334423</td>\n",
              "      <td>-0.239748</td>\n",
              "      <td>-0.058462</td>\n",
              "      <td>0.341092</td>\n",
              "      <td>...</td>\n",
              "      <td>0.128762</td>\n",
              "      <td>0.111403</td>\n",
              "      <td>0.022093</td>\n",
              "      <td>-0.161144</td>\n",
              "      <td>0.273438</td>\n",
              "      <td>-0.351178</td>\n",
              "      <td>0.091363</td>\n",
              "      <td>0.147706</td>\n",
              "      <td>0.012927</td>\n",
              "      <td>0.236450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>7.114773</td>\n",
              "      <td>0.999335</td>\n",
              "      <td>0.408795</td>\n",
              "      <td>-0.790729</td>\n",
              "      <td>1.222958</td>\n",
              "      <td>0.188703</td>\n",
              "      <td>0.416799</td>\n",
              "      <td>-0.561276</td>\n",
              "      <td>-0.129169</td>\n",
              "      <td>-0.361816</td>\n",
              "      <td>...</td>\n",
              "      <td>0.298246</td>\n",
              "      <td>-0.120146</td>\n",
              "      <td>-0.243777</td>\n",
              "      <td>-0.193970</td>\n",
              "      <td>-0.187693</td>\n",
              "      <td>0.193393</td>\n",
              "      <td>-0.084965</td>\n",
              "      <td>-0.243552</td>\n",
              "      <td>0.255849</td>\n",
              "      <td>-0.150146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>8.566109</td>\n",
              "      <td>0.652017</td>\n",
              "      <td>0.555999</td>\n",
              "      <td>-0.039120</td>\n",
              "      <td>1.827801</td>\n",
              "      <td>0.268175</td>\n",
              "      <td>0.529672</td>\n",
              "      <td>0.209192</td>\n",
              "      <td>-0.334205</td>\n",
              "      <td>-0.545892</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.106814</td>\n",
              "      <td>0.010436</td>\n",
              "      <td>0.240260</td>\n",
              "      <td>0.106577</td>\n",
              "      <td>-0.048935</td>\n",
              "      <td>0.053032</td>\n",
              "      <td>-0.120631</td>\n",
              "      <td>-0.076274</td>\n",
              "      <td>0.216099</td>\n",
              "      <td>-0.193066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>-0.645925</td>\n",
              "      <td>-2.615459</td>\n",
              "      <td>-1.874452</td>\n",
              "      <td>0.331043</td>\n",
              "      <td>-0.124215</td>\n",
              "      <td>0.806257</td>\n",
              "      <td>-0.853601</td>\n",
              "      <td>0.681877</td>\n",
              "      <td>-0.201955</td>\n",
              "      <td>0.029968</td>\n",
              "      <td>...</td>\n",
              "      <td>0.084995</td>\n",
              "      <td>-0.950816</td>\n",
              "      <td>0.152380</td>\n",
              "      <td>-0.096333</td>\n",
              "      <td>-0.360521</td>\n",
              "      <td>-0.636963</td>\n",
              "      <td>0.182494</td>\n",
              "      <td>0.116272</td>\n",
              "      <td>0.145837</td>\n",
              "      <td>-0.011910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>-1.407767</td>\n",
              "      <td>-2.200869</td>\n",
              "      <td>-1.877372</td>\n",
              "      <td>-0.870802</td>\n",
              "      <td>-0.213478</td>\n",
              "      <td>-1.136117</td>\n",
              "      <td>-0.138414</td>\n",
              "      <td>-1.388718</td>\n",
              "      <td>0.304442</td>\n",
              "      <td>0.732854</td>\n",
              "      <td>...</td>\n",
              "      <td>0.083153</td>\n",
              "      <td>0.141068</td>\n",
              "      <td>-0.196071</td>\n",
              "      <td>-0.017133</td>\n",
              "      <td>0.093527</td>\n",
              "      <td>0.108013</td>\n",
              "      <td>-0.018965</td>\n",
              "      <td>0.009624</td>\n",
              "      <td>-0.072017</td>\n",
              "      <td>-0.212278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.321378</td>\n",
              "      <td>-1.875157</td>\n",
              "      <td>-0.427114</td>\n",
              "      <td>1.717297</td>\n",
              "      <td>0.642612</td>\n",
              "      <td>-0.206885</td>\n",
              "      <td>-1.512476</td>\n",
              "      <td>0.858094</td>\n",
              "      <td>-1.546386</td>\n",
              "      <td>0.572665</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.411556</td>\n",
              "      <td>-0.015249</td>\n",
              "      <td>-0.172982</td>\n",
              "      <td>-0.080124</td>\n",
              "      <td>-0.420673</td>\n",
              "      <td>0.275791</td>\n",
              "      <td>-0.236614</td>\n",
              "      <td>-0.043578</td>\n",
              "      <td>-0.221336</td>\n",
              "      <td>0.329065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.253918</td>\n",
              "      <td>-0.777346</td>\n",
              "      <td>-0.444279</td>\n",
              "      <td>2.374200</td>\n",
              "      <td>-0.030165</td>\n",
              "      <td>-0.970837</td>\n",
              "      <td>-0.715522</td>\n",
              "      <td>1.849128</td>\n",
              "      <td>1.309309</td>\n",
              "      <td>0.333597</td>\n",
              "      <td>...</td>\n",
              "      <td>0.076484</td>\n",
              "      <td>-0.439216</td>\n",
              "      <td>0.089563</td>\n",
              "      <td>0.138923</td>\n",
              "      <td>0.454713</td>\n",
              "      <td>-0.014073</td>\n",
              "      <td>0.436484</td>\n",
              "      <td>0.061091</td>\n",
              "      <td>0.088260</td>\n",
              "      <td>-0.462703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>1.031057</td>\n",
              "      <td>0.081395</td>\n",
              "      <td>-0.383579</td>\n",
              "      <td>2.403520</td>\n",
              "      <td>-1.243376</td>\n",
              "      <td>0.543347</td>\n",
              "      <td>-1.806651</td>\n",
              "      <td>0.558427</td>\n",
              "      <td>-2.667445</td>\n",
              "      <td>1.703981</td>\n",
              "      <td>...</td>\n",
              "      <td>0.220287</td>\n",
              "      <td>0.151462</td>\n",
              "      <td>0.054684</td>\n",
              "      <td>0.104405</td>\n",
              "      <td>0.354576</td>\n",
              "      <td>-0.015021</td>\n",
              "      <td>0.062562</td>\n",
              "      <td>-0.013381</td>\n",
              "      <td>0.046793</td>\n",
              "      <td>-0.040719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>0.807086</td>\n",
              "      <td>-1.028513</td>\n",
              "      <td>-2.842726</td>\n",
              "      <td>1.866244</td>\n",
              "      <td>-0.376761</td>\n",
              "      <td>0.616221</td>\n",
              "      <td>-0.697409</td>\n",
              "      <td>1.367291</td>\n",
              "      <td>1.624622</td>\n",
              "      <td>0.368991</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229554</td>\n",
              "      <td>0.133209</td>\n",
              "      <td>0.237593</td>\n",
              "      <td>-0.176736</td>\n",
              "      <td>-0.334725</td>\n",
              "      <td>0.028267</td>\n",
              "      <td>-0.065541</td>\n",
              "      <td>-0.124133</td>\n",
              "      <td>0.011299</td>\n",
              "      <td>0.184846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>-0.395075</td>\n",
              "      <td>-2.009826</td>\n",
              "      <td>-2.418936</td>\n",
              "      <td>1.200081</td>\n",
              "      <td>-0.904394</td>\n",
              "      <td>0.677463</td>\n",
              "      <td>-1.362367</td>\n",
              "      <td>-0.608868</td>\n",
              "      <td>-0.813081</td>\n",
              "      <td>0.126835</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018812</td>\n",
              "      <td>0.267450</td>\n",
              "      <td>-0.128386</td>\n",
              "      <td>0.080484</td>\n",
              "      <td>0.013867</td>\n",
              "      <td>0.082405</td>\n",
              "      <td>-0.019154</td>\n",
              "      <td>0.007168</td>\n",
              "      <td>-0.002415</td>\n",
              "      <td>-0.011750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>-0.186865</td>\n",
              "      <td>1.001387</td>\n",
              "      <td>3.046893</td>\n",
              "      <td>0.714120</td>\n",
              "      <td>-1.772765</td>\n",
              "      <td>1.926453</td>\n",
              "      <td>-0.771086</td>\n",
              "      <td>-0.402627</td>\n",
              "      <td>0.104742</td>\n",
              "      <td>2.651770</td>\n",
              "      <td>...</td>\n",
              "      <td>0.098177</td>\n",
              "      <td>-0.111877</td>\n",
              "      <td>0.059881</td>\n",
              "      <td>0.004402</td>\n",
              "      <td>-0.130449</td>\n",
              "      <td>-0.039883</td>\n",
              "      <td>-0.035877</td>\n",
              "      <td>-0.000887</td>\n",
              "      <td>0.034442</td>\n",
              "      <td>0.006651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>0.237640</td>\n",
              "      <td>-0.383473</td>\n",
              "      <td>1.286859</td>\n",
              "      <td>-0.997945</td>\n",
              "      <td>-1.344278</td>\n",
              "      <td>-2.151856</td>\n",
              "      <td>-0.595879</td>\n",
              "      <td>1.329868</td>\n",
              "      <td>0.522781</td>\n",
              "      <td>-0.722415</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.013792</td>\n",
              "      <td>-0.225170</td>\n",
              "      <td>0.282117</td>\n",
              "      <td>0.269362</td>\n",
              "      <td>-0.101066</td>\n",
              "      <td>-0.071562</td>\n",
              "      <td>0.062494</td>\n",
              "      <td>-0.243757</td>\n",
              "      <td>0.005644</td>\n",
              "      <td>0.111357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>-1.861955</td>\n",
              "      <td>-0.168720</td>\n",
              "      <td>0.976660</td>\n",
              "      <td>-0.534565</td>\n",
              "      <td>0.455795</td>\n",
              "      <td>-0.776867</td>\n",
              "      <td>-0.452693</td>\n",
              "      <td>0.807612</td>\n",
              "      <td>-1.241835</td>\n",
              "      <td>-0.856995</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.404808</td>\n",
              "      <td>0.134439</td>\n",
              "      <td>-0.011237</td>\n",
              "      <td>0.114788</td>\n",
              "      <td>-0.153699</td>\n",
              "      <td>0.113997</td>\n",
              "      <td>0.306465</td>\n",
              "      <td>0.153019</td>\n",
              "      <td>0.296461</td>\n",
              "      <td>0.578454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>-1.001201</td>\n",
              "      <td>-0.630144</td>\n",
              "      <td>0.452858</td>\n",
              "      <td>-0.791519</td>\n",
              "      <td>-0.471162</td>\n",
              "      <td>-0.120609</td>\n",
              "      <td>-1.146068</td>\n",
              "      <td>0.575198</td>\n",
              "      <td>-0.205599</td>\n",
              "      <td>-1.098592</td>\n",
              "      <td>...</td>\n",
              "      <td>0.464877</td>\n",
              "      <td>0.212237</td>\n",
              "      <td>0.600586</td>\n",
              "      <td>-0.221337</td>\n",
              "      <td>-0.022835</td>\n",
              "      <td>-0.319404</td>\n",
              "      <td>0.166073</td>\n",
              "      <td>-0.512772</td>\n",
              "      <td>-0.361424</td>\n",
              "      <td>-0.032517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0.473754</td>\n",
              "      <td>1.692235</td>\n",
              "      <td>2.473895</td>\n",
              "      <td>0.629114</td>\n",
              "      <td>-1.383129</td>\n",
              "      <td>-0.759267</td>\n",
              "      <td>-1.741127</td>\n",
              "      <td>1.266693</td>\n",
              "      <td>0.232469</td>\n",
              "      <td>-1.812457</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004904</td>\n",
              "      <td>-0.185628</td>\n",
              "      <td>-0.350573</td>\n",
              "      <td>-0.336581</td>\n",
              "      <td>-0.136353</td>\n",
              "      <td>-0.042791</td>\n",
              "      <td>0.136589</td>\n",
              "      <td>0.500420</td>\n",
              "      <td>-0.004391</td>\n",
              "      <td>-0.130279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>-0.487110</td>\n",
              "      <td>2.589771</td>\n",
              "      <td>3.490235</td>\n",
              "      <td>0.222079</td>\n",
              "      <td>-0.976462</td>\n",
              "      <td>1.375417</td>\n",
              "      <td>-0.971469</td>\n",
              "      <td>1.235543</td>\n",
              "      <td>1.822634</td>\n",
              "      <td>-1.156856</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.133691</td>\n",
              "      <td>0.545830</td>\n",
              "      <td>-0.102365</td>\n",
              "      <td>-0.323477</td>\n",
              "      <td>-0.020121</td>\n",
              "      <td>0.130904</td>\n",
              "      <td>0.147145</td>\n",
              "      <td>0.290111</td>\n",
              "      <td>-0.048614</td>\n",
              "      <td>-0.078692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>-0.087586</td>\n",
              "      <td>0.320785</td>\n",
              "      <td>0.233389</td>\n",
              "      <td>0.674989</td>\n",
              "      <td>-1.858322</td>\n",
              "      <td>1.800417</td>\n",
              "      <td>-0.060712</td>\n",
              "      <td>0.065915</td>\n",
              "      <td>0.926925</td>\n",
              "      <td>-0.423506</td>\n",
              "      <td>...</td>\n",
              "      <td>0.124573</td>\n",
              "      <td>-0.122813</td>\n",
              "      <td>0.115199</td>\n",
              "      <td>0.041773</td>\n",
              "      <td>-0.095950</td>\n",
              "      <td>-0.080089</td>\n",
              "      <td>-0.320160</td>\n",
              "      <td>-0.034541</td>\n",
              "      <td>0.005961</td>\n",
              "      <td>0.263597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.077380</td>\n",
              "      <td>3.572014</td>\n",
              "      <td>0.896210</td>\n",
              "      <td>0.744622</td>\n",
              "      <td>-3.367878</td>\n",
              "      <td>3.256363</td>\n",
              "      <td>0.249904</td>\n",
              "      <td>-0.928727</td>\n",
              "      <td>0.021332</td>\n",
              "      <td>-0.950284</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.111266</td>\n",
              "      <td>0.093473</td>\n",
              "      <td>-0.106342</td>\n",
              "      <td>0.053073</td>\n",
              "      <td>-0.002879</td>\n",
              "      <td>-0.119720</td>\n",
              "      <td>0.188424</td>\n",
              "      <td>-0.140838</td>\n",
              "      <td>0.016097</td>\n",
              "      <td>0.267991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>-0.458112</td>\n",
              "      <td>1.236125</td>\n",
              "      <td>4.384637</td>\n",
              "      <td>0.560432</td>\n",
              "      <td>-2.208922</td>\n",
              "      <td>2.313967</td>\n",
              "      <td>-0.802622</td>\n",
              "      <td>-1.025953</td>\n",
              "      <td>-1.692228</td>\n",
              "      <td>2.282681</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.065489</td>\n",
              "      <td>0.016242</td>\n",
              "      <td>-0.002261</td>\n",
              "      <td>-0.017577</td>\n",
              "      <td>-0.059603</td>\n",
              "      <td>-0.027986</td>\n",
              "      <td>0.102249</td>\n",
              "      <td>-0.080871</td>\n",
              "      <td>0.056780</td>\n",
              "      <td>0.008198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>-0.673502</td>\n",
              "      <td>0.026571</td>\n",
              "      <td>2.225577</td>\n",
              "      <td>0.275774</td>\n",
              "      <td>-0.205987</td>\n",
              "      <td>0.014778</td>\n",
              "      <td>-0.940846</td>\n",
              "      <td>1.237993</td>\n",
              "      <td>0.477398</td>\n",
              "      <td>-0.691240</td>\n",
              "      <td>...</td>\n",
              "      <td>0.348846</td>\n",
              "      <td>0.204388</td>\n",
              "      <td>-0.137155</td>\n",
              "      <td>0.588687</td>\n",
              "      <td>-0.184511</td>\n",
              "      <td>-0.089425</td>\n",
              "      <td>0.280229</td>\n",
              "      <td>-0.663489</td>\n",
              "      <td>-0.166260</td>\n",
              "      <td>0.181005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>0.058923</td>\n",
              "      <td>-0.221803</td>\n",
              "      <td>0.737666</td>\n",
              "      <td>1.151257</td>\n",
              "      <td>-1.184505</td>\n",
              "      <td>0.206355</td>\n",
              "      <td>0.120637</td>\n",
              "      <td>1.580138</td>\n",
              "      <td>-0.950986</td>\n",
              "      <td>-0.260030</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.077231</td>\n",
              "      <td>-0.019061</td>\n",
              "      <td>0.074754</td>\n",
              "      <td>-0.591578</td>\n",
              "      <td>0.095841</td>\n",
              "      <td>0.198789</td>\n",
              "      <td>-0.336367</td>\n",
              "      <td>0.104396</td>\n",
              "      <td>0.270823</td>\n",
              "      <td>-0.588938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>-0.760917</td>\n",
              "      <td>0.429009</td>\n",
              "      <td>0.926646</td>\n",
              "      <td>-0.466413</td>\n",
              "      <td>-1.378488</td>\n",
              "      <td>1.120628</td>\n",
              "      <td>-1.294069</td>\n",
              "      <td>0.200679</td>\n",
              "      <td>-0.702918</td>\n",
              "      <td>-0.229318</td>\n",
              "      <td>...</td>\n",
              "      <td>0.546424</td>\n",
              "      <td>-0.282623</td>\n",
              "      <td>0.020602</td>\n",
              "      <td>0.301686</td>\n",
              "      <td>0.072675</td>\n",
              "      <td>0.271945</td>\n",
              "      <td>-0.248259</td>\n",
              "      <td>0.250986</td>\n",
              "      <td>-0.292536</td>\n",
              "      <td>-0.092558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.219442</td>\n",
              "      <td>1.270641</td>\n",
              "      <td>3.100799</td>\n",
              "      <td>0.898660</td>\n",
              "      <td>-1.054064</td>\n",
              "      <td>0.788914</td>\n",
              "      <td>-0.990733</td>\n",
              "      <td>0.515385</td>\n",
              "      <td>-0.417799</td>\n",
              "      <td>-0.476812</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.172917</td>\n",
              "      <td>-0.390296</td>\n",
              "      <td>0.496663</td>\n",
              "      <td>-0.263594</td>\n",
              "      <td>0.213383</td>\n",
              "      <td>-0.165381</td>\n",
              "      <td>-0.518336</td>\n",
              "      <td>0.160600</td>\n",
              "      <td>0.145911</td>\n",
              "      <td>0.233921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>-0.441407</td>\n",
              "      <td>0.296999</td>\n",
              "      <td>1.497629</td>\n",
              "      <td>0.187879</td>\n",
              "      <td>-1.194458</td>\n",
              "      <td>1.600436</td>\n",
              "      <td>-1.746629</td>\n",
              "      <td>1.181340</td>\n",
              "      <td>0.886013</td>\n",
              "      <td>0.025388</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.500080</td>\n",
              "      <td>0.189998</td>\n",
              "      <td>-0.160553</td>\n",
              "      <td>0.169078</td>\n",
              "      <td>0.432112</td>\n",
              "      <td>0.094936</td>\n",
              "      <td>0.095585</td>\n",
              "      <td>-0.312528</td>\n",
              "      <td>0.147194</td>\n",
              "      <td>-0.202735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>-1.509170</td>\n",
              "      <td>1.832384</td>\n",
              "      <td>0.814599</td>\n",
              "      <td>-0.880316</td>\n",
              "      <td>-0.857682</td>\n",
              "      <td>-0.492055</td>\n",
              "      <td>0.454086</td>\n",
              "      <td>1.153471</td>\n",
              "      <td>1.448765</td>\n",
              "      <td>-1.293766</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.068755</td>\n",
              "      <td>-0.106495</td>\n",
              "      <td>0.170446</td>\n",
              "      <td>0.169195</td>\n",
              "      <td>0.262094</td>\n",
              "      <td>-0.117901</td>\n",
              "      <td>-0.164800</td>\n",
              "      <td>-0.069491</td>\n",
              "      <td>-0.050880</td>\n",
              "      <td>-0.021687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>-0.155126</td>\n",
              "      <td>1.393178</td>\n",
              "      <td>0.365538</td>\n",
              "      <td>-1.099525</td>\n",
              "      <td>-1.299107</td>\n",
              "      <td>-0.408043</td>\n",
              "      <td>0.279709</td>\n",
              "      <td>-0.119767</td>\n",
              "      <td>0.985277</td>\n",
              "      <td>-0.515693</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.150809</td>\n",
              "      <td>0.028025</td>\n",
              "      <td>-0.112792</td>\n",
              "      <td>-0.361274</td>\n",
              "      <td>-0.143445</td>\n",
              "      <td>-0.003529</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.092871</td>\n",
              "      <td>0.046724</td>\n",
              "      <td>0.017889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>-1.172911</td>\n",
              "      <td>2.346800</td>\n",
              "      <td>2.090372</td>\n",
              "      <td>-1.631477</td>\n",
              "      <td>-0.952774</td>\n",
              "      <td>0.236958</td>\n",
              "      <td>-0.389693</td>\n",
              "      <td>0.292413</td>\n",
              "      <td>-0.029682</td>\n",
              "      <td>-0.423091</td>\n",
              "      <td>...</td>\n",
              "      <td>0.061643</td>\n",
              "      <td>-0.252224</td>\n",
              "      <td>0.104380</td>\n",
              "      <td>0.054052</td>\n",
              "      <td>-0.149576</td>\n",
              "      <td>0.054327</td>\n",
              "      <td>-0.085206</td>\n",
              "      <td>-0.020826</td>\n",
              "      <td>-0.022760</td>\n",
              "      <td>-0.051240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>-0.328603</td>\n",
              "      <td>3.061680</td>\n",
              "      <td>2.256522</td>\n",
              "      <td>-0.424378</td>\n",
              "      <td>-1.868915</td>\n",
              "      <td>-0.350414</td>\n",
              "      <td>0.226009</td>\n",
              "      <td>1.162425</td>\n",
              "      <td>0.603590</td>\n",
              "      <td>-0.217285</td>\n",
              "      <td>...</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.008060</td>\n",
              "      <td>-0.163510</td>\n",
              "      <td>0.093020</td>\n",
              "      <td>-0.086098</td>\n",
              "      <td>0.027652</td>\n",
              "      <td>0.021020</td>\n",
              "      <td>0.002824</td>\n",
              "      <td>-0.078523</td>\n",
              "      <td>-0.032950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>-0.718224</td>\n",
              "      <td>0.687967</td>\n",
              "      <td>-1.267417</td>\n",
              "      <td>-0.670412</td>\n",
              "      <td>-1.567853</td>\n",
              "      <td>1.657080</td>\n",
              "      <td>0.818342</td>\n",
              "      <td>0.515184</td>\n",
              "      <td>-0.144439</td>\n",
              "      <td>0.125603</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.044075</td>\n",
              "      <td>0.357937</td>\n",
              "      <td>0.251036</td>\n",
              "      <td>-0.005687</td>\n",
              "      <td>0.141986</td>\n",
              "      <td>-0.258837</td>\n",
              "      <td>0.109927</td>\n",
              "      <td>-0.115133</td>\n",
              "      <td>-0.061322</td>\n",
              "      <td>-0.033643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>0.445232</td>\n",
              "      <td>1.184668</td>\n",
              "      <td>0.467780</td>\n",
              "      <td>-2.138310</td>\n",
              "      <td>-1.128685</td>\n",
              "      <td>-0.514149</td>\n",
              "      <td>0.245567</td>\n",
              "      <td>2.021899</td>\n",
              "      <td>0.228138</td>\n",
              "      <td>-0.953547</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.011740</td>\n",
              "      <td>0.419290</td>\n",
              "      <td>0.062435</td>\n",
              "      <td>0.191918</td>\n",
              "      <td>0.126068</td>\n",
              "      <td>0.287124</td>\n",
              "      <td>-0.040200</td>\n",
              "      <td>0.194610</td>\n",
              "      <td>-0.116119</td>\n",
              "      <td>0.169397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>-1.120806</td>\n",
              "      <td>0.495716</td>\n",
              "      <td>0.477714</td>\n",
              "      <td>-1.356863</td>\n",
              "      <td>-0.036207</td>\n",
              "      <td>0.809519</td>\n",
              "      <td>-0.160998</td>\n",
              "      <td>-0.731385</td>\n",
              "      <td>1.327201</td>\n",
              "      <td>0.967042</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.069784</td>\n",
              "      <td>0.047647</td>\n",
              "      <td>-0.162943</td>\n",
              "      <td>-0.053178</td>\n",
              "      <td>0.104861</td>\n",
              "      <td>0.008137</td>\n",
              "      <td>-0.253575</td>\n",
              "      <td>-0.041771</td>\n",
              "      <td>-0.019096</td>\n",
              "      <td>0.024908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>-1.746276</td>\n",
              "      <td>1.329017</td>\n",
              "      <td>1.284418</td>\n",
              "      <td>-0.877288</td>\n",
              "      <td>-0.101824</td>\n",
              "      <td>0.200079</td>\n",
              "      <td>0.221382</td>\n",
              "      <td>-0.569326</td>\n",
              "      <td>1.123788</td>\n",
              "      <td>-0.238124</td>\n",
              "      <td>...</td>\n",
              "      <td>0.070366</td>\n",
              "      <td>-0.196414</td>\n",
              "      <td>0.009736</td>\n",
              "      <td>0.119265</td>\n",
              "      <td>0.025462</td>\n",
              "      <td>0.260401</td>\n",
              "      <td>0.209111</td>\n",
              "      <td>-0.092553</td>\n",
              "      <td>0.108953</td>\n",
              "      <td>-0.142083</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>106 rows × 94 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c00092b0-ab7b-4de3-9614-ceacbc582024')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c00092b0-ab7b-4de3-9614-ceacbc582024 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c00092b0-ab7b-4de3-9614-ceacbc582024');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-137a23b2-4e51-457c-91d0-6d4dbc8c2f17\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-137a23b2-4e51-457c-91d0-6d4dbc8c2f17')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-137a23b2-4e51-457c-91d0-6d4dbc8c2f17 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_4107c637-124d-4fde-9a99-445c022c4763\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('X_train_df_copy')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4107c637-124d-4fde-9a99-445c022c4763 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('X_train_df_copy');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train_df_copy"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "X.drop(columns=['label'],axis=1,inplace=True)\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "f14fe96d-da32-46e4-af9a-adb84db689a7",
      "metadata": {
        "id": "f14fe96d-da32-46e4-af9a-adb84db689a7"
      },
      "outputs": [],
      "source": [
        "indices_to_change = [4, 105, 84, 27, 98, 88, 18, 65, 9, 2, 5, 49, 99, 69, 86, 67, 7, 28, 78, 70, 18, 74]\n",
        "\n",
        "# Create a boolean mask to identify indices for the test set\n",
        "test_mask = np.zeros(len(X), dtype=bool)\n",
        "test_mask[indices_to_change] = True\n",
        "\n",
        "# Split the dataset based on the boolean mask\n",
        "X_train = X[~test_mask]\n",
        "X_test = X[test_mask]\n",
        "y_train = y[~test_mask]\n",
        "y_test = y[test_mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "232245f6",
      "metadata": {
        "id": "232245f6",
        "outputId": "3624143b-a428-4080-ea2d-479688cb14ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'C': 0.1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
            "Best Score: 0.5767651888341544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "12 fits failed out of a total of 72.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "12 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 80, in _check_solver\n",
            "    raise ValueError(\"penalty=None is not supported for the liblinear solver\")\n",
            "ValueError: penalty=None is not supported for the liblinear solver\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.43513957 0.57676519 0.50697865 0.48357964        nan 0.44704433\n",
            " 0.4954844  0.47208539 0.47085386 0.51847291        nan 0.44704433\n",
            " 0.4954844  0.47208539 0.4589491  0.51847291        nan 0.44704433\n",
            " 0.4954844  0.48399015 0.4589491  0.51847291        nan 0.44704433]\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "param_grid = {'penalty':['l1','l2',None],'C': [0.1,2,2.5,3],\n",
        "                   'solver':['liblinear','saga'],'max_iter':[500]}\n",
        "LR=LogisticRegression(random_state=0)\n",
        "grid_search_LR = GridSearchCV(estimator=LR,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=3)\n",
        "\n",
        "grid_search_LR.fit(X_train, y_train)\n",
        "print(\"Best parameters found: \", grid_search_LR.best_params_)\n",
        "print(\"Best Score: {}\".format(grid_search_LR.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "3cc09049",
      "metadata": {
        "id": "3cc09049",
        "outputId": "ff095104-6fb2-4d50-a5a7-bfa071d946d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'C': 0.1, 'l1_ratio': 0.7, 'max_iter': 500, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
            "Best Score: 0.6586378737541528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "param_grid = {'penalty':['elasticnet',None],'C': [0.1,2,2.5,3],\n",
        "                   'solver':['saga'],'max_iter':[500],'l1_ratio':[0.5,0.2,0.7]}\n",
        "LR=LogisticRegression(random_state=0)\n",
        "grid_search_LR = GridSearchCV(estimator=LR,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=2)\n",
        "grid_search_LR.fit(X_train, y_train)\n",
        "print(\"Best parameters found: \", grid_search_LR.best_params_)\n",
        "print(\"Best Score: {}\".format(grid_search_LR.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "7d4dfadd",
      "metadata": {
        "id": "7d4dfadd",
        "outputId": "c94430f3-5583-4752-cdb5-9ea56366c904",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'C': 2, 'max_iter': 500, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Best Score: 0.5414614121510674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "param_grid = {'penalty':['l2',None],'C': [0.1,2,2.5,3],\n",
        "                   'solver':['lbfgs','newton-cg','sag'],'max_iter':[500]}\n",
        "LR=LogisticRegression(random_state=0)\n",
        "grid_search_LR = GridSearchCV(estimator=LR,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=3\n",
        "                          )\n",
        "\n",
        "grid_search_LR.fit(X_train, y_train)\n",
        "print(\"Best parameters found: \", grid_search_LR.best_params_)\n",
        "print(\"Best Score: {}\".format(grid_search_LR.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "36fa37d7",
      "metadata": {
        "id": "36fa37d7",
        "outputId": "326de8a4-35dd-4a3b-fcb8-70416af68fee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.1, max_iter=1000, penalty=None)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.1, max_iter=1000, penalty=None)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=0.1, max_iter=1000, penalty=None)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "LR=LogisticRegression(C=0.1,penalty=None,solver='lbfgs',max_iter=1000)\n",
        "LR.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "17158811",
      "metadata": {
        "id": "17158811",
        "outputId": "d549b330-794d-43d5-d46e-076aeaa2a980",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_accuracy 1.0\n",
            "test_accuracy 0.5714285714285714\n"
          ]
        }
      ],
      "source": [
        "LR_pred = LR.predict(X_train)\n",
        "acc_train=accuracy_score(y_train,LR_pred)\n",
        "print('train_accuracy',acc_train)\n",
        "LR_pred = LR.predict(X_test)\n",
        "acc=accuracy_score(y_test,LR_pred)\n",
        "print('test_accuracy',acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7b8a2f2-3db9-4cd9-9af5-06720d76eccf",
      "metadata": {
        "id": "b7b8a2f2-3db9-4cd9-9af5-06720d76eccf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ba961e4-336d-41c3-8509-7e816bf5f69e",
      "metadata": {
        "id": "9ba961e4-336d-41c3-8509-7e816bf5f69e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "4e2df025-4cd8-4584-b163-3e1722f1ef04",
      "metadata": {
        "id": "4e2df025-4cd8-4584-b163-3e1722f1ef04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "eb239924-5534-4c59-cd06-ed9e3c00430d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 2.0587\n",
            "Epoch 2/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9713  \n",
            "Epoch 3/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9102 \n",
            "Epoch 4/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.8237 \n",
            "Epoch 5/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.7718 \n",
            "Epoch 6/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.6969\n",
            "Epoch 7/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.6178 \n",
            "Epoch 8/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.5574 \n",
            "Epoch 9/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.5171 \n",
            "Epoch 10/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4487 \n",
            "Epoch 11/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.3955 \n",
            "Epoch 12/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.3563 \n",
            "Epoch 13/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.3212  \n",
            "Epoch 14/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.2747 \n",
            "Epoch 15/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.2236 \n",
            "Epoch 16/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.1718\n",
            "Epoch 17/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.1429  \n",
            "Epoch 18/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.1324 \n",
            "Epoch 19/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0768 \n",
            "Epoch 20/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0394 \n",
            "Epoch 21/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.0203 \n",
            "Epoch 22/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.9920 \n",
            "Epoch 23/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.9677 \n",
            "Epoch 24/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.9334 \n",
            "Epoch 25/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.8983 \n",
            "Epoch 26/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.8840\n",
            "Epoch 27/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.8663  \n",
            "Epoch 28/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.8233  \n",
            "Epoch 29/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7836\n",
            "Epoch 30/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7519 \n",
            "Epoch 31/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7330 \n",
            "Epoch 32/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7286 \n",
            "Epoch 33/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7039 \n",
            "Epoch 34/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6873\n",
            "Epoch 35/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6823 \n",
            "Epoch 36/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6282 \n",
            "Epoch 37/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6413 \n",
            "Epoch 38/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6040 \n",
            "Epoch 39/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6053 \n",
            "Epoch 40/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5646\n",
            "Epoch 41/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5715 \n",
            "Epoch 42/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5082 \n",
            "Epoch 43/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5359 \n",
            "Epoch 44/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4990\n",
            "Epoch 45/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5010 \n",
            "Epoch 46/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4353\n",
            "Epoch 47/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4258 \n",
            "Epoch 48/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4729 \n",
            "Epoch 49/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4447 \n",
            "Epoch 50/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4164 \n",
            "Epoch 51/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3862 \n",
            "Epoch 52/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4009 \n",
            "Epoch 53/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3649 \n",
            "Epoch 54/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3696 \n",
            "Epoch 55/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3648\n",
            "Epoch 56/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3471 \n",
            "Epoch 57/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3299 \n",
            "Epoch 58/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3243\n",
            "Epoch 59/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2869 \n",
            "Epoch 60/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3036 \n",
            "Epoch 61/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2835 \n",
            "Epoch 62/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3024 \n",
            "Epoch 63/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2945  \n",
            "Epoch 64/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2763 \n",
            "Epoch 65/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2571 \n",
            "Epoch 66/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2466 \n",
            "Epoch 67/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2242 \n",
            "Epoch 68/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2399 \n",
            "Epoch 69/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2320 \n",
            "Epoch 70/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2100 \n",
            "Epoch 71/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2334\n",
            "Epoch 72/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2390\n",
            "Epoch 73/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1943\n",
            "Epoch 74/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1992 \n",
            "Epoch 75/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1866 \n",
            "Epoch 76/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2033 \n",
            "Epoch 77/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1867\n",
            "Epoch 78/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1905 \n",
            "Epoch 79/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1825 \n",
            "Epoch 80/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1851 \n",
            "Epoch 81/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1802\n",
            "Epoch 82/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1811 \n",
            "Epoch 83/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1831\n",
            "Epoch 84/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1921 \n",
            "Epoch 85/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1518  \n",
            "Epoch 86/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1524 \n",
            "Epoch 87/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1720 \n",
            "Epoch 88/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1447 \n",
            "Epoch 89/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1449 \n",
            "Epoch 90/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1522\n",
            "Epoch 91/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1625\n",
            "Epoch 92/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1397 \n",
            "Epoch 93/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1506 \n",
            "Epoch 94/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1345 \n",
            "Epoch 95/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1414 \n",
            "Epoch 96/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1307 \n",
            "Epoch 97/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1352\n",
            "Epoch 98/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1339 \n",
            "Epoch 99/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1232\n",
            "Epoch 100/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1450\n",
            "Epoch 101/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1322 \n",
            "Epoch 102/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1196\n",
            "Epoch 103/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1193 \n",
            "Epoch 104/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1222\n",
            "Epoch 105/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1387 \n",
            "Epoch 106/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1114 \n",
            "Epoch 107/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1222 \n",
            "Epoch 108/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1085\n",
            "Epoch 109/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1163 \n",
            "Epoch 110/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1193 \n",
            "Epoch 111/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1103\n",
            "Epoch 112/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1006  \n",
            "Epoch 113/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1180\n",
            "Epoch 114/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1036 \n",
            "Epoch 115/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1030 \n",
            "Epoch 116/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1062 \n",
            "Epoch 117/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0934 \n",
            "Epoch 118/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1119 \n",
            "Epoch 119/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0913 \n",
            "Epoch 120/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1039 \n",
            "Epoch 121/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0998 \n",
            "Epoch 122/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0894 \n",
            "Epoch 123/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1011 \n",
            "Epoch 124/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1049 \n",
            "Epoch 125/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0931\n",
            "Epoch 126/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0913 \n",
            "Epoch 127/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1112 \n",
            "Epoch 128/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0891 \n",
            "Epoch 129/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0761 \n",
            "Epoch 130/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0913 \n",
            "Epoch 131/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0831 \n",
            "Epoch 132/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0860 \n",
            "Epoch 133/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0831 \n",
            "Epoch 134/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0847 \n",
            "Epoch 135/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0867\n",
            "Epoch 136/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0856  \n",
            "Epoch 137/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0919 \n",
            "Epoch 138/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0819 \n",
            "Epoch 139/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0799 \n",
            "Epoch 140/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0970\n",
            "Epoch 141/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0862 \n",
            "Epoch 142/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0730 \n",
            "Epoch 143/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0732 \n",
            "Epoch 144/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0773 \n",
            "Epoch 145/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0757 \n",
            "Epoch 146/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0776 \n",
            "Epoch 147/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0703 \n",
            "Epoch 148/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0693 \n",
            "Epoch 149/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0705  \n",
            "Epoch 150/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0677 \n",
            "Epoch 151/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0688 \n",
            "Epoch 152/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0660 \n",
            "Epoch 153/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0680\n",
            "Epoch 154/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0663 \n",
            "Epoch 155/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0616 \n",
            "Epoch 156/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0632 \n",
            "Epoch 157/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0659 \n",
            "Epoch 158/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0583 \n",
            "Epoch 159/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0567 \n",
            "Epoch 160/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0647 \n",
            "Epoch 161/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0666 \n",
            "Epoch 162/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0736 \n",
            "Epoch 163/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0644\n",
            "Epoch 164/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0581 \n",
            "Epoch 165/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0630 \n",
            "Epoch 166/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0622 \n",
            "Epoch 167/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0625 \n",
            "Epoch 168/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0627 \n",
            "Epoch 169/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0726 \n",
            "Epoch 170/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0617  \n",
            "Epoch 171/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0578 \n",
            "Epoch 172/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0630 \n",
            "Epoch 173/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0607 \n",
            "Epoch 174/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0536 \n",
            "Epoch 175/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0510 \n",
            "Epoch 176/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0559 \n",
            "Epoch 177/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0551\n",
            "Epoch 178/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0652 \n",
            "Epoch 179/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0603\n",
            "Epoch 180/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0519 \n",
            "Epoch 181/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0567 \n",
            "Epoch 182/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0471\n",
            "Epoch 183/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0459 \n",
            "Epoch 184/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0636 \n",
            "Epoch 185/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0473 \n",
            "Epoch 186/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0561  \n",
            "Epoch 187/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0523 \n",
            "Epoch 188/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0435\n",
            "Epoch 189/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0447 \n",
            "Epoch 190/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0438 \n",
            "Epoch 191/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0494\n",
            "Epoch 192/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0527  \n",
            "Epoch 193/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0531\n",
            "Epoch 194/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0465\n",
            "Epoch 195/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0481 \n",
            "Epoch 196/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0553 \n",
            "Epoch 197/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0542\n",
            "Epoch 198/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0405 \n",
            "Epoch 199/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0559 \n",
            "Epoch 200/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0452 \n",
            "Epoch 201/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0476 \n",
            "Epoch 202/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0493\n",
            "Epoch 203/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0523 \n",
            "Epoch 204/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0467 \n",
            "Epoch 205/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0425 \n",
            "Epoch 206/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0454 \n",
            "Epoch 207/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0460 \n",
            "Epoch 208/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0455\n",
            "Epoch 209/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0403\n",
            "Epoch 210/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0434 \n",
            "Epoch 211/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0499 \n",
            "Epoch 212/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0430 \n",
            "Epoch 213/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0422 \n",
            "Epoch 214/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0432 \n",
            "Epoch 215/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0425 \n",
            "Epoch 216/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0388 \n",
            "Epoch 217/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0366 \n",
            "Epoch 218/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0486\n",
            "Epoch 219/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0357 \n",
            "Epoch 220/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0430 \n",
            "Epoch 221/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0465 \n",
            "Epoch 222/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0363 \n",
            "Epoch 223/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0385\n",
            "Epoch 224/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0434\n",
            "Epoch 225/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0418  \n",
            "Epoch 226/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0381 \n",
            "Epoch 227/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0324 \n",
            "Epoch 228/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0343 \n",
            "Epoch 229/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0403 \n",
            "Epoch 230/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0375\n",
            "Epoch 231/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0386 \n",
            "Epoch 232/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0405 \n",
            "Epoch 233/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0429 \n",
            "Epoch 234/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0385 \n",
            "Epoch 235/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0325 \n",
            "Epoch 236/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0366 \n",
            "Epoch 237/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0370 \n",
            "Epoch 238/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0449\n",
            "Epoch 239/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0381  \n",
            "Epoch 240/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0324\n",
            "Epoch 241/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0393 \n",
            "Epoch 242/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0320 \n",
            "Epoch 243/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0341\n",
            "Epoch 244/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0329 \n",
            "Epoch 245/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0316 \n",
            "Epoch 246/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0395 \n",
            "Epoch 247/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0398\n",
            "Epoch 248/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0301 \n",
            "Epoch 249/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0493\n",
            "Epoch 250/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0313\n",
            "Epoch 251/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0323 \n",
            "Epoch 252/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0356 \n",
            "Epoch 253/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0328  \n",
            "Epoch 254/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0307 \n",
            "Epoch 255/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0332\n",
            "Epoch 256/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0295 \n",
            "Epoch 257/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0303 \n",
            "Epoch 258/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0292 \n",
            "Epoch 259/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0331 \n",
            "Epoch 260/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0338 \n",
            "Epoch 261/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0300 \n",
            "Epoch 262/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0298\n",
            "Epoch 263/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0314 \n",
            "Epoch 264/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0334\n",
            "Epoch 265/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0317 \n",
            "Epoch 266/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0328 \n",
            "Epoch 267/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0382 \n",
            "Epoch 268/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0331 \n",
            "Epoch 269/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0329 \n",
            "Epoch 270/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0256\n",
            "Epoch 271/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0304 \n",
            "Epoch 272/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0304 \n",
            "Epoch 273/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0270\n",
            "Epoch 274/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0300 \n",
            "Epoch 275/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0323 \n",
            "Epoch 276/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0340 \n",
            "Epoch 277/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0329 \n",
            "Epoch 278/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0303 \n",
            "Epoch 279/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0271 \n",
            "Epoch 280/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0287 \n",
            "Epoch 281/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0313 \n",
            "Epoch 282/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0305\n",
            "Epoch 283/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0318\n",
            "Epoch 284/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0270\n",
            "Epoch 285/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0275\n",
            "Epoch 286/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0306  \n",
            "Epoch 287/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0277 \n",
            "Epoch 288/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0300 \n",
            "Epoch 289/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0270\n",
            "Epoch 290/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0271 \n",
            "Epoch 291/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0290 \n",
            "Epoch 292/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0288\n",
            "Epoch 293/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0276\n",
            "Epoch 294/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0279 \n",
            "Epoch 295/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0254 \n",
            "Epoch 296/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0285 \n",
            "Epoch 297/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0259 \n",
            "Epoch 298/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0256 \n",
            "Epoch 299/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0288 \n",
            "Epoch 300/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0226 \n",
            "Epoch 301/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0293 \n",
            "Epoch 302/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0236\n",
            "Epoch 303/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0259 \n",
            "Epoch 304/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0250\n",
            "Epoch 305/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0220 \n",
            "Epoch 306/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0254\n",
            "Epoch 307/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0232\n",
            "Epoch 308/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0299\n",
            "Epoch 309/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0244\n",
            "Epoch 310/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0230\n",
            "Epoch 311/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0253\n",
            "Epoch 312/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0248\n",
            "Epoch 313/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0226  \n",
            "Epoch 314/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0272\n",
            "Epoch 315/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0242 \n",
            "Epoch 316/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0288 \n",
            "Epoch 317/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0230 \n",
            "Epoch 318/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0226 \n",
            "Epoch 319/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0234\n",
            "Epoch 320/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0218\n",
            "Epoch 321/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0235\n",
            "Epoch 322/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0213\n",
            "Epoch 323/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0306  \n",
            "Epoch 324/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0204 \n",
            "Epoch 325/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0227 \n",
            "Epoch 326/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0253  \n",
            "Epoch 327/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0223\n",
            "Epoch 328/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0227 \n",
            "Epoch 329/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0226 \n",
            "Epoch 330/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0201 \n",
            "Epoch 331/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0250 \n",
            "Epoch 332/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0178\n",
            "Epoch 333/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0269\n",
            "Epoch 334/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0225\n",
            "Epoch 335/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0240\n",
            "Epoch 336/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0235 \n",
            "Epoch 337/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0226 \n",
            "Epoch 338/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0211 \n",
            "Epoch 339/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0212\n",
            "Epoch 340/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0261\n",
            "Epoch 341/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0240  \n",
            "Epoch 342/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0229 \n",
            "Epoch 343/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0204 \n",
            "Epoch 344/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0233 \n",
            "Epoch 345/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0207 \n",
            "Epoch 346/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0206\n",
            "Epoch 347/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0169  \n",
            "Epoch 348/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0235 \n",
            "Epoch 349/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0200 \n",
            "Epoch 350/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0245 \n",
            "Epoch 351/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0191\n",
            "Epoch 352/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0179\n",
            "Epoch 353/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0229 \n",
            "Epoch 354/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0240 \n",
            "Epoch 355/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0203\n",
            "Epoch 356/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0223 \n",
            "Epoch 357/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0182\n",
            "Epoch 358/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0230 \n",
            "Epoch 359/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0187 \n",
            "Epoch 360/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0193\n",
            "Epoch 361/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0220 \n",
            "Epoch 362/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0187 \n",
            "Epoch 363/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0175 \n",
            "Epoch 364/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0203 \n",
            "Epoch 365/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0231 \n",
            "Epoch 366/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0205 \n",
            "Epoch 367/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0216 \n",
            "Epoch 368/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0206 \n",
            "Epoch 369/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0191 \n",
            "Epoch 370/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0202\n",
            "Epoch 371/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0195\n",
            "Epoch 372/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0213\n",
            "Epoch 373/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0222  \n",
            "Epoch 374/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0160 \n",
            "Epoch 375/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0213\n",
            "Epoch 376/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0191 \n",
            "Epoch 377/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0184 \n",
            "Epoch 378/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0196\n",
            "Epoch 379/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0178  \n",
            "Epoch 380/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0170 \n",
            "Epoch 381/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0200 \n",
            "Epoch 382/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0194\n",
            "Epoch 383/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0170\n",
            "Epoch 384/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0201 \n",
            "Epoch 385/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0167  \n",
            "Epoch 386/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0202\n",
            "Epoch 387/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0166 \n",
            "Epoch 388/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0169\n",
            "Epoch 389/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0216 \n",
            "Epoch 390/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0212 \n",
            "Epoch 391/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0176\n",
            "Epoch 392/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0195 \n",
            "Epoch 393/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0177\n",
            "Epoch 394/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0159 \n",
            "Epoch 395/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0190 \n",
            "Epoch 396/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0200\n",
            "Epoch 397/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0174\n",
            "Epoch 398/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0210\n",
            "Epoch 399/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0194 \n",
            "Epoch 400/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0184\n",
            "Epoch 401/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0187 \n",
            "Epoch 402/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0152 \n",
            "Epoch 403/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0172 \n",
            "Epoch 404/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0183 \n",
            "Epoch 405/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0178 \n",
            "Epoch 406/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0167 \n",
            "Epoch 407/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0167 \n",
            "Epoch 408/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0173 \n",
            "Epoch 409/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0204\n",
            "Epoch 410/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0151 \n",
            "Epoch 411/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0190 \n",
            "Epoch 412/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0162 \n",
            "Epoch 413/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0189 \n",
            "Epoch 414/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0182 \n",
            "Epoch 415/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0153 \n",
            "Epoch 416/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0199 \n",
            "Epoch 417/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0157  \n",
            "Epoch 418/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0187 \n",
            "Epoch 419/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0162 \n",
            "Epoch 420/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0147 \n",
            "Epoch 421/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0156 \n",
            "Epoch 422/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0185 \n",
            "Epoch 423/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0188  \n",
            "Epoch 424/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0205\n",
            "Epoch 425/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0188  \n",
            "Epoch 426/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0154 \n",
            "Epoch 427/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0142\n",
            "Epoch 428/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0198\n",
            "Epoch 429/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0149\n",
            "Epoch 430/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0168\n",
            "Epoch 431/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0158 \n",
            "Epoch 432/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0165 \n",
            "Epoch 433/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0150 \n",
            "Epoch 434/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0148\n",
            "Epoch 435/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0164 \n",
            "Epoch 436/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0180\n",
            "Epoch 437/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0148 \n",
            "Epoch 438/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0141\n",
            "Epoch 439/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0155 \n",
            "Epoch 440/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0168 \n",
            "Epoch 441/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0171\n",
            "Epoch 442/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0146 \n",
            "Epoch 443/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0177\n",
            "Epoch 444/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0160 \n",
            "Epoch 445/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0162 \n",
            "Epoch 446/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0155 \n",
            "Epoch 447/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0186\n",
            "Epoch 448/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0144\n",
            "Epoch 449/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0163 \n",
            "Epoch 450/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0151 \n",
            "Epoch 451/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0140 \n",
            "Epoch 452/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0149\n",
            "Epoch 453/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0151\n",
            "Epoch 454/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0179\n",
            "Epoch 455/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0191\n",
            "Epoch 456/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0174 \n",
            "Epoch 457/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0182\n",
            "Epoch 458/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0155 \n",
            "Epoch 459/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0181 \n",
            "Epoch 460/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0176\n",
            "Epoch 461/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0153 \n",
            "Epoch 462/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0128 \n",
            "Epoch 463/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0140\n",
            "Epoch 464/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0141 \n",
            "Epoch 465/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0137\n",
            "Epoch 466/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0131\n",
            "Epoch 467/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0130\n",
            "Epoch 468/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0140\n",
            "Epoch 469/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0125 \n",
            "Epoch 470/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0139\n",
            "Epoch 471/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0157\n",
            "Epoch 472/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0140 \n",
            "Epoch 473/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0129 \n",
            "Epoch 474/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0141 \n",
            "Epoch 475/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0218 \n",
            "Epoch 476/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0141 \n",
            "Epoch 477/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0180\n",
            "Epoch 478/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0158 \n",
            "Epoch 479/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0129 \n",
            "Epoch 480/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0156 \n",
            "Epoch 481/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0152 \n",
            "Epoch 482/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0132\n",
            "Epoch 483/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0145\n",
            "Epoch 484/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0138 \n",
            "Epoch 485/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0144\n",
            "Epoch 486/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0141\n",
            "Epoch 487/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0118 \n",
            "Epoch 488/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0137 \n",
            "Epoch 489/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0119\n",
            "Epoch 490/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0130\n",
            "Epoch 491/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0133 \n",
            "Epoch 492/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0112 \n",
            "Epoch 493/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0128\n",
            "Epoch 494/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0112\n",
            "Epoch 495/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0129\n",
            "Epoch 496/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0128\n",
            "Epoch 497/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0137  \n",
            "Epoch 498/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0131\n",
            "Epoch 499/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0103\n",
            "Epoch 500/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0135\n",
            "Epoch 501/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0124 \n",
            "Epoch 502/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0123 \n",
            "Epoch 503/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0130\n",
            "Epoch 504/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0125\n",
            "Epoch 505/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0144\n",
            "Epoch 506/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0131 \n",
            "Epoch 507/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0175 \n",
            "Epoch 508/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0137\n",
            "Epoch 509/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0138\n",
            "Epoch 510/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0152 \n",
            "Epoch 511/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0146 \n",
            "Epoch 512/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0158 \n",
            "Epoch 513/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0129\n",
            "Epoch 514/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0150 \n",
            "Epoch 515/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0126\n",
            "Epoch 516/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0132\n",
            "Epoch 517/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0124\n",
            "Epoch 518/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0144 \n",
            "Epoch 519/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0112\n",
            "Epoch 520/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0141 \n",
            "Epoch 521/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0105\n",
            "Epoch 522/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0116\n",
            "Epoch 523/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0118\n",
            "Epoch 524/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0114 \n",
            "Epoch 525/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0118\n",
            "Epoch 526/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0167 \n",
            "Epoch 527/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0137\n",
            "Epoch 528/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0133\n",
            "Epoch 529/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0177\n",
            "Epoch 530/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0132  \n",
            "Epoch 531/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0140\n",
            "Epoch 532/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0149\n",
            "Epoch 533/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0127\n",
            "Epoch 534/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0142\n",
            "Epoch 535/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0125\n",
            "Epoch 536/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0160 \n",
            "Epoch 537/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0139\n",
            "Epoch 538/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0108 \n",
            "Epoch 539/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0134\n",
            "Epoch 540/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0119\n",
            "Epoch 541/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0116 \n",
            "Epoch 542/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0116\n",
            "Epoch 543/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0123\n",
            "Epoch 544/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0120 \n",
            "Epoch 545/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0130 \n",
            "Epoch 546/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0130 \n",
            "Epoch 547/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0129\n",
            "Epoch 548/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0112 \n",
            "Epoch 549/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0112\n",
            "Epoch 550/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0123 \n",
            "Epoch 551/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0107 \n",
            "Epoch 552/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0110\n",
            "Epoch 553/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0115\n",
            "Epoch 554/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0109 \n",
            "Epoch 555/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0153 \n",
            "Epoch 556/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0109\n",
            "Epoch 557/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0128 \n",
            "Epoch 558/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0114\n",
            "Epoch 559/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0119 \n",
            "Epoch 560/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0103 \n",
            "Epoch 561/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0101 \n",
            "Epoch 562/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0119\n",
            "Epoch 563/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0126 \n",
            "Epoch 564/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0108 \n",
            "Epoch 565/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0100 \n",
            "Epoch 566/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0107 \n",
            "Epoch 567/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0099\n",
            "Epoch 568/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0149 \n",
            "Epoch 569/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0102\n",
            "Epoch 570/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0097 \n",
            "Epoch 571/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0135 \n",
            "Epoch 572/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0107  \n",
            "Epoch 573/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0107 \n",
            "Epoch 574/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0094 \n",
            "Epoch 575/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0129 \n",
            "Epoch 576/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0118\n",
            "Epoch 577/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0123  \n",
            "Epoch 578/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0103 \n",
            "Epoch 579/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0102 \n",
            "Epoch 580/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0133\n",
            "Epoch 581/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0129 \n",
            "Epoch 582/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0129\n",
            "Epoch 583/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0102  \n",
            "Epoch 584/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0121\n",
            "Epoch 585/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0091  \n",
            "Epoch 586/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0119  \n",
            "Epoch 587/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0138 \n",
            "Epoch 588/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0098 \n",
            "Epoch 589/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0118\n",
            "Epoch 590/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0123 \n",
            "Epoch 591/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0116 \n",
            "Epoch 592/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0116  \n",
            "Epoch 593/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0150 \n",
            "Epoch 594/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0117 \n",
            "Epoch 595/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0112\n",
            "Epoch 596/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0105\n",
            "Epoch 597/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0126\n",
            "Epoch 598/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0118\n",
            "Epoch 599/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0117  \n",
            "Epoch 600/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0100 \n",
            "Epoch 601/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0122\n",
            "Epoch 602/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0114 \n",
            "Epoch 603/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0114\n",
            "Epoch 604/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0104\n",
            "Epoch 605/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0111\n",
            "Epoch 606/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0111\n",
            "Epoch 607/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0097\n",
            "Epoch 608/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0105\n",
            "Epoch 609/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0111\n",
            "Epoch 610/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0110 \n",
            "Epoch 611/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0091\n",
            "Epoch 612/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0104 \n",
            "Epoch 613/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0089\n",
            "Epoch 614/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0099 \n",
            "Epoch 615/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0098\n",
            "Epoch 616/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0121\n",
            "Epoch 617/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0095 \n",
            "Epoch 618/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0086\n",
            "Epoch 619/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0090\n",
            "Epoch 620/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0095 \n",
            "Epoch 621/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0090\n",
            "Epoch 622/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0098 \n",
            "Epoch 623/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0109\n",
            "Epoch 624/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0095 \n",
            "Epoch 625/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0090\n",
            "Epoch 626/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0101  \n",
            "Epoch 627/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0087 \n",
            "Epoch 628/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0083\n",
            "Epoch 629/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0083\n",
            "Epoch 630/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0106 \n",
            "Epoch 631/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0095\n",
            "Epoch 632/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0108 \n",
            "Epoch 633/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0080 \n",
            "Epoch 634/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0094  \n",
            "Epoch 635/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0105\n",
            "Epoch 636/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0104\n",
            "Epoch 637/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0086  \n",
            "Epoch 638/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0124\n",
            "Epoch 639/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0133 \n",
            "Epoch 640/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0081\n",
            "Epoch 641/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0094  \n",
            "Epoch 642/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0090 \n",
            "Epoch 643/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0103\n",
            "Epoch 644/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0093\n",
            "Epoch 645/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0097\n",
            "Epoch 646/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0097\n",
            "Epoch 647/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0097 \n",
            "Epoch 648/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0107\n",
            "Epoch 649/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0100\n",
            "Epoch 650/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0091\n",
            "Epoch 651/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0101 \n",
            "Epoch 652/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0085 \n",
            "Epoch 653/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0085 \n",
            "Epoch 654/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0089\n",
            "Epoch 655/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0102\n",
            "Epoch 656/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0088 \n",
            "Epoch 657/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0094\n",
            "Epoch 658/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0111 \n",
            "Epoch 659/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0113\n",
            "Epoch 660/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0090 \n",
            "Epoch 661/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0116 \n",
            "Epoch 662/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0095\n",
            "Epoch 663/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0085\n",
            "Epoch 664/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0087  \n",
            "Epoch 665/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0086\n",
            "Epoch 666/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0115\n",
            "Epoch 667/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0095 \n",
            "Epoch 668/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0094 \n",
            "Epoch 669/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0090\n",
            "Epoch 670/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0103\n",
            "Epoch 671/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0081  \n",
            "Epoch 672/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0083 \n",
            "Epoch 673/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0078\n",
            "Epoch 674/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0098\n",
            "Epoch 675/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0096\n",
            "Epoch 676/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0096\n",
            "Epoch 677/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0084\n",
            "Epoch 678/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0088\n",
            "Epoch 679/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0094\n",
            "Epoch 680/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0092 \n",
            "Epoch 681/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0074\n",
            "Epoch 682/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0114\n",
            "Epoch 683/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0098\n",
            "Epoch 684/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0076 \n",
            "Epoch 685/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0078 \n",
            "Epoch 686/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0106\n",
            "Epoch 687/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0088\n",
            "Epoch 688/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0085 \n",
            "Epoch 689/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0079 \n",
            "Epoch 690/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0092 \n",
            "Epoch 691/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0082 \n",
            "Epoch 692/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0090 \n",
            "Epoch 693/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0099 \n",
            "Epoch 694/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0079\n",
            "Epoch 695/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0082 \n",
            "Epoch 696/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0090 \n",
            "Epoch 697/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0094 \n",
            "Epoch 698/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0082\n",
            "Epoch 699/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0083 \n",
            "Epoch 700/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0077\n",
            "Epoch 701/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0083 \n",
            "Epoch 702/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0079\n",
            "Epoch 703/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0076 \n",
            "Epoch 704/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0086 \n",
            "Epoch 705/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0072 \n",
            "Epoch 706/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0072\n",
            "Epoch 707/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0070\n",
            "Epoch 708/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0083 \n",
            "Epoch 709/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0075\n",
            "Epoch 710/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0092\n",
            "Epoch 711/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0072 \n",
            "Epoch 712/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0075 \n",
            "Epoch 713/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0070 \n",
            "Epoch 714/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0089  \n",
            "Epoch 715/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0082\n",
            "Epoch 716/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0092 \n",
            "Epoch 717/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0090\n",
            "Epoch 718/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0076  \n",
            "Epoch 719/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0075\n",
            "Epoch 720/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0075  \n",
            "Epoch 721/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0071\n",
            "Epoch 722/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0085 \n",
            "Epoch 723/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0085\n",
            "Epoch 724/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0085 \n",
            "Epoch 725/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0100\n",
            "Epoch 726/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0073 \n",
            "Epoch 727/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0080\n",
            "Epoch 728/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0100  \n",
            "Epoch 729/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0073\n",
            "Epoch 730/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0076\n",
            "Epoch 731/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0072\n",
            "Epoch 732/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0100\n",
            "Epoch 733/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0149\n",
            "Epoch 734/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0081\n",
            "Epoch 735/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0089\n",
            "Epoch 736/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0095 \n",
            "Epoch 737/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0081 \n",
            "Epoch 738/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0089 \n",
            "Epoch 739/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0080\n",
            "Epoch 740/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0081 \n",
            "Epoch 741/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0085\n",
            "Epoch 742/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0070\n",
            "Epoch 743/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0133\n",
            "Epoch 744/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0081 \n",
            "Epoch 745/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0076\n",
            "Epoch 746/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0072 \n",
            "Epoch 747/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0075\n",
            "Epoch 748/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0092 \n",
            "Epoch 749/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0082\n",
            "Epoch 750/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0086\n",
            "Epoch 751/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0090\n",
            "Epoch 752/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0097\n",
            "Epoch 753/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0077 \n",
            "Epoch 754/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0108  \n",
            "Epoch 755/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0068\n",
            "Epoch 756/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0088 \n",
            "Epoch 757/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0071\n",
            "Epoch 758/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0078\n",
            "Epoch 759/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0070 \n",
            "Epoch 760/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0069\n",
            "Epoch 761/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0063 \n",
            "Epoch 762/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0088\n",
            "Epoch 763/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0066  \n",
            "Epoch 764/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0069\n",
            "Epoch 765/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0068 \n",
            "Epoch 766/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0075\n",
            "Epoch 767/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0118\n",
            "Epoch 768/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0066 \n",
            "Epoch 769/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0079\n",
            "Epoch 770/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0085 \n",
            "Epoch 771/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0077\n",
            "Epoch 772/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0080\n",
            "Epoch 773/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0068 \n",
            "Epoch 774/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0073 \n",
            "Epoch 775/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0082\n",
            "Epoch 776/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0070\n",
            "Epoch 777/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0079 \n",
            "Epoch 778/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0082\n",
            "Epoch 779/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0073\n",
            "Epoch 780/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0074\n",
            "Epoch 781/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0075 \n",
            "Epoch 782/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0069 \n",
            "Epoch 783/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0071\n",
            "Epoch 784/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0069\n",
            "Epoch 785/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0082\n",
            "Epoch 786/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0070\n",
            "Epoch 787/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0067\n",
            "Epoch 788/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0062 \n",
            "Epoch 789/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0159 \n",
            "Epoch 790/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0078 \n",
            "Epoch 791/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0096\n",
            "Epoch 792/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0072  \n",
            "Epoch 793/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0068  \n",
            "Epoch 794/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0074\n",
            "Epoch 795/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0086  \n",
            "Epoch 796/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0077\n",
            "Epoch 797/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0088\n",
            "Epoch 798/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0108\n",
            "Epoch 799/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0082 \n",
            "Epoch 800/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0070 \n",
            "Epoch 801/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0082 \n",
            "Epoch 802/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0077 \n",
            "Epoch 803/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0083 \n",
            "Epoch 804/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0093\n",
            "Epoch 805/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0086 \n",
            "Epoch 806/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0088 \n",
            "Epoch 807/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0073 \n",
            "Epoch 808/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0102 \n",
            "Epoch 809/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0073  \n",
            "Epoch 810/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0069\n",
            "Epoch 811/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0087 \n",
            "Epoch 812/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0069 \n",
            "Epoch 813/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0069\n",
            "Epoch 814/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0068  \n",
            "Epoch 815/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0085\n",
            "Epoch 816/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0072 \n",
            "Epoch 817/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0065\n",
            "Epoch 818/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0074 \n",
            "Epoch 819/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0069 \n",
            "Epoch 820/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0059 \n",
            "Epoch 821/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0069 \n",
            "Epoch 822/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0079 \n",
            "Epoch 823/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0086 \n",
            "Epoch 824/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0057\n",
            "Epoch 825/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0070\n",
            "Epoch 826/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0067\n",
            "Epoch 827/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0069\n",
            "Epoch 828/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0059 \n",
            "Epoch 829/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0082\n",
            "Epoch 830/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0063 \n",
            "Epoch 831/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0065\n",
            "Epoch 832/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0068 \n",
            "Epoch 833/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0053\n",
            "Epoch 834/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0073\n",
            "Epoch 835/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0077 \n",
            "Epoch 836/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0084\n",
            "Epoch 837/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0069\n",
            "Epoch 838/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0063\n",
            "Epoch 839/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0066 \n",
            "Epoch 840/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0054\n",
            "Epoch 841/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0064 \n",
            "Epoch 842/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0063 \n",
            "Epoch 843/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0060\n",
            "Epoch 844/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0061\n",
            "Epoch 845/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0076\n",
            "Epoch 846/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0063\n",
            "Epoch 847/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0052 \n",
            "Epoch 848/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0070 \n",
            "Epoch 849/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0073\n",
            "Epoch 850/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0053 \n",
            "Epoch 851/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0184 \n",
            "Epoch 852/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0064 \n",
            "Epoch 853/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0069\n",
            "Epoch 854/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0093 \n",
            "Epoch 855/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0087\n",
            "Epoch 856/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0072  \n",
            "Epoch 857/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0078\n",
            "Epoch 858/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0088\n",
            "Epoch 859/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0071  \n",
            "Epoch 860/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0068\n",
            "Epoch 861/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0102  \n",
            "Epoch 862/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0073  \n",
            "Epoch 863/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0082 \n",
            "Epoch 864/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0071 \n",
            "Epoch 865/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0073\n",
            "Epoch 866/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0086\n",
            "Epoch 867/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0084 \n",
            "Epoch 868/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0071\n",
            "Epoch 869/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0090 \n",
            "Epoch 870/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0088\n",
            "Epoch 871/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0070\n",
            "Epoch 872/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0065 \n",
            "Epoch 873/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0066  \n",
            "Epoch 874/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0066 \n",
            "Epoch 875/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0067 \n",
            "Epoch 876/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0063 \n",
            "Epoch 877/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0060\n",
            "Epoch 878/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0076\n",
            "Epoch 879/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0060\n",
            "Epoch 880/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0066\n",
            "Epoch 881/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0075\n",
            "Epoch 882/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0075 \n",
            "Epoch 883/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0076\n",
            "Epoch 884/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0064 \n",
            "Epoch 885/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0061\n",
            "Epoch 886/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0058 \n",
            "Epoch 887/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0064\n",
            "Epoch 888/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0078 \n",
            "Epoch 889/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0054 \n",
            "Epoch 890/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0059 \n",
            "Epoch 891/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0059 \n",
            "Epoch 892/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0053 \n",
            "Epoch 893/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0056  \n",
            "Epoch 894/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0064 \n",
            "Epoch 895/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0071\n",
            "Epoch 896/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0055 \n",
            "Epoch 897/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0057\n",
            "Epoch 898/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0060\n",
            "Epoch 899/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0060\n",
            "Epoch 900/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0056 \n",
            "Epoch 901/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0062\n",
            "Epoch 902/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0055 \n",
            "Epoch 903/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0064  \n",
            "Epoch 904/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0051\n",
            "Epoch 905/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0067 \n",
            "Epoch 906/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0057\n",
            "Epoch 907/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0063\n",
            "Epoch 908/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0059 \n",
            "Epoch 909/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0051\n",
            "Epoch 910/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0072 \n",
            "Epoch 911/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0055\n",
            "Epoch 912/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0051 \n",
            "Epoch 913/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0061 \n",
            "Epoch 914/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0050 \n",
            "Epoch 915/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0069\n",
            "Epoch 916/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0049 \n",
            "Epoch 917/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0067 \n",
            "Epoch 918/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0063\n",
            "Epoch 919/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0068 \n",
            "Epoch 920/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0048 \n",
            "Epoch 921/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0060 \n",
            "Epoch 922/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0054\n",
            "Epoch 923/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0052\n",
            "Epoch 924/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0071 \n",
            "Epoch 925/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0062 \n",
            "Epoch 926/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0060 \n",
            "Epoch 927/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0067\n",
            "Epoch 928/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0049\n",
            "Epoch 929/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0051\n",
            "Epoch 930/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0088 \n",
            "Epoch 931/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0049 \n",
            "Epoch 932/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0065 \n",
            "Epoch 933/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0059\n",
            "Epoch 934/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0067 \n",
            "Epoch 935/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0064\n",
            "Epoch 936/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0049 \n",
            "Epoch 937/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0068\n",
            "Epoch 938/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0053 \n",
            "Epoch 939/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0055\n",
            "Epoch 940/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0058 \n",
            "Epoch 941/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0052\n",
            "Epoch 942/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0058\n",
            "Epoch 943/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0055\n",
            "Epoch 944/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0060  \n",
            "Epoch 945/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0073 \n",
            "Epoch 946/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0060 \n",
            "Epoch 947/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0064\n",
            "Epoch 948/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0056 \n",
            "Epoch 949/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0046\n",
            "Epoch 950/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0081 \n",
            "Epoch 951/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0052 \n",
            "Epoch 952/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0066 \n",
            "Epoch 953/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0071\n",
            "Epoch 954/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0060\n",
            "Epoch 955/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0056 \n",
            "Epoch 956/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0056 \n",
            "Epoch 957/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0052\n",
            "Epoch 958/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0055\n",
            "Epoch 959/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0061 \n",
            "Epoch 960/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0054 \n",
            "Epoch 961/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0068 \n",
            "Epoch 962/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0061\n",
            "Epoch 963/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0057 \n",
            "Epoch 964/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0065\n",
            "Epoch 965/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0051\n",
            "Epoch 966/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0047\n",
            "Epoch 967/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0064 \n",
            "Epoch 968/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0048\n",
            "Epoch 969/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0051 \n",
            "Epoch 970/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0054\n",
            "Epoch 971/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0047\n",
            "Epoch 972/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0052\n",
            "Epoch 973/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0046\n",
            "Epoch 974/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0041 \n",
            "Epoch 975/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0052 \n",
            "Epoch 976/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0048 \n",
            "Epoch 977/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0046 \n",
            "Epoch 978/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0048 \n",
            "Epoch 979/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0042\n",
            "Epoch 980/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0053 \n",
            "Epoch 981/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0043 \n",
            "Epoch 982/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0057\n",
            "Epoch 983/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0048\n",
            "Epoch 984/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0054 \n",
            "Epoch 985/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0043\n",
            "Epoch 986/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0043 \n",
            "Epoch 987/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0045  \n",
            "Epoch 988/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0047 \n",
            "Epoch 989/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0052 \n",
            "Epoch 990/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0062 \n",
            "Epoch 991/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0049 \n",
            "Epoch 992/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0046\n",
            "Epoch 993/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0057 \n",
            "Epoch 994/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0059\n",
            "Epoch 995/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0044 \n",
            "Epoch 996/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0050 \n",
            "Epoch 997/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0053 \n",
            "Epoch 998/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0049  \n",
            "Epoch 999/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0049\n",
            "Epoch 1000/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0070\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n"
          ]
        }
      ],
      "source": [
        "X_train_values = X_train.values\n",
        "X_test_values = X_test.values\n",
        "#X_valid_values = X_valid.values\n",
        "\n",
        "# Reshape input data to 3D [samples, timesteps, features] so it'll fit the LSTM layer\n",
        "# Use a timestep of 1.\n",
        "X_train_reshaped = np.reshape(X_train_values, (X_train_values.shape[0], 1, X_train_values.shape[1]))\n",
        "X_test_reshaped = np.reshape(X_test_values, (X_test_values.shape[0], 1, X_test_values.shape[1]))\n",
        "#X_valid_reshaped = np.reshape(X_valid_values, (X_valid_values.shape[0], 1, X_valid_values.shape[1]))\n",
        "\n",
        "# LSTM model\n",
        "lstm = Sequential()\n",
        "lstm.add(LSTM(100,return_sequences=True,activation='selu',input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
        "lstm.add(Dropout(0.7))\n",
        "\n",
        "lstm.add(LSTM(50,activation='selu',return_sequences=False,kernel_regularizer=l2(0.01)))\n",
        "lstm.add(Dropout(0.7))\n",
        "lstm.add(Dense(1,activation='sigmoid'))  # Prediction of the next closing value\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "lstm.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "lstm.fit(X_train_reshaped, y_train, epochs=1000, batch_size=32, verbose=1)\n",
        "\n",
        "# Predicting and inverse transforming the predictions\n",
        "y_pred_train_lstm = lstm.predict(X_train_reshaped)\n",
        "y_pred_test_lstm = lstm.predict(X_test_reshaped)\n",
        "\n",
        "#y_pred = scaler.inverse_transform(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "37f7966e-0c00-4050-9c4b-c955c14561b6",
      "metadata": {
        "id": "37f7966e-0c00-4050-9c4b-c955c14561b6",
        "outputId": "67162aff-3835-4644-a365-455231b08038",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train accuracy 1.0\n",
            "test accuracy 0.5714285714285714\n"
          ]
        }
      ],
      "source": [
        "y_pred_train_lstm = np.squeeze(y_pred_train_lstm)\n",
        "y_pred_test_lstm = np.squeeze(y_pred_test_lstm)\n",
        "y_pred_train_lstm = (y_pred_train_lstm > 0.5).astype(int)\n",
        "acc_train=accuracy_score(y_train,y_pred_train_lstm)\n",
        "print('train accuracy',acc_train)\n",
        "y_pred_test_lstm = (y_pred_test_lstm > 0.5).astype(int)\n",
        "acc_test=accuracy_score(y_test,y_pred_test_lstm)\n",
        "print('test accuracy',acc_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "59673cad-837a-484c-9ccf-a7fda0957464",
      "metadata": {
        "id": "59673cad-837a-484c-9ccf-a7fda0957464",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "20bb737f-1c01-47aa-afd2-b96eaf5f971a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 19ms/step - loss: 7.6569\n",
            "Epoch 2/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 7.1342\n",
            "Epoch 3/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.6363 \n",
            "Epoch 4/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.1714 \n",
            "Epoch 5/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 5.7344\n",
            "Epoch 6/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 5.3194\n",
            "Epoch 7/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.9405 \n",
            "Epoch 8/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 4.5835\n",
            "Epoch 9/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.2508\n",
            "Epoch 10/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.9385\n",
            "Epoch 11/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6652\n",
            "Epoch 12/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.3994 \n",
            "Epoch 13/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 3.1493\n",
            "Epoch 14/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.9271\n",
            "Epoch 15/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.7309\n",
            "Epoch 16/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.5320 \n",
            "Epoch 17/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.3587  \n",
            "Epoch 18/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.1992\n",
            "Epoch 19/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.0557\n",
            "Epoch 20/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.9301\n",
            "Epoch 21/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.8021\n",
            "Epoch 22/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6962\n",
            "Epoch 23/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.5836 \n",
            "Epoch 24/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5033\n",
            "Epoch 25/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4210 \n",
            "Epoch 26/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.3422 \n",
            "Epoch 27/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.2759\n",
            "Epoch 28/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.1862\n",
            "Epoch 29/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.1494 \n",
            "Epoch 30/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.0944\n",
            "Epoch 31/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.0548 \n",
            "Epoch 32/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.9969 \n",
            "Epoch 33/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.9665 \n",
            "Epoch 34/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.9161 \n",
            "Epoch 35/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.8875\n",
            "Epoch 36/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.8643 \n",
            "Epoch 37/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.8435\n",
            "Epoch 38/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.8038 \n",
            "Epoch 39/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.7767 \n",
            "Epoch 40/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.7527\n",
            "Epoch 41/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.7453\n",
            "Epoch 42/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.7201 \n",
            "Epoch 43/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.7356\n",
            "Epoch 44/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.7092\n",
            "Epoch 45/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.6981 \n",
            "Epoch 46/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.6869 \n",
            "Epoch 47/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6635\n",
            "Epoch 48/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6514\n",
            "Epoch 49/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6316\n",
            "Epoch 50/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.6360\n",
            "Epoch 51/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6410 \n",
            "Epoch 52/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5997\n",
            "Epoch 53/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6229 \n",
            "Epoch 54/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5985 \n",
            "Epoch 55/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5929\n",
            "Epoch 56/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5584\n",
            "Epoch 57/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5481 \n",
            "Epoch 58/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5661 \n",
            "Epoch 59/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.5663\n",
            "Epoch 60/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5595 \n",
            "Epoch 61/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5405\n",
            "Epoch 62/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.5389\n",
            "Epoch 63/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.5614 \n",
            "Epoch 64/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5305\n",
            "Epoch 65/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.5113\n",
            "Epoch 66/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5136\n",
            "Epoch 67/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5240 \n",
            "Epoch 68/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5021\n",
            "Epoch 69/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.5069\n",
            "Epoch 70/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5128 \n",
            "Epoch 71/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5055\n",
            "Epoch 72/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4986 \n",
            "Epoch 73/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5036\n",
            "Epoch 74/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4935\n",
            "Epoch 75/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.4778 \n",
            "Epoch 76/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4896 \n",
            "Epoch 77/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4766\n",
            "Epoch 78/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4743\n",
            "Epoch 79/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4806\n",
            "Epoch 80/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4818\n",
            "Epoch 81/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4837\n",
            "Epoch 82/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4744\n",
            "Epoch 83/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4640\n",
            "Epoch 84/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4739 \n",
            "Epoch 85/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4627\n",
            "Epoch 86/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4553\n",
            "Epoch 87/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4785\n",
            "Epoch 88/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.4583\n",
            "Epoch 89/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4481\n",
            "Epoch 90/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4624\n",
            "Epoch 91/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4553\n",
            "Epoch 92/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4498\n",
            "Epoch 93/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4599\n",
            "Epoch 94/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4547\n",
            "Epoch 95/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4540\n",
            "Epoch 96/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4423 \n",
            "Epoch 97/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.4439\n",
            "Epoch 98/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4385 \n",
            "Epoch 99/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4481\n",
            "Epoch 100/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4442\n",
            "Epoch 101/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4436\n",
            "Epoch 102/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4321\n",
            "Epoch 103/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4303\n",
            "Epoch 104/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4384\n",
            "Epoch 105/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4381\n",
            "Epoch 106/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4324\n",
            "Epoch 107/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4412\n",
            "Epoch 108/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4420\n",
            "Epoch 109/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4393\n",
            "Epoch 110/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4211 \n",
            "Epoch 111/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4271 \n",
            "Epoch 112/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4209\n",
            "Epoch 113/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4368\n",
            "Epoch 114/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4302\n",
            "Epoch 115/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4284\n",
            "Epoch 116/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4267 \n",
            "Epoch 117/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4231\n",
            "Epoch 118/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4229\n",
            "Epoch 119/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4191\n",
            "Epoch 120/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4208\n",
            "Epoch 121/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4259\n",
            "Epoch 122/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4143\n",
            "Epoch 123/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4195 \n",
            "Epoch 124/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4169\n",
            "Epoch 125/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.4189\n",
            "Epoch 126/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4044\n",
            "Epoch 127/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4137\n",
            "Epoch 128/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4098 \n",
            "Epoch 129/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4111\n",
            "Epoch 130/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4044\n",
            "Epoch 131/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4040\n",
            "Epoch 132/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4091\n",
            "Epoch 133/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4057\n",
            "Epoch 134/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4103\n",
            "Epoch 135/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4033\n",
            "Epoch 136/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4025 \n",
            "Epoch 137/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4092\n",
            "Epoch 138/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3938 \n",
            "Epoch 139/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4076\n",
            "Epoch 140/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3996\n",
            "Epoch 141/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3953\n",
            "Epoch 142/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3989\n",
            "Epoch 143/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3934 \n",
            "Epoch 144/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3939\n",
            "Epoch 145/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4047\n",
            "Epoch 146/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3995 \n",
            "Epoch 147/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3905 \n",
            "Epoch 148/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3887 \n",
            "Epoch 149/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3949\n",
            "Epoch 150/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3935\n",
            "Epoch 151/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3895\n",
            "Epoch 152/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3875\n",
            "Epoch 153/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3848\n",
            "Epoch 154/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3834 \n",
            "Epoch 155/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3819\n",
            "Epoch 156/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3836 \n",
            "Epoch 157/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3927 \n",
            "Epoch 158/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3810 \n",
            "Epoch 159/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3768\n",
            "Epoch 160/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3887\n",
            "Epoch 161/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3764\n",
            "Epoch 162/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3825 \n",
            "Epoch 163/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3774\n",
            "Epoch 164/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3759\n",
            "Epoch 165/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3832\n",
            "Epoch 166/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3730 \n",
            "Epoch 167/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3785 \n",
            "Epoch 168/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3842\n",
            "Epoch 169/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3792\n",
            "Epoch 170/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3747 \n",
            "Epoch 171/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3725\n",
            "Epoch 172/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3756 \n",
            "Epoch 173/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3722 \n",
            "Epoch 174/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3648\n",
            "Epoch 175/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3781\n",
            "Epoch 176/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.3714\n",
            "Epoch 177/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3676\n",
            "Epoch 178/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.3712\n",
            "Epoch 179/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3708\n",
            "Epoch 180/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3625 \n",
            "Epoch 181/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3659\n",
            "Epoch 182/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3665 \n",
            "Epoch 183/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3651 \n",
            "Epoch 184/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3665\n",
            "Epoch 185/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3573\n",
            "Epoch 186/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3630\n",
            "Epoch 187/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.3540\n",
            "Epoch 188/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3558\n",
            "Epoch 189/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3501\n",
            "Epoch 190/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3581 \n",
            "Epoch 191/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3620\n",
            "Epoch 192/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3625\n",
            "Epoch 193/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3529\n",
            "Epoch 194/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3570\n",
            "Epoch 195/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3578 \n",
            "Epoch 196/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3528\n",
            "Epoch 197/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3534\n",
            "Epoch 198/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3579\n",
            "Epoch 199/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3564\n",
            "Epoch 200/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3427\n",
            "Epoch 201/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3467\n",
            "Epoch 202/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3523\n",
            "Epoch 203/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3483 \n",
            "Epoch 204/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3410 \n",
            "Epoch 205/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3450\n",
            "Epoch 206/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3438\n",
            "Epoch 207/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3416 \n",
            "Epoch 208/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3462\n",
            "Epoch 209/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3451\n",
            "Epoch 210/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3401\n",
            "Epoch 211/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3440\n",
            "Epoch 212/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3435 \n",
            "Epoch 213/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3384\n",
            "Epoch 214/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3416 \n",
            "Epoch 215/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3402 \n",
            "Epoch 216/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3385\n",
            "Epoch 217/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3417 \n",
            "Epoch 218/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3399\n",
            "Epoch 219/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3432 \n",
            "Epoch 220/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3366\n",
            "Epoch 221/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3384\n",
            "Epoch 222/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3367\n",
            "Epoch 223/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3354 \n",
            "Epoch 224/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3421\n",
            "Epoch 225/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3402\n",
            "Epoch 226/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3377\n",
            "Epoch 227/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.3290\n",
            "Epoch 228/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3342\n",
            "Epoch 229/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3316\n",
            "Epoch 230/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3297\n",
            "Epoch 231/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3308\n",
            "Epoch 232/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3356 \n",
            "Epoch 233/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3309 \n",
            "Epoch 234/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3254\n",
            "Epoch 235/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3289 \n",
            "Epoch 236/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3223\n",
            "Epoch 237/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3256\n",
            "Epoch 238/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3250\n",
            "Epoch 239/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3263 \n",
            "Epoch 240/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3231\n",
            "Epoch 241/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3210\n",
            "Epoch 242/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3223\n",
            "Epoch 243/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3164\n",
            "Epoch 244/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3204\n",
            "Epoch 245/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3214\n",
            "Epoch 246/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3215 \n",
            "Epoch 247/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3145 \n",
            "Epoch 248/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3212\n",
            "Epoch 249/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3183\n",
            "Epoch 250/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3125\n",
            "Epoch 251/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3211 \n",
            "Epoch 252/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3121\n",
            "Epoch 253/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3142\n",
            "Epoch 254/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3101\n",
            "Epoch 255/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3161\n",
            "Epoch 256/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3104 \n",
            "Epoch 257/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3146 \n",
            "Epoch 258/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3131 \n",
            "Epoch 259/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3118\n",
            "Epoch 260/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3150\n",
            "Epoch 261/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3098 \n",
            "Epoch 262/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3137\n",
            "Epoch 263/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3028 \n",
            "Epoch 264/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3077\n",
            "Epoch 265/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3057\n",
            "Epoch 266/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3096\n",
            "Epoch 267/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.3065 \n",
            "Epoch 268/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3008\n",
            "Epoch 269/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3052\n",
            "Epoch 270/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3053 \n",
            "Epoch 271/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3071\n",
            "Epoch 272/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3047\n",
            "Epoch 273/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3046\n",
            "Epoch 274/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3038 \n",
            "Epoch 275/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2965\n",
            "Epoch 276/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2980 \n",
            "Epoch 277/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2971 \n",
            "Epoch 278/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2981\n",
            "Epoch 279/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3007\n",
            "Epoch 280/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2939\n",
            "Epoch 281/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2955\n",
            "Epoch 282/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3005\n",
            "Epoch 283/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2946\n",
            "Epoch 284/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3000 \n",
            "Epoch 285/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2953 \n",
            "Epoch 286/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2984\n",
            "Epoch 287/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2906\n",
            "Epoch 288/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2962\n",
            "Epoch 289/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2893\n",
            "Epoch 290/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2850\n",
            "Epoch 291/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2921 \n",
            "Epoch 292/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2901\n",
            "Epoch 293/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2894\n",
            "Epoch 294/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2851\n",
            "Epoch 295/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2872\n",
            "Epoch 296/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2878 \n",
            "Epoch 297/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2822 \n",
            "Epoch 298/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2877 \n",
            "Epoch 299/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2843 \n",
            "Epoch 300/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2899\n",
            "Epoch 301/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2878\n",
            "Epoch 302/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2856\n",
            "Epoch 303/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2821\n",
            "Epoch 304/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2853\n",
            "Epoch 305/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2861\n",
            "Epoch 306/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2791 \n",
            "Epoch 307/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2791\n",
            "Epoch 308/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2793\n",
            "Epoch 309/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2822\n",
            "Epoch 310/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2794 \n",
            "Epoch 311/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2809 \n",
            "Epoch 312/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2806 \n",
            "Epoch 313/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2834\n",
            "Epoch 314/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2787\n",
            "Epoch 315/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2762 \n",
            "Epoch 316/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2767\n",
            "Epoch 317/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2754 \n",
            "Epoch 318/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2739\n",
            "Epoch 319/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2760 \n",
            "Epoch 320/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2710\n",
            "Epoch 321/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2729 \n",
            "Epoch 322/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2707 \n",
            "Epoch 323/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2712\n",
            "Epoch 324/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2740 \n",
            "Epoch 325/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2711 \n",
            "Epoch 326/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2708 \n",
            "Epoch 327/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2728 \n",
            "Epoch 328/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2736\n",
            "Epoch 329/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2718\n",
            "Epoch 330/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2709 \n",
            "Epoch 331/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2660\n",
            "Epoch 332/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2646\n",
            "Epoch 333/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2690\n",
            "Epoch 334/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2686\n",
            "Epoch 335/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2722\n",
            "Epoch 336/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2624 \n",
            "Epoch 337/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2639\n",
            "Epoch 338/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2668 \n",
            "Epoch 339/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2645\n",
            "Epoch 340/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2697\n",
            "Epoch 341/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2638\n",
            "Epoch 342/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2625\n",
            "Epoch 343/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2607\n",
            "Epoch 344/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2641\n",
            "Epoch 345/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2651\n",
            "Epoch 346/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2609\n",
            "Epoch 347/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2608\n",
            "Epoch 348/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2597\n",
            "Epoch 349/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2621 \n",
            "Epoch 350/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2638 \n",
            "Epoch 351/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2572\n",
            "Epoch 352/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2638\n",
            "Epoch 353/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2593 \n",
            "Epoch 354/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2590\n",
            "Epoch 355/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2589\n",
            "Epoch 356/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2600 \n",
            "Epoch 357/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2543\n",
            "Epoch 358/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2539\n",
            "Epoch 359/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2593\n",
            "Epoch 360/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2540 \n",
            "Epoch 361/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2542 \n",
            "Epoch 362/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2541 \n",
            "Epoch 363/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2538 \n",
            "Epoch 364/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2497\n",
            "Epoch 365/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2539\n",
            "Epoch 366/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2541\n",
            "Epoch 367/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2553\n",
            "Epoch 368/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2508 \n",
            "Epoch 369/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2514\n",
            "Epoch 370/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2546\n",
            "Epoch 371/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2512\n",
            "Epoch 372/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2548\n",
            "Epoch 373/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2493 \n",
            "Epoch 374/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2527 \n",
            "Epoch 375/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2497\n",
            "Epoch 376/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2481\n",
            "Epoch 377/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2484 \n",
            "Epoch 378/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2492\n",
            "Epoch 379/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2513 \n",
            "Epoch 380/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2454\n",
            "Epoch 381/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2515\n",
            "Epoch 382/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2449\n",
            "Epoch 383/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2456\n",
            "Epoch 384/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2465\n",
            "Epoch 385/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2443\n",
            "Epoch 386/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2477\n",
            "Epoch 387/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2413\n",
            "Epoch 388/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2441\n",
            "Epoch 389/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2430\n",
            "Epoch 390/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2440\n",
            "Epoch 391/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2441\n",
            "Epoch 392/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2425\n",
            "Epoch 393/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2479\n",
            "Epoch 394/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2453\n",
            "Epoch 395/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2454\n",
            "Epoch 396/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2439\n",
            "Epoch 397/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2403 \n",
            "Epoch 398/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2381\n",
            "Epoch 399/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2421\n",
            "Epoch 400/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2442\n",
            "Epoch 401/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2373\n",
            "Epoch 402/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2442\n",
            "Epoch 403/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2416\n",
            "Epoch 404/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2387\n",
            "Epoch 405/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2421 \n",
            "Epoch 406/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2392\n",
            "Epoch 407/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2375\n",
            "Epoch 408/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2430 \n",
            "Epoch 409/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2409 \n",
            "Epoch 410/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2399\n",
            "Epoch 411/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2387\n",
            "Epoch 412/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2377\n",
            "Epoch 413/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2341\n",
            "Epoch 414/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2388\n",
            "Epoch 415/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2354 \n",
            "Epoch 416/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2330\n",
            "Epoch 417/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2334\n",
            "Epoch 418/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2358\n",
            "Epoch 419/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2355\n",
            "Epoch 420/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2336\n",
            "Epoch 421/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2343\n",
            "Epoch 422/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2305\n",
            "Epoch 423/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2359 \n",
            "Epoch 424/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2348\n",
            "Epoch 425/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2346\n",
            "Epoch 426/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2331\n",
            "Epoch 427/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2337 \n",
            "Epoch 428/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2365\n",
            "Epoch 429/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2328\n",
            "Epoch 430/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2340\n",
            "Epoch 431/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2326\n",
            "Epoch 432/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2343\n",
            "Epoch 433/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2305\n",
            "Epoch 434/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2343\n",
            "Epoch 435/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2338\n",
            "Epoch 436/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2341\n",
            "Epoch 437/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2345 \n",
            "Epoch 438/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2302 \n",
            "Epoch 439/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2285\n",
            "Epoch 440/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2294\n",
            "Epoch 441/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2285 \n",
            "Epoch 442/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2300\n",
            "Epoch 443/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2327\n",
            "Epoch 444/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2333\n",
            "Epoch 445/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2307 \n",
            "Epoch 446/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2294\n",
            "Epoch 447/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2303 \n",
            "Epoch 448/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2319\n",
            "Epoch 449/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2284\n",
            "Epoch 450/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2278\n",
            "Epoch 451/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2254\n",
            "Epoch 452/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2274 \n",
            "Epoch 453/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2297\n",
            "Epoch 454/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2263 \n",
            "Epoch 455/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2255 \n",
            "Epoch 456/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2267 \n",
            "Epoch 457/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2286\n",
            "Epoch 458/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2273\n",
            "Epoch 459/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2262\n",
            "Epoch 460/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2248\n",
            "Epoch 461/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2245\n",
            "Epoch 462/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2242\n",
            "Epoch 463/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2262\n",
            "Epoch 464/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2273 \n",
            "Epoch 465/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2244\n",
            "Epoch 466/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2257\n",
            "Epoch 467/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2258 \n",
            "Epoch 468/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2241 \n",
            "Epoch 469/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2223 \n",
            "Epoch 470/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2240\n",
            "Epoch 471/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2242\n",
            "Epoch 472/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2268\n",
            "Epoch 473/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2256\n",
            "Epoch 474/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2252\n",
            "Epoch 475/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2257 \n",
            "Epoch 476/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2219\n",
            "Epoch 477/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2222\n",
            "Epoch 478/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2250\n",
            "Epoch 479/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2191\n",
            "Epoch 480/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2253\n",
            "Epoch 481/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2203\n",
            "Epoch 482/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2229\n",
            "Epoch 483/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2237\n",
            "Epoch 484/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2227\n",
            "Epoch 485/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2254\n",
            "Epoch 486/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2195\n",
            "Epoch 487/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2215 \n",
            "Epoch 488/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2208 \n",
            "Epoch 489/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2169 \n",
            "Epoch 490/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2184 \n",
            "Epoch 491/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2190\n",
            "Epoch 492/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2223\n",
            "Epoch 493/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2172\n",
            "Epoch 494/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2181 \n",
            "Epoch 495/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2200 \n",
            "Epoch 496/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2175 \n",
            "Epoch 497/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2196\n",
            "Epoch 498/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2181\n",
            "Epoch 499/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2199\n",
            "Epoch 500/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2189\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 608ms/step"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 113 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7a799ce3b010> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 317ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n"
          ]
        }
      ],
      "source": [
        "bilstm = Sequential()\n",
        "bilstm.add(Bidirectional(LSTM(100, return_sequences=True,kernel_regularizer=l2(0.01)), input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
        "#bilstm.add(Dropout(0.7))\n",
        "bilstm.add(Bidirectional(LSTM(50, return_sequences=False,kernel_regularizer=l2(0.01))))\n",
        "#bilstm.add(Dropout(0.7))\n",
        "bilstm.add(Dense(1, activation='sigmoid',kernel_regularizer=l2(0.01)))  # Binary classification output\n",
        "\n",
        "# Compile the model\n",
        "bilstm.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "bilstm.fit(X_train_reshaped, y_train, epochs=500, batch_size=32, verbose=1)\n",
        "\n",
        "# Predicting\n",
        "y_pred_train_bilstm = bilstm.predict(X_train_reshaped)\n",
        "y_pred_test_bilstm = bilstm.predict(X_test_reshaped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "76247ddc",
      "metadata": {
        "id": "76247ddc"
      },
      "outputs": [],
      "source": [
        "y_pred_train_bilstm = np.squeeze(y_pred_train_bilstm)\n",
        "y_pred_test_bilstm = np.squeeze(y_pred_test_bilstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "fd68f6a0",
      "metadata": {
        "id": "fd68f6a0",
        "outputId": "cc074c90-8da2-488e-c78f-272c04028698",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train accuracy 1.0\n",
            "test accuracy 0.5238095238095238\n"
          ]
        }
      ],
      "source": [
        "y_pred_train_bilstm = (y_pred_train_bilstm > 0.5).astype(int)\n",
        "acc_train=accuracy_score(y_train,y_pred_train_bilstm)\n",
        "print('train accuracy',acc_train)\n",
        "y_pred_test_bilstm = (y_pred_test_bilstm > 0.5).astype(int)\n",
        "acc_test=accuracy_score(y_test,y_pred_test_bilstm)\n",
        "print('test accuracy',acc_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e26bd467-e763-40f3-a335-b5a7602e379e",
      "metadata": {
        "id": "e26bd467-e763-40f3-a335-b5a7602e379e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}