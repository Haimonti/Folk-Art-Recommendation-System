{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install dgl==0.9.0\n",
        "!pip install torch_geometric\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Vt3bE89GnlZ",
        "outputId": "4fa50334-c463-475d-acdf-03c3c257ff5e"
      },
      "id": "1Vt3bE89GnlZ",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: dgl==0.9.0 in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9.0) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9.0) (3.4.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl==0.9.0) (4.66.6)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl==0.9.0) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==0.9.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==0.9.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==0.9.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl==0.9.0) (2024.8.30)\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.11.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d6b0edec-4f1d-4a83-a3e1-418c863dc18c",
      "metadata": {
        "id": "d6b0edec-4f1d-4a83-a3e1-418c863dc18c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import os\n",
        "from natsort import natsorted\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import FastText\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import dgl.data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, Dense,Bidirectional\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "from dgl.nn import GraphConv\n",
        "from IPython.display import FileLink\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch==2.2.0"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TBIGvZ2LcdEr"
      },
      "id": "TBIGvZ2LcdEr",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install dgl torch_geometric tensorflow"
      ],
      "metadata": {
        "id": "yOd8xS9zuuJz",
        "collapsed": true
      },
      "id": "yOd8xS9zuuJz",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7f9cc503-d869-486a-bde1-b0872d692955",
      "metadata": {
        "id": "7f9cc503-d869-486a-bde1-b0872d692955"
      },
      "outputs": [],
      "source": [
        "image_features=pd.read_csv('/Image_features_Densenet.csv')\n",
        "\n",
        "image_features.drop(['Unnamed: 0'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "-8yo3YcaeT4R",
        "outputId": "5b2b4d05-7d1a-45d1-d42f-08db301136bc"
      },
      "id": "-8yo3YcaeT4R",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6  \\\n",
              "0   -2.637099 -0.347802 -1.305130  0.477352  0.222379  2.597939  1.865067   \n",
              "1   -2.798194  3.634745  2.086230  3.196251  1.620699 -0.670871  1.726479   \n",
              "2   -1.739392 -1.241732 -1.321686 -1.510875 -0.779627 -1.543128  1.477094   \n",
              "3   -0.994942  1.074568  1.187057  1.138465 -0.079325 -4.044148  1.202367   \n",
              "4   -1.329136  2.676327  0.291800  0.271295 -0.906031 -1.966462  1.868663   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "101 -0.328603  3.061680  2.256522 -0.424378 -1.868915 -0.350414  0.226009   \n",
              "102 -0.718224  0.687967 -1.267417 -0.670412 -1.567853  1.657080  0.818342   \n",
              "103  0.445232  1.184668  0.467780 -2.138310 -1.128685 -0.514149  0.245567   \n",
              "104 -1.120806  0.495716  0.477714 -1.356863 -0.036207  0.809519 -0.160998   \n",
              "105 -1.746276  1.329017  1.284418 -0.877288 -0.101824  0.200079  0.221383   \n",
              "\n",
              "            7         8         9  ...        84        85        86  \\\n",
              "0   -0.494544  0.243539  0.256078  ...  0.245732  0.019818 -0.246347   \n",
              "1   -0.083495  0.892471  0.753701  ... -0.439034 -0.363662  0.125037   \n",
              "2   -1.396603  0.295762 -0.055510  ... -0.171100  0.180284  0.044292   \n",
              "3   -0.249646 -0.755334  1.255697  ...  0.026375  0.007201 -0.165179   \n",
              "4   -1.244299 -0.616884 -0.451323  ... -0.145272  0.047353 -0.150540   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "101  1.162425  0.603590 -0.217285  ...  0.227745  0.008060 -0.163510   \n",
              "102  0.515184 -0.144439  0.125603  ... -0.044075  0.357937  0.251036   \n",
              "103  2.021899  0.228138 -0.953547  ... -0.011740  0.419290  0.062435   \n",
              "104 -0.731385  1.327201  0.967042  ... -0.069784  0.047647 -0.162943   \n",
              "105 -0.569326  1.123788 -0.238124  ...  0.070366 -0.196414  0.009736   \n",
              "\n",
              "           87        88        89        90        91        92        93  \n",
              "0   -0.334189  0.001933  0.099690  0.005821  0.098615  0.322466 -0.026514  \n",
              "1   -0.009941  0.000198  0.598846  0.399056 -0.448141 -0.133951  0.163589  \n",
              "2    0.000196  0.003606 -0.028537  0.019995  0.014549 -0.207424 -0.134106  \n",
              "3   -0.136537  0.035306 -0.042689 -0.063572 -0.160413  0.073046 -0.025301  \n",
              "4   -0.009131 -0.106808 -0.137046 -0.175852  0.152502  0.045290 -0.086046  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "101  0.093020 -0.086098  0.027652  0.021020  0.002824 -0.078523 -0.032950  \n",
              "102 -0.005687  0.141986 -0.258837  0.109927 -0.115133 -0.061322 -0.033643  \n",
              "103  0.191918  0.126068  0.287123 -0.040200  0.194610 -0.116119  0.169397  \n",
              "104 -0.053178  0.104861  0.008137 -0.253575 -0.041771 -0.019096  0.024908  \n",
              "105  0.119265  0.025462  0.260401  0.209111 -0.092553  0.108953 -0.142083  \n",
              "\n",
              "[106 rows x 94 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-004fe704-c984-406a-9c38-ace42f333a90\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-2.637099</td>\n",
              "      <td>-0.347802</td>\n",
              "      <td>-1.305130</td>\n",
              "      <td>0.477352</td>\n",
              "      <td>0.222379</td>\n",
              "      <td>2.597939</td>\n",
              "      <td>1.865067</td>\n",
              "      <td>-0.494544</td>\n",
              "      <td>0.243539</td>\n",
              "      <td>0.256078</td>\n",
              "      <td>...</td>\n",
              "      <td>0.245732</td>\n",
              "      <td>0.019818</td>\n",
              "      <td>-0.246347</td>\n",
              "      <td>-0.334189</td>\n",
              "      <td>0.001933</td>\n",
              "      <td>0.099690</td>\n",
              "      <td>0.005821</td>\n",
              "      <td>0.098615</td>\n",
              "      <td>0.322466</td>\n",
              "      <td>-0.026514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-2.798194</td>\n",
              "      <td>3.634745</td>\n",
              "      <td>2.086230</td>\n",
              "      <td>3.196251</td>\n",
              "      <td>1.620699</td>\n",
              "      <td>-0.670871</td>\n",
              "      <td>1.726479</td>\n",
              "      <td>-0.083495</td>\n",
              "      <td>0.892471</td>\n",
              "      <td>0.753701</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.439034</td>\n",
              "      <td>-0.363662</td>\n",
              "      <td>0.125037</td>\n",
              "      <td>-0.009941</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.598846</td>\n",
              "      <td>0.399056</td>\n",
              "      <td>-0.448141</td>\n",
              "      <td>-0.133951</td>\n",
              "      <td>0.163589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.739392</td>\n",
              "      <td>-1.241732</td>\n",
              "      <td>-1.321686</td>\n",
              "      <td>-1.510875</td>\n",
              "      <td>-0.779627</td>\n",
              "      <td>-1.543128</td>\n",
              "      <td>1.477094</td>\n",
              "      <td>-1.396603</td>\n",
              "      <td>0.295762</td>\n",
              "      <td>-0.055510</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.171100</td>\n",
              "      <td>0.180284</td>\n",
              "      <td>0.044292</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.003606</td>\n",
              "      <td>-0.028537</td>\n",
              "      <td>0.019995</td>\n",
              "      <td>0.014549</td>\n",
              "      <td>-0.207424</td>\n",
              "      <td>-0.134106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.994942</td>\n",
              "      <td>1.074568</td>\n",
              "      <td>1.187057</td>\n",
              "      <td>1.138465</td>\n",
              "      <td>-0.079325</td>\n",
              "      <td>-4.044148</td>\n",
              "      <td>1.202367</td>\n",
              "      <td>-0.249646</td>\n",
              "      <td>-0.755334</td>\n",
              "      <td>1.255697</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026375</td>\n",
              "      <td>0.007201</td>\n",
              "      <td>-0.165179</td>\n",
              "      <td>-0.136537</td>\n",
              "      <td>0.035306</td>\n",
              "      <td>-0.042689</td>\n",
              "      <td>-0.063572</td>\n",
              "      <td>-0.160413</td>\n",
              "      <td>0.073046</td>\n",
              "      <td>-0.025301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.329136</td>\n",
              "      <td>2.676327</td>\n",
              "      <td>0.291800</td>\n",
              "      <td>0.271295</td>\n",
              "      <td>-0.906031</td>\n",
              "      <td>-1.966462</td>\n",
              "      <td>1.868663</td>\n",
              "      <td>-1.244299</td>\n",
              "      <td>-0.616884</td>\n",
              "      <td>-0.451323</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.145272</td>\n",
              "      <td>0.047353</td>\n",
              "      <td>-0.150540</td>\n",
              "      <td>-0.009131</td>\n",
              "      <td>-0.106808</td>\n",
              "      <td>-0.137046</td>\n",
              "      <td>-0.175852</td>\n",
              "      <td>0.152502</td>\n",
              "      <td>0.045290</td>\n",
              "      <td>-0.086046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>-0.328603</td>\n",
              "      <td>3.061680</td>\n",
              "      <td>2.256522</td>\n",
              "      <td>-0.424378</td>\n",
              "      <td>-1.868915</td>\n",
              "      <td>-0.350414</td>\n",
              "      <td>0.226009</td>\n",
              "      <td>1.162425</td>\n",
              "      <td>0.603590</td>\n",
              "      <td>-0.217285</td>\n",
              "      <td>...</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.008060</td>\n",
              "      <td>-0.163510</td>\n",
              "      <td>0.093020</td>\n",
              "      <td>-0.086098</td>\n",
              "      <td>0.027652</td>\n",
              "      <td>0.021020</td>\n",
              "      <td>0.002824</td>\n",
              "      <td>-0.078523</td>\n",
              "      <td>-0.032950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>-0.718224</td>\n",
              "      <td>0.687967</td>\n",
              "      <td>-1.267417</td>\n",
              "      <td>-0.670412</td>\n",
              "      <td>-1.567853</td>\n",
              "      <td>1.657080</td>\n",
              "      <td>0.818342</td>\n",
              "      <td>0.515184</td>\n",
              "      <td>-0.144439</td>\n",
              "      <td>0.125603</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.044075</td>\n",
              "      <td>0.357937</td>\n",
              "      <td>0.251036</td>\n",
              "      <td>-0.005687</td>\n",
              "      <td>0.141986</td>\n",
              "      <td>-0.258837</td>\n",
              "      <td>0.109927</td>\n",
              "      <td>-0.115133</td>\n",
              "      <td>-0.061322</td>\n",
              "      <td>-0.033643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>0.445232</td>\n",
              "      <td>1.184668</td>\n",
              "      <td>0.467780</td>\n",
              "      <td>-2.138310</td>\n",
              "      <td>-1.128685</td>\n",
              "      <td>-0.514149</td>\n",
              "      <td>0.245567</td>\n",
              "      <td>2.021899</td>\n",
              "      <td>0.228138</td>\n",
              "      <td>-0.953547</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.011740</td>\n",
              "      <td>0.419290</td>\n",
              "      <td>0.062435</td>\n",
              "      <td>0.191918</td>\n",
              "      <td>0.126068</td>\n",
              "      <td>0.287123</td>\n",
              "      <td>-0.040200</td>\n",
              "      <td>0.194610</td>\n",
              "      <td>-0.116119</td>\n",
              "      <td>0.169397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>-1.120806</td>\n",
              "      <td>0.495716</td>\n",
              "      <td>0.477714</td>\n",
              "      <td>-1.356863</td>\n",
              "      <td>-0.036207</td>\n",
              "      <td>0.809519</td>\n",
              "      <td>-0.160998</td>\n",
              "      <td>-0.731385</td>\n",
              "      <td>1.327201</td>\n",
              "      <td>0.967042</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.069784</td>\n",
              "      <td>0.047647</td>\n",
              "      <td>-0.162943</td>\n",
              "      <td>-0.053178</td>\n",
              "      <td>0.104861</td>\n",
              "      <td>0.008137</td>\n",
              "      <td>-0.253575</td>\n",
              "      <td>-0.041771</td>\n",
              "      <td>-0.019096</td>\n",
              "      <td>0.024908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>-1.746276</td>\n",
              "      <td>1.329017</td>\n",
              "      <td>1.284418</td>\n",
              "      <td>-0.877288</td>\n",
              "      <td>-0.101824</td>\n",
              "      <td>0.200079</td>\n",
              "      <td>0.221383</td>\n",
              "      <td>-0.569326</td>\n",
              "      <td>1.123788</td>\n",
              "      <td>-0.238124</td>\n",
              "      <td>...</td>\n",
              "      <td>0.070366</td>\n",
              "      <td>-0.196414</td>\n",
              "      <td>0.009736</td>\n",
              "      <td>0.119265</td>\n",
              "      <td>0.025462</td>\n",
              "      <td>0.260401</td>\n",
              "      <td>0.209111</td>\n",
              "      <td>-0.092553</td>\n",
              "      <td>0.108953</td>\n",
              "      <td>-0.142083</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>106 rows Ã— 94 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-004fe704-c984-406a-9c38-ace42f333a90')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-004fe704-c984-406a-9c38-ace42f333a90 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-004fe704-c984-406a-9c38-ace42f333a90');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-46ecabc3-ae4f-43e2-be61-4a22ef3b5cfd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-46ecabc3-ae4f-43e2-be61-4a22ef3b5cfd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-46ecabc3-ae4f-43e2-be61-4a22ef3b5cfd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_5bab1b67-d934-4378-85e7-81bc2f2f004f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('image_features')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5bab1b67-d934-4378-85e7-81bc2f2f004f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('image_features');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "image_features"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a12d5c34-df37-460e-aea4-c4918baec35f",
      "metadata": {
        "id": "a12d5c34-df37-460e-aea4-c4918baec35f"
      },
      "outputs": [],
      "source": [
        "text_features=pd.read_csv('/Text_features_roberta.csv')\n",
        "\n",
        "# text_features.drop(['Unnamed: 0','index'],axis=1,inplace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "ada2a103-3061-49c3-8620-a84a0e1f7af5",
      "metadata": {
        "id": "ada2a103-3061-49c3-8620-a84a0e1f7af5",
        "outputId": "87a75a95-cae8-45e6-8fb4-97e3a7eac3bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6  \\\n",
              "0   -0.048460  0.084757  0.003497 -0.087329  0.059048 -0.209606 -0.006506   \n",
              "1   -0.065723  0.025389 -0.025804 -0.130403  0.077852 -0.120231 -0.001798   \n",
              "2   -0.045842  0.040817 -0.028311 -0.119993  0.082768 -0.169268 -0.019755   \n",
              "3   -0.049971  0.032408 -0.005414 -0.096241  0.066642 -0.137509 -0.046769   \n",
              "4   -0.045510  0.020915 -0.008356 -0.120176  0.046261 -0.174338 -0.013932   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "101 -0.060480  0.032145  0.005485 -0.084974  0.062038 -0.145933 -0.047886   \n",
              "102 -0.055155 -0.000772 -0.013739 -0.069411  0.043931 -0.236069  0.007162   \n",
              "103 -0.040352 -0.013974 -0.021251 -0.069300  0.054273 -0.207470 -0.014627   \n",
              "104 -0.051738  0.011170 -0.012154 -0.084336  0.054111 -0.143253 -0.028876   \n",
              "105 -0.051058  0.028734 -0.002164 -0.122716  0.081772 -0.161522 -0.033399   \n",
              "\n",
              "            7         8         9  ...       758       759       760  \\\n",
              "0    0.048831  0.017141 -0.048087  ...  0.013604  0.032850 -0.101248   \n",
              "1    0.034838  0.010572 -0.088210  ...  0.007799  0.013520 -0.135819   \n",
              "2    0.051325  0.019362 -0.082979  ...  0.033639  0.019891 -0.131964   \n",
              "3    0.021914  0.060602 -0.076358  ...  0.071187 -0.008804 -0.099127   \n",
              "4    0.018657  0.020323 -0.080896  ...  0.047783 -0.003582 -0.096276   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "101  0.023430 -0.000973 -0.073406  ...  0.071269  0.011389 -0.102351   \n",
              "102  0.073971 -0.005430 -0.089415  ...  0.040675  0.040890 -0.107817   \n",
              "103  0.039035  0.016669 -0.079122  ...  0.059889  0.024202 -0.134411   \n",
              "104  0.015378  0.012340 -0.065018  ...  0.076928  0.016929 -0.126763   \n",
              "105  0.069389 -0.016274 -0.082618  ...  0.073130  0.023664 -0.097147   \n",
              "\n",
              "          761       762       763       764       765       766       767  \n",
              "0   -0.028741 -0.000748  0.088436  0.065536 -0.118125 -0.061766 -0.032498  \n",
              "1   -0.036519 -0.043678  0.078114  0.019443 -0.130293 -0.037107  0.014137  \n",
              "2   -0.049223 -0.001145  0.100791  0.072946 -0.162404 -0.066192  0.011734  \n",
              "3   -0.071892 -0.024793  0.082018  0.093390 -0.118121 -0.078987  0.012262  \n",
              "4   -0.044621 -0.051367  0.058235  0.079637 -0.182815 -0.076388  0.035959  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "101 -0.069919  0.001416  0.078189  0.042280 -0.141668 -0.042697 -0.004684  \n",
              "102 -0.040903 -0.034730  0.079233  0.068175 -0.142191 -0.074767  0.024146  \n",
              "103 -0.076633 -0.022368  0.069342  0.052974 -0.124388 -0.036107  0.016075  \n",
              "104 -0.078424 -0.036591  0.099697  0.058289 -0.105556 -0.050893 -0.016323  \n",
              "105 -0.054556 -0.029842  0.042678  0.047881 -0.115789 -0.055526  0.042692  \n",
              "\n",
              "[106 rows x 768 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-98a1901f-c7f6-446e-8a7b-6e9372d1a35d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.048460</td>\n",
              "      <td>0.084757</td>\n",
              "      <td>0.003497</td>\n",
              "      <td>-0.087329</td>\n",
              "      <td>0.059048</td>\n",
              "      <td>-0.209606</td>\n",
              "      <td>-0.006506</td>\n",
              "      <td>0.048831</td>\n",
              "      <td>0.017141</td>\n",
              "      <td>-0.048087</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013604</td>\n",
              "      <td>0.032850</td>\n",
              "      <td>-0.101248</td>\n",
              "      <td>-0.028741</td>\n",
              "      <td>-0.000748</td>\n",
              "      <td>0.088436</td>\n",
              "      <td>0.065536</td>\n",
              "      <td>-0.118125</td>\n",
              "      <td>-0.061766</td>\n",
              "      <td>-0.032498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.065723</td>\n",
              "      <td>0.025389</td>\n",
              "      <td>-0.025804</td>\n",
              "      <td>-0.130403</td>\n",
              "      <td>0.077852</td>\n",
              "      <td>-0.120231</td>\n",
              "      <td>-0.001798</td>\n",
              "      <td>0.034838</td>\n",
              "      <td>0.010572</td>\n",
              "      <td>-0.088210</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007799</td>\n",
              "      <td>0.013520</td>\n",
              "      <td>-0.135819</td>\n",
              "      <td>-0.036519</td>\n",
              "      <td>-0.043678</td>\n",
              "      <td>0.078114</td>\n",
              "      <td>0.019443</td>\n",
              "      <td>-0.130293</td>\n",
              "      <td>-0.037107</td>\n",
              "      <td>0.014137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.045842</td>\n",
              "      <td>0.040817</td>\n",
              "      <td>-0.028311</td>\n",
              "      <td>-0.119993</td>\n",
              "      <td>0.082768</td>\n",
              "      <td>-0.169268</td>\n",
              "      <td>-0.019755</td>\n",
              "      <td>0.051325</td>\n",
              "      <td>0.019362</td>\n",
              "      <td>-0.082979</td>\n",
              "      <td>...</td>\n",
              "      <td>0.033639</td>\n",
              "      <td>0.019891</td>\n",
              "      <td>-0.131964</td>\n",
              "      <td>-0.049223</td>\n",
              "      <td>-0.001145</td>\n",
              "      <td>0.100791</td>\n",
              "      <td>0.072946</td>\n",
              "      <td>-0.162404</td>\n",
              "      <td>-0.066192</td>\n",
              "      <td>0.011734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.049971</td>\n",
              "      <td>0.032408</td>\n",
              "      <td>-0.005414</td>\n",
              "      <td>-0.096241</td>\n",
              "      <td>0.066642</td>\n",
              "      <td>-0.137509</td>\n",
              "      <td>-0.046769</td>\n",
              "      <td>0.021914</td>\n",
              "      <td>0.060602</td>\n",
              "      <td>-0.076358</td>\n",
              "      <td>...</td>\n",
              "      <td>0.071187</td>\n",
              "      <td>-0.008804</td>\n",
              "      <td>-0.099127</td>\n",
              "      <td>-0.071892</td>\n",
              "      <td>-0.024793</td>\n",
              "      <td>0.082018</td>\n",
              "      <td>0.093390</td>\n",
              "      <td>-0.118121</td>\n",
              "      <td>-0.078987</td>\n",
              "      <td>0.012262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.045510</td>\n",
              "      <td>0.020915</td>\n",
              "      <td>-0.008356</td>\n",
              "      <td>-0.120176</td>\n",
              "      <td>0.046261</td>\n",
              "      <td>-0.174338</td>\n",
              "      <td>-0.013932</td>\n",
              "      <td>0.018657</td>\n",
              "      <td>0.020323</td>\n",
              "      <td>-0.080896</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047783</td>\n",
              "      <td>-0.003582</td>\n",
              "      <td>-0.096276</td>\n",
              "      <td>-0.044621</td>\n",
              "      <td>-0.051367</td>\n",
              "      <td>0.058235</td>\n",
              "      <td>0.079637</td>\n",
              "      <td>-0.182815</td>\n",
              "      <td>-0.076388</td>\n",
              "      <td>0.035959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>-0.060480</td>\n",
              "      <td>0.032145</td>\n",
              "      <td>0.005485</td>\n",
              "      <td>-0.084974</td>\n",
              "      <td>0.062038</td>\n",
              "      <td>-0.145933</td>\n",
              "      <td>-0.047886</td>\n",
              "      <td>0.023430</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>-0.073406</td>\n",
              "      <td>...</td>\n",
              "      <td>0.071269</td>\n",
              "      <td>0.011389</td>\n",
              "      <td>-0.102351</td>\n",
              "      <td>-0.069919</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.078189</td>\n",
              "      <td>0.042280</td>\n",
              "      <td>-0.141668</td>\n",
              "      <td>-0.042697</td>\n",
              "      <td>-0.004684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>-0.055155</td>\n",
              "      <td>-0.000772</td>\n",
              "      <td>-0.013739</td>\n",
              "      <td>-0.069411</td>\n",
              "      <td>0.043931</td>\n",
              "      <td>-0.236069</td>\n",
              "      <td>0.007162</td>\n",
              "      <td>0.073971</td>\n",
              "      <td>-0.005430</td>\n",
              "      <td>-0.089415</td>\n",
              "      <td>...</td>\n",
              "      <td>0.040675</td>\n",
              "      <td>0.040890</td>\n",
              "      <td>-0.107817</td>\n",
              "      <td>-0.040903</td>\n",
              "      <td>-0.034730</td>\n",
              "      <td>0.079233</td>\n",
              "      <td>0.068175</td>\n",
              "      <td>-0.142191</td>\n",
              "      <td>-0.074767</td>\n",
              "      <td>0.024146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>-0.040352</td>\n",
              "      <td>-0.013974</td>\n",
              "      <td>-0.021251</td>\n",
              "      <td>-0.069300</td>\n",
              "      <td>0.054273</td>\n",
              "      <td>-0.207470</td>\n",
              "      <td>-0.014627</td>\n",
              "      <td>0.039035</td>\n",
              "      <td>0.016669</td>\n",
              "      <td>-0.079122</td>\n",
              "      <td>...</td>\n",
              "      <td>0.059889</td>\n",
              "      <td>0.024202</td>\n",
              "      <td>-0.134411</td>\n",
              "      <td>-0.076633</td>\n",
              "      <td>-0.022368</td>\n",
              "      <td>0.069342</td>\n",
              "      <td>0.052974</td>\n",
              "      <td>-0.124388</td>\n",
              "      <td>-0.036107</td>\n",
              "      <td>0.016075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>-0.051738</td>\n",
              "      <td>0.011170</td>\n",
              "      <td>-0.012154</td>\n",
              "      <td>-0.084336</td>\n",
              "      <td>0.054111</td>\n",
              "      <td>-0.143253</td>\n",
              "      <td>-0.028876</td>\n",
              "      <td>0.015378</td>\n",
              "      <td>0.012340</td>\n",
              "      <td>-0.065018</td>\n",
              "      <td>...</td>\n",
              "      <td>0.076928</td>\n",
              "      <td>0.016929</td>\n",
              "      <td>-0.126763</td>\n",
              "      <td>-0.078424</td>\n",
              "      <td>-0.036591</td>\n",
              "      <td>0.099697</td>\n",
              "      <td>0.058289</td>\n",
              "      <td>-0.105556</td>\n",
              "      <td>-0.050893</td>\n",
              "      <td>-0.016323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>-0.051058</td>\n",
              "      <td>0.028734</td>\n",
              "      <td>-0.002164</td>\n",
              "      <td>-0.122716</td>\n",
              "      <td>0.081772</td>\n",
              "      <td>-0.161522</td>\n",
              "      <td>-0.033399</td>\n",
              "      <td>0.069389</td>\n",
              "      <td>-0.016274</td>\n",
              "      <td>-0.082618</td>\n",
              "      <td>...</td>\n",
              "      <td>0.073130</td>\n",
              "      <td>0.023664</td>\n",
              "      <td>-0.097147</td>\n",
              "      <td>-0.054556</td>\n",
              "      <td>-0.029842</td>\n",
              "      <td>0.042678</td>\n",
              "      <td>0.047881</td>\n",
              "      <td>-0.115789</td>\n",
              "      <td>-0.055526</td>\n",
              "      <td>0.042692</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>106 rows Ã— 768 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-98a1901f-c7f6-446e-8a7b-6e9372d1a35d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-98a1901f-c7f6-446e-8a7b-6e9372d1a35d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-98a1901f-c7f6-446e-8a7b-6e9372d1a35d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-618155f3-98b2-4f02-9fb9-491646824525\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-618155f3-98b2-4f02-9fb9-491646824525')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-618155f3-98b2-4f02-9fb9-491646824525 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_104b2624-c888-4b08-865a-1810521831a9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('text_features')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_104b2624-c888-4b08-865a-1810521831a9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('text_features');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "text_features"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "text_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e87dfff5-f518-4535-8d6a-503b7f790ce2",
      "metadata": {
        "id": "e87dfff5-f518-4535-8d6a-503b7f790ce2"
      },
      "outputs": [],
      "source": [
        "MM= pd.concat([text_features,image_features], axis=1)\n",
        "X_train=MM.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "4fa8a498-9258-430c-a575-0558b31e0502",
      "metadata": {
        "id": "4fa8a498-9258-430c-a575-0558b31e0502",
        "outputId": "5d118e4f-1619-4bb4-aefa-fc9ffcd4e6b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(106, 862)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "04d07bfc-a3b8-47c5-9efa-a0a6c8429b2b",
      "metadata": {
        "id": "04d07bfc-a3b8-47c5-9efa-a0a6c8429b2b"
      },
      "outputs": [],
      "source": [
        "X_train_df=pd.DataFrame(X_train)\n",
        "X_train_df_copy=X_train_df.copy()\n",
        "X_train_df.reset_index(inplace=True)\n",
        "\n",
        "\n",
        "Src_ID=[i for i in range(0,106) for _ in range(0,105-i)]\n",
        "Dst_ID=[i for i in range(0,106) for i in range(1+i,106)]\n",
        "elements_to_repeat=X_train.tolist()\n",
        "repetition_counts=[105-i for i in range(0,105)]\n",
        "\n",
        "Src_feature=[]\n",
        "for element, count in zip(elements_to_repeat, repetition_counts):\n",
        "    Src_feature.extend([element] * count)\n",
        "\n",
        "Dst_feature=[]\n",
        "for i in range(1,106):\n",
        "    for j in range(i,106):\n",
        "        Dst_feature.append(X_train.tolist()[j])\n",
        "\n",
        "Nodes_Data=pd.DataFrame()\n",
        "Nodes_Data['Id']=[i for i in range(0,106)]\n",
        "labels = pd.read_csv(\"/AnimalLabels.csv\")\n",
        "labels['majority_vote'] = labels.mode(axis=1, numeric_only=True).astype(int)\n",
        "Nodes_Data['features']=X_train.tolist()\n",
        "Nodes_Data['label']=labels['majority_vote']\n",
        "\n",
        "dup=[0 for i in range(0,106)]\n",
        "Edge=pd.DataFrame()\n",
        "Edge['Src Id']=Src_ID\n",
        "Edge['Src_feature']=Src_feature\n",
        "Edge['Dst_feature']=Dst_feature\n",
        "Edge['Dst Id']=Dst_ID\n",
        "\n",
        "\n",
        "Src_Ids=[i for i in range(0,106) for _ in range(0,106)]\n",
        "Dst_Ids = [i % 106 for i in range(106 * 106)]\n",
        "Src_features=[X_train.tolist()[i] for i in range(0,106)  for i in range(0,106)]\n",
        "elements_to_repeat=X_train.tolist()\n",
        "repetition_counts=[106 for i in range(0,106)]\n",
        "Dst_features=[]\n",
        "for element, count in zip(elements_to_repeat, repetition_counts):\n",
        "    Dst_features.extend([element] * count)\n",
        "\n",
        "Edge_Data=pd.DataFrame()\n",
        "Edge_Data['Src Ids']=Src_Ids\n",
        "Edge_Data['Src_features']=Src_features\n",
        "Edge_Data['Dst_features']=Dst_features\n",
        "Edge_Data['Dst Ids']=Dst_Ids\n",
        "\n",
        "edge_weight=[]\n",
        "for i in range(0,5565):\n",
        "    A=np.array(Edge['Src_feature'][i])\n",
        "    B=np.array(Edge['Dst_feature'][i])\n",
        "    Cosine_similarity=np.dot(A,B)/(norm(A)*norm(B))\n",
        "    edge_weight.append(Cosine_similarity)\n",
        "\n",
        "for i in range(len(edge_weight)):\n",
        "    if edge_weight[i]<0:\n",
        "        edge_weight[i]=0\n",
        "\n",
        "Edge['edge weights']=edge_weight\n",
        "\n",
        "\n",
        "edge_weights=[]\n",
        "for i in range(0,11236):\n",
        "    A=np.array(Edge_Data['Src_features'][i])\n",
        "    B=np.array(Edge_Data['Dst_features'][i])\n",
        "    Cosine_similarity=np.dot(A,B)/(norm(A)*norm(B))\n",
        "    edge_weights.append(Cosine_similarity)\n",
        "\n",
        "Edge_Data['edge weights']=edge_weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edge_weight"
      ],
      "metadata": {
        "id": "WyXwHiZ8srMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3fbec54-32ef-423c-ddee-7fd8f42e241f"
      },
      "id": "WyXwHiZ8srMa",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7796408548409581,\n",
              " 0.8328019906396308,\n",
              " 0.7055243988750663,\n",
              " 0.7783259481411773,\n",
              " 0.7303962666612972,\n",
              " 0.7733989820689057,\n",
              " 0.7971934674773552,\n",
              " 0.8466760112771331,\n",
              " 0.8313724121864834,\n",
              " 0.7746195572377169,\n",
              " 0.778993676050981,\n",
              " 0.8064765268039201,\n",
              " 0.8200837856769234,\n",
              " 0.8228050526582315,\n",
              " 0.8505785026801473,\n",
              " 0.8468109835186194,\n",
              " 0.8391249935748182,\n",
              " 0.8594215058992217,\n",
              " 0.8551725073774077,\n",
              " 0.7560562241592254,\n",
              " 0.7408735903624087,\n",
              " 0.8131463158331481,\n",
              " 0.7552695852073049,\n",
              " 0.794431209754382,\n",
              " 0.8188852631308657,\n",
              " 0.8099809697076356,\n",
              " 0.7832908870742487,\n",
              " 0.7501994611411947,\n",
              " 0.802441650726862,\n",
              " 0.6773879384917223,\n",
              " 0.7402084274347283,\n",
              " 0.826395132973987,\n",
              " 0.8640246938569972,\n",
              " 0.8615927779123032,\n",
              " 0.8457298347467002,\n",
              " 0.8767925723939515,\n",
              " 0.864314541154144,\n",
              " 0.8516803080583467,\n",
              " 0.8753059310810704,\n",
              " 0.7597401378730584,\n",
              " 0.6989821107853115,\n",
              " 0.7589187790192264,\n",
              " 0.7377170566296561,\n",
              " 0.7661386361877413,\n",
              " 0.7563664462256254,\n",
              " 0.7862697630233826,\n",
              " 0.7816503644684977,\n",
              " 0.8104640517430881,\n",
              " 0.9005533052885634,\n",
              " 0.8066993992761121,\n",
              " 0.8646650261405682,\n",
              " 0.826110645776907,\n",
              " 0.7863523079447828,\n",
              " 0.801795323567958,\n",
              " 0.8311028311849292,\n",
              " 0.9231921739419817,\n",
              " 0.701927725469513,\n",
              " 0.7480461951921289,\n",
              " 0.7318169417321649,\n",
              " 0.7311724930486684,\n",
              " 0.7678019375628048,\n",
              " 0.7338151232112742,\n",
              " 0.582963933717195,\n",
              " 0.6440186961901881,\n",
              " 0.609786700120657,\n",
              " 0.6983756967535225,\n",
              " 0.6695488044855187,\n",
              " 0.6662753941285272,\n",
              " 0.589769352188224,\n",
              " 0.6568451085461169,\n",
              " 0.6692340445169382,\n",
              " 0.5164021231544749,\n",
              " 0.5894602244527993,\n",
              " 0.6035259975346919,\n",
              " 0.5914965179203857,\n",
              " 0.5462622254909415,\n",
              " 0.8501324027455971,\n",
              " 0.784562355691085,\n",
              " 0.7926639806467298,\n",
              " 0.7904140905799373,\n",
              " 0.7465172911628222,\n",
              " 0.7943379070449321,\n",
              " 0.795757796663388,\n",
              " 0.7487296994442062,\n",
              " 0.733650739122923,\n",
              " 0.8244301689527013,\n",
              " 0.8376633178093935,\n",
              " 0.7229772109592808,\n",
              " 0.745655904758662,\n",
              " 0.8310580311671001,\n",
              " 0.7617467142316542,\n",
              " 0.7170694458634003,\n",
              " 0.7998831805236735,\n",
              " 0.8148819236241078,\n",
              " 0.8314929244833594,\n",
              " 0.7741103863184542,\n",
              " 0.8183901009685742,\n",
              " 0.7918763310486708,\n",
              " 0.785279177278468,\n",
              " 0.7720885494759069,\n",
              " 0.7135651274381434,\n",
              " 0.8610296940177036,\n",
              " 0.7468043103030909,\n",
              " 0.8234180346757355,\n",
              " 0.8140402277562184,\n",
              " 0.7043182283697969,\n",
              " 0.7945689324203579,\n",
              " 0.8238695640466569,\n",
              " 0.7378326367044925,\n",
              " 0.8279850015150197,\n",
              " 0.8905422121689223,\n",
              " 0.8535519911696143,\n",
              " 0.8669770612418826,\n",
              " 0.9220879478925426,\n",
              " 0.9079165252095924,\n",
              " 0.7874424648290822,\n",
              " 0.7972024843175424,\n",
              " 0.7980493277124873,\n",
              " 0.7736913400423552,\n",
              " 0.7813470292125493,\n",
              " 0.7620943077416037,\n",
              " 0.7474900366884655,\n",
              " 0.7550124941596341,\n",
              " 0.765590219895693,\n",
              " 0.6417633753229154,\n",
              " 0.7062503843283134,\n",
              " 0.7000788940289494,\n",
              " 0.7077249952232366,\n",
              " 0.7452493207111037,\n",
              " 0.6913208998534498,\n",
              " 0.7329057092690549,\n",
              " 0.7128482262060637,\n",
              " 0.7544179924838603,\n",
              " 0.6559257247755071,\n",
              " 0.6747870173998596,\n",
              " 0.7083705869103651,\n",
              " 0.7309853443143213,\n",
              " 0.7359127136779353,\n",
              " 0.6885593078221709,\n",
              " 0.7181017887967254,\n",
              " 0.711212973098403,\n",
              " 0.7106612018980699,\n",
              " 0.7040638672234119,\n",
              " 0.7104727408043112,\n",
              " 0.6222899236032637,\n",
              " 0.6818072247680057,\n",
              " 0.7303037953381937,\n",
              " 0.6983102429261427,\n",
              " 0.671784916109972,\n",
              " 0.6979883194427167,\n",
              " 0.7122961143614315,\n",
              " 0.7977344127958569,\n",
              " 0.7386306119985538,\n",
              " 0.6883520531575276,\n",
              " 0.7069048602250312,\n",
              " 0.6874059236426305,\n",
              " 0.6338068131273489,\n",
              " 0.6688730009363885,\n",
              " 0.6766951112033465,\n",
              " 0.7328458253329594,\n",
              " 0.7511410729082052,\n",
              " 0.7231578711452215,\n",
              " 0.7311257884497295,\n",
              " 0.7269541095680747,\n",
              " 0.7327290620655362,\n",
              " 0.7660847958665348,\n",
              " 0.5464359009243581,\n",
              " 0.6321717829951726,\n",
              " 0.5633822109598705,\n",
              " 0.622371349203903,\n",
              " 0.6126151517127456,\n",
              " 0.6188316454161453,\n",
              " 0.5928447647212531,\n",
              " 0.5678801414059707,\n",
              " 0.592873156677486,\n",
              " 0.48961147503584834,\n",
              " 0.5431895389262921,\n",
              " 0.5725890211223287,\n",
              " 0.5714110037920168,\n",
              " 0.5354143739705751,\n",
              " 0.6891852562464457,\n",
              " 0.6639810281625584,\n",
              " 0.7063581651685781,\n",
              " 0.7681497307841858,\n",
              " 0.712709134262905,\n",
              " 0.6830018620918125,\n",
              " 0.661633643638749,\n",
              " 0.7455634856857472,\n",
              " 0.7067831197517371,\n",
              " 0.783614024068581,\n",
              " 0.7590810022323742,\n",
              " 0.7742106869941692,\n",
              " 0.8048329614456398,\n",
              " 0.7653547526673475,\n",
              " 0.738111005370426,\n",
              " 0.7385073201605175,\n",
              " 0.7930032009820891,\n",
              " 0.7799774008249044,\n",
              " 0.7652764990446782,\n",
              " 0.7928665670100131,\n",
              " 0.7691656911021193,\n",
              " 0.7959428599770599,\n",
              " 0.7562087036058213,\n",
              " 0.7759876320939467,\n",
              " 0.7883855704065416,\n",
              " 0.7499891737783142,\n",
              " 0.7173585695277792,\n",
              " 0.7653447632973381,\n",
              " 0.8031221162388871,\n",
              " 0.7819142107239305,\n",
              " 0.8098790605749524,\n",
              " 0.8673275909559576,\n",
              " 0.8022278312346367,\n",
              " 0.758308128471693,\n",
              " 0.8034384568769855,\n",
              " 0.817856011037931,\n",
              " 0.7391528738316714,\n",
              " 0.6674632535870382,\n",
              " 0.7115366379491589,\n",
              " 0.753250682880097,\n",
              " 0.7676241241447184,\n",
              " 0.7845441075251838,\n",
              " 0.8025735169218885,\n",
              " 0.7840672556914079,\n",
              " 0.8079144634856105,\n",
              " 0.8336096585203234,\n",
              " 0.7740663559519269,\n",
              " 0.7213734096925096,\n",
              " 0.8054596384348836,\n",
              " 0.7811244609321455,\n",
              " 0.8194886082843298,\n",
              " 0.8266412687602237,\n",
              " 0.8285363414097773,\n",
              " 0.7889435686455287,\n",
              " 0.7522106397087659,\n",
              " 0.7983223760859801,\n",
              " 0.7030074367047824,\n",
              " 0.7361768796762834,\n",
              " 0.826243891759127,\n",
              " 0.8327012698939832,\n",
              " 0.8461084756989947,\n",
              " 0.8465794238320969,\n",
              " 0.8492485117461749,\n",
              " 0.8463197726815169,\n",
              " 0.8557828218546704,\n",
              " 0.8252657097985691,\n",
              " 0.7932749868678299,\n",
              " 0.8226520544030647,\n",
              " 0.805139923871425,\n",
              " 0.7682480823422009,\n",
              " 0.8494647173051625,\n",
              " 0.8050135390565515,\n",
              " 0.8354490674852666,\n",
              " 0.8068884687146141,\n",
              " 0.7678402678227985,\n",
              " 0.8407524549205578,\n",
              " 0.8308251931680408,\n",
              " 0.8234213937042941,\n",
              " 0.858840744271672,\n",
              " 0.8325742848982051,\n",
              " 0.8505078631518016,\n",
              " 0.8426252924909106,\n",
              " 0.8104233974587359,\n",
              " 0.68249552675531,\n",
              " 0.7351113404634572,\n",
              " 0.732480009857706,\n",
              " 0.6833139907347278,\n",
              " 0.7241626128431303,\n",
              " 0.691963740769763,\n",
              " 0.6152017229238934,\n",
              " 0.6542416541788967,\n",
              " 0.6500600370165072,\n",
              " 0.6864169982724725,\n",
              " 0.679529126631472,\n",
              " 0.6945726895283636,\n",
              " 0.5696328127866066,\n",
              " 0.6586506056917136,\n",
              " 0.6894483443450433,\n",
              " 0.5526658791370651,\n",
              " 0.6155837475160424,\n",
              " 0.6425563675706236,\n",
              " 0.6316914882571371,\n",
              " 0.5680421487529929,\n",
              " 0.8401168891437754,\n",
              " 0.8346258837186765,\n",
              " 0.780744714731232,\n",
              " 0.7670032159514235,\n",
              " 0.7135392400427135,\n",
              " 0.7475108742224474,\n",
              " 0.793728536342278,\n",
              " 0.7163123240438435,\n",
              " 0.8004532472439464,\n",
              " 0.8403030308371078,\n",
              " 0.8619143697914856,\n",
              " 0.7340977581728623,\n",
              " 0.7015114010880334,\n",
              " 0.7813074378417437,\n",
              " 0.6838453727646819,\n",
              " 0.665751738336515,\n",
              " 0.7964294752408835,\n",
              " 0.8078319127503449,\n",
              " 0.8118549941576886,\n",
              " 0.7443448140658557,\n",
              " 0.7592421963489718,\n",
              " 0.800128209870191,\n",
              " 0.8103720092035219,\n",
              " 0.762227193127944,\n",
              " 0.7103764286453783,\n",
              " 0.8211180868578911,\n",
              " 0.7682514758653601,\n",
              " 0.8187001293439565,\n",
              " 0.8203678573076313,\n",
              " 0.8203683894479178,\n",
              " 0.8181357458837047,\n",
              " 0.8414099876117639,\n",
              " 0.8020292988035892,\n",
              " 0.8502581528087921,\n",
              " 0.8645900596351361,\n",
              " 0.8306615062539618,\n",
              " 0.7211744871484741,\n",
              " 0.6724966986767674,\n",
              " 0.728234909420139,\n",
              " 0.7302266366234073,\n",
              " 0.74196293402585,\n",
              " 0.7550104709730896,\n",
              " 0.726299790931795,\n",
              " 0.7382851019947807,\n",
              " 0.7728877071371711,\n",
              " 0.7877732367952381,\n",
              " 0.6541656459036808,\n",
              " 0.7400823763633092,\n",
              " 0.7563810592819181,\n",
              " 0.7602428347650833,\n",
              " 0.8006239334079148,\n",
              " 0.7583724465469204,\n",
              " 0.7692239831883143,\n",
              " 0.7680986632142712,\n",
              " 0.7857395010976745,\n",
              " 0.7143222678310446,\n",
              " 0.738591088627783,\n",
              " 0.7738639380804332,\n",
              " 0.7158563475856756,\n",
              " 0.7511729547232544,\n",
              " 0.7393368303527306,\n",
              " 0.7471111999648126,\n",
              " 0.7478996005049093,\n",
              " 0.756219812698325,\n",
              " 0.7043060872663005,\n",
              " 0.7320515161374832,\n",
              " 0.7396001091532612,\n",
              " 0.7401240348950414,\n",
              " 0.7660608363057178,\n",
              " 0.8041618090247367,\n",
              " 0.7566257835914113,\n",
              " 0.7671642196113841,\n",
              " 0.7604439146252389,\n",
              " 0.7748382365640962,\n",
              " 0.7177134140668385,\n",
              " 0.7464720571416501,\n",
              " 0.7197246122391063,\n",
              " 0.7595499088087303,\n",
              " 0.6981385215466984,\n",
              " 0.7204481891287972,\n",
              " 0.7292292352906647,\n",
              " 0.6906816718946854,\n",
              " 0.6874317962143875,\n",
              " 0.6977244873285983,\n",
              " 0.723591227088177,\n",
              " 0.6536218933355666,\n",
              " 0.6966314038741975,\n",
              " 0.6766347804713653,\n",
              " 0.6233482323207133,\n",
              " 0.6801128668560009,\n",
              " 0.6473868157695593,\n",
              " 0.6614751288271861,\n",
              " 0.6937661760220584,\n",
              " 0.6987152796956092,\n",
              " 0.598565180115992,\n",
              " 0.6262204873585809,\n",
              " 0.6563299630393186,\n",
              " 0.567770506745488,\n",
              " 0.6083833930037443,\n",
              " 0.6411210225653897,\n",
              " 0.6365347148663217,\n",
              " 0.59012215577652,\n",
              " 0.7292118895036059,\n",
              " 0.7490704596701636,\n",
              " 0.764236180890249,\n",
              " 0.7982514467869561,\n",
              " 0.7450131423246361,\n",
              " 0.6989769557062985,\n",
              " 0.6840238099313164,\n",
              " 0.719339230917878,\n",
              " 0.8029992431036517,\n",
              " 0.8085579258614534,\n",
              " 0.7972541010726272,\n",
              " 0.7803101145950091,\n",
              " 0.7241212962530239,\n",
              " 0.7379378133139145,\n",
              " 0.6565236753101816,\n",
              " 0.7060954458897051,\n",
              " 0.7831795817928272,\n",
              " 0.7988958900921039,\n",
              " 0.771653346350104,\n",
              " 0.7860608663675935,\n",
              " 0.7433189891305189,\n",
              " 0.7624555397770884,\n",
              " 0.7890442157839412,\n",
              " 0.75516037340041,\n",
              " 0.8051655146096137,\n",
              " 0.7461495146666903,\n",
              " 0.7621017672364911,\n",
              " 0.7642660197221436,\n",
              " 0.7771481650271642,\n",
              " 0.832672776051502,\n",
              " 0.8612771549655625,\n",
              " 0.868479808940139,\n",
              " 0.8276903941015935,\n",
              " 0.8442740100287736,\n",
              " 0.819309498047678,\n",
              " 0.7612308832420611,\n",
              " 0.6834239372984381,\n",
              " 0.7169848703921871,\n",
              " 0.7305462341955048,\n",
              " 0.7396837618258029,\n",
              " 0.7435732282618882,\n",
              " 0.7186372327945746,\n",
              " 0.765352398037433,\n",
              " 0.8128122655028848,\n",
              " 0.8193684100894776,\n",
              " 0.6837616753151811,\n",
              " 0.7611301953747157,\n",
              " 0.7776700267104282,\n",
              " 0.8038854183261324,\n",
              " 0.8116096325472146,\n",
              " 0.7747041587329203,\n",
              " 0.7716266932618152,\n",
              " 0.7518411606555941,\n",
              " 0.7629905535488107,\n",
              " 0.6749098796458848,\n",
              " 0.7000737645399656,\n",
              " 0.7798007507338803,\n",
              " 0.750171225809552,\n",
              " 0.7565595382758802,\n",
              " 0.7270005191516776,\n",
              " 0.7532079256878533,\n",
              " 0.7458677663779528,\n",
              " 0.744949467301159,\n",
              " 0.7213924405201904,\n",
              " 0.7722417498441334,\n",
              " 0.6963018759650738,\n",
              " 0.7498059939616618,\n",
              " 0.7777054259878032,\n",
              " 0.8111882613779319,\n",
              " 0.7358510585615151,\n",
              " 0.7725860621713309,\n",
              " 0.7717746146526924,\n",
              " 0.8058016140231438,\n",
              " 0.8009820532391314,\n",
              " 0.810804074042036,\n",
              " 0.7790481027715094,\n",
              " 0.79326912950281,\n",
              " 0.7399628955945298,\n",
              " 0.7514832700742373,\n",
              " 0.7800890949274092,\n",
              " 0.7785733422601313,\n",
              " 0.7408170610685165,\n",
              " 0.7774704620944334,\n",
              " 0.772255148419318,\n",
              " 0.765895293937422,\n",
              " 0.7447493222435685,\n",
              " 0.7311209036525036,\n",
              " 0.6066434033815421,\n",
              " 0.6877610160788058,\n",
              " 0.6327292319745575,\n",
              " 0.6849741417376805,\n",
              " 0.6801256857690893,\n",
              " 0.6962986163401221,\n",
              " 0.6760424829365561,\n",
              " 0.6449148527130071,\n",
              " 0.6679849919067969,\n",
              " 0.5485923596831205,\n",
              " 0.6010078320318764,\n",
              " 0.6389164862527301,\n",
              " 0.6623516980063404,\n",
              " 0.5976217955529145,\n",
              " 0.7416249824328747,\n",
              " 0.7483268514566241,\n",
              " 0.7394839530070262,\n",
              " 0.7606870031935549,\n",
              " 0.7272811806815289,\n",
              " 0.7109964771440028,\n",
              " 0.713452858625786,\n",
              " 0.7513981030629493,\n",
              " 0.7819117926902115,\n",
              " 0.8348142883008173,\n",
              " 0.8240810002618121,\n",
              " 0.8033906824159947,\n",
              " 0.7681390053311424,\n",
              " 0.7752080537391219,\n",
              " 0.7770546630302403,\n",
              " 0.7305109050577603,\n",
              " 0.8030239811580883,\n",
              " 0.8097445171321587,\n",
              " 0.8107816343108912,\n",
              " 0.8124335624490008,\n",
              " 0.7621452752913276,\n",
              " 0.8279862460857393,\n",
              " 0.8086160767158471,\n",
              " 0.8149393299216567,\n",
              " 0.782896345998558,\n",
              " 0.8047615481245517,\n",
              " 0.7909472319252803,\n",
              " 0.7856589222935011,\n",
              " 0.8428483903270162,\n",
              " 0.8069428205433734,\n",
              " 0.7667848885486834,\n",
              " 0.8016056146216356,\n",
              " 0.8382470104378406,\n",
              " 0.7666587865543137,\n",
              " 0.687985265600981,\n",
              " 0.6852090850786862,\n",
              " 0.7336347972017363,\n",
              " 0.7486675426223315,\n",
              " 0.7691582331616057,\n",
              " 0.7723791007100221,\n",
              " 0.7386561802625657,\n",
              " 0.7442532919049983,\n",
              " 0.8015398215624363,\n",
              " 0.7748063589586306,\n",
              " 0.663883572050242,\n",
              " 0.7526403550460624,\n",
              " 0.7724873860511486,\n",
              " 0.7963736432140659,\n",
              " 0.8155363161483012,\n",
              " 0.7785966678962862,\n",
              " 0.7647870088330906,\n",
              " 0.752082520046471,\n",
              " 0.7512986409899138,\n",
              " 0.690267646079751,\n",
              " 0.7098753464111379,\n",
              " 0.7930158616640269,\n",
              " 0.7768962352121915,\n",
              " 0.7961467281916543,\n",
              " 0.7793879860105922,\n",
              " 0.7979068222879503,\n",
              " 0.7962606870165071,\n",
              " 0.8039587083743732,\n",
              " 0.7618762236769436,\n",
              " 0.8069751703640037,\n",
              " 0.8285772399407565,\n",
              " 0.8202671027488909,\n",
              " 0.8104738672031002,\n",
              " 0.8587177569387558,\n",
              " 0.808675443196502,\n",
              " 0.8354651986738023,\n",
              " 0.8302366448053468,\n",
              " 0.7788867464458508,\n",
              " 0.7691021066381383,\n",
              " 0.8086849551658961,\n",
              " 0.7625978206699189,\n",
              " 0.8026131206947129,\n",
              " 0.7785730778612796,\n",
              " 0.8016854706215321,\n",
              " 0.7937880817499885,\n",
              " 0.7255057814063598,\n",
              " 0.7062563885006518,\n",
              " 0.7282118293086701,\n",
              " 0.747371432557249,\n",
              " 0.6795248413303105,\n",
              " 0.7179394644850464,\n",
              " 0.6951860881140202,\n",
              " 0.6347899149943264,\n",
              " 0.6817086457726951,\n",
              " 0.6730003498807436,\n",
              " 0.668748311490143,\n",
              " 0.699873672425879,\n",
              " 0.6987604710543063,\n",
              " 0.6207542834204899,\n",
              " 0.6409981792382189,\n",
              " 0.6849010278593853,\n",
              " 0.5940908154351225,\n",
              " 0.6344188599311535,\n",
              " 0.657897002397233,\n",
              " 0.6498464177360698,\n",
              " 0.6009792507657892,\n",
              " 0.7757651289155223,\n",
              " 0.7920055374937607,\n",
              " 0.7547639168471507,\n",
              " 0.762443646479824,\n",
              " 0.7065843900591016,\n",
              " 0.703512946661949,\n",
              " 0.7432028667022214,\n",
              " 0.6975609839000776,\n",
              " 0.8261979170027323,\n",
              " 0.8467743677118004,\n",
              " 0.8632490133434483,\n",
              " 0.7776615133071396,\n",
              " 0.725789408655366,\n",
              " 0.7639395196013011,\n",
              " 0.6698512646854449,\n",
              " 0.6889334425071377,\n",
              " 0.795059673043035,\n",
              " 0.7947303612775362,\n",
              " 0.8026325447440346,\n",
              " 0.7729270004403276,\n",
              " 0.7651349886486525,\n",
              " 0.8122329585265539,\n",
              " 0.8260034536150871,\n",
              " 0.7939509782664274,\n",
              " 0.7770433698224259,\n",
              " 0.7781898621884864,\n",
              " 0.800128432339837,\n",
              " 0.813665571612758,\n",
              " 0.8087250386604751,\n",
              " 0.8970884376999109,\n",
              " 0.8731631404474539,\n",
              " 0.8877820781634138,\n",
              " 0.8769691864849093,\n",
              " 0.7273853311719766,\n",
              " 0.6987308029787452,\n",
              " 0.7257806049640217,\n",
              " 0.7398707284874119,\n",
              " 0.7547718200605161,\n",
              " 0.7610194196412357,\n",
              " 0.7449403819441367,\n",
              " 0.8025197770729845,\n",
              " 0.8148588596771721,\n",
              " 0.8136226225540643,\n",
              " 0.7026285590667343,\n",
              " 0.7714795570669154,\n",
              " 0.7862156297184292,\n",
              " 0.8011741024247577,\n",
              " 0.8021987897927787,\n",
              " 0.7973570953654875,\n",
              " 0.8141946821202893,\n",
              " 0.7883418908283676,\n",
              " 0.8186766505734427,\n",
              " 0.7349585361934013,\n",
              " 0.7751724484878817,\n",
              " 0.8153404748909177,\n",
              " 0.7618528834290207,\n",
              " 0.7820848362063887,\n",
              " 0.7607627268283841,\n",
              " 0.785463672073137,\n",
              " 0.7799734195418773,\n",
              " 0.7968677972883934,\n",
              " 0.7401053427497816,\n",
              " 0.7445371856884205,\n",
              " 0.6918641051507641,\n",
              " 0.7528753145904832,\n",
              " 0.7638759765617256,\n",
              " 0.8040525825395494,\n",
              " 0.7270076447523676,\n",
              " 0.7804482453291076,\n",
              " 0.7535066877850591,\n",
              " 0.7759891627426719,\n",
              " 0.7702889898274061,\n",
              " 0.7696901748067435,\n",
              " 0.7686152200668395,\n",
              " 0.7901036209426405,\n",
              " 0.7044207460827775,\n",
              " 0.7600862229690293,\n",
              " 0.7575507157635137,\n",
              " 0.7534965889993459,\n",
              " 0.674246475747489,\n",
              " 0.7246479037022342,\n",
              " 0.7126428032260373,\n",
              " 0.6923832069997845,\n",
              " 0.6981550918665844,\n",
              " 0.68619327910487,\n",
              " 0.630307440568584,\n",
              " 0.7150547176195629,\n",
              " 0.6522326808342834,\n",
              " 0.6804200580452868,\n",
              " 0.6852097919896168,\n",
              " 0.6981728114459154,\n",
              " 0.5920931459431668,\n",
              " 0.6509267168174372,\n",
              " 0.6785583457416479,\n",
              " 0.5780015245849971,\n",
              " 0.638287121690462,\n",
              " 0.6464888689906777,\n",
              " 0.6446769050761103,\n",
              " 0.6144301793252624,\n",
              " 0.7688757237231637,\n",
              " 0.7443107077739678,\n",
              " 0.7719684513271502,\n",
              " 0.8260813557473471,\n",
              " 0.7430851991498536,\n",
              " 0.7556546117272768,\n",
              " 0.7410235070865674,\n",
              " 0.7198492606948397,\n",
              " 0.7954695090137566,\n",
              " 0.8203865896988749,\n",
              " 0.8055313583188666,\n",
              " 0.7905775733053482,\n",
              " 0.7399603296533763,\n",
              " 0.7779643256857082,\n",
              " 0.7144746361902784,\n",
              " 0.68483896338132,\n",
              " 0.8030722734226621,\n",
              " 0.8226886429986967,\n",
              " 0.7679585551795659,\n",
              " 0.7710177719613425,\n",
              " 0.7495096904905505,\n",
              " 0.7893986830665537,\n",
              " 0.7799711329356266,\n",
              " 0.7533620127927667,\n",
              " 0.7419723972596588,\n",
              " 0.7785416467419048,\n",
              " 0.7687930306550859,\n",
              " 0.7491050311889523,\n",
              " 0.7813461571627957,\n",
              " 0.8577894079137202,\n",
              " 0.8874736338146365,\n",
              " 0.9014931499706103,\n",
              " 0.8182381162938449,\n",
              " 0.7471256974830096,\n",
              " 0.7524458462615515,\n",
              " 0.7605429208858046,\n",
              " 0.7470232254058232,\n",
              " 0.7612201528672011,\n",
              " 0.7530670640029512,\n",
              " 0.7786318612731339,\n",
              " 0.7983225915382216,\n",
              " 0.7950683724695524,\n",
              " 0.6731458240547962,\n",
              " 0.7422956414725957,\n",
              " 0.7545012009389748,\n",
              " 0.7703006752829259,\n",
              " 0.7831719832903382,\n",
              " 0.7505649571793946,\n",
              " 0.7705682582606124,\n",
              " 0.739942703492127,\n",
              " 0.7838847598862267,\n",
              " 0.6755290772186483,\n",
              " 0.7131181863196173,\n",
              " 0.7708024348439962,\n",
              " 0.7600573838346024,\n",
              " 0.7658511800697257,\n",
              " 0.7292697015221794,\n",
              " 0.7496028624588216,\n",
              " 0.7472586977481398,\n",
              " 0.7482817816096449,\n",
              " 0.7281375234042423,\n",
              " 0.7313344948128545,\n",
              " 0.6582749365469057,\n",
              " 0.7105332168930816,\n",
              " 0.7440987963007443,\n",
              " 0.7423538186997026,\n",
              " 0.6905542594318262,\n",
              " 0.7367844747360832,\n",
              " 0.7307552396462785,\n",
              " 0.7892350512963463,\n",
              " 0.7719307520159673,\n",
              " 0.7347181102054934,\n",
              " 0.7497546716421672,\n",
              " 0.7509431417191046,\n",
              " 0.6827691690542823,\n",
              " 0.7218204762429696,\n",
              " 0.7102674942803028,\n",
              " 0.7658411459884465,\n",
              " 0.7296504383817374,\n",
              " 0.7485446992533167,\n",
              " 0.7209462521740192,\n",
              " 0.7382250859035794,\n",
              " 0.7309057354215838,\n",
              " 0.7458548700470004,\n",
              " 0.572811190959619,\n",
              " 0.6645242952559982,\n",
              " 0.5943386637941409,\n",
              " 0.6485110991869228,\n",
              " 0.6358660204509103,\n",
              " 0.6588571592004201,\n",
              " 0.6109171653920701,\n",
              " 0.6108818529590857,\n",
              " 0.6250019619399849,\n",
              " 0.5087605435321274,\n",
              " 0.5792180216067901,\n",
              " 0.5960108078077365,\n",
              " 0.6088074639849892,\n",
              " 0.568920978284382,\n",
              " 0.7331399882397509,\n",
              " 0.705441117206935,\n",
              " 0.7464294284787014,\n",
              " 0.7873178130838226,\n",
              " 0.7170712525216586,\n",
              " 0.7236051139026191,\n",
              " 0.7093752779927263,\n",
              " 0.7351475514792818,\n",
              " 0.7445801578116269,\n",
              " 0.8076090792963858,\n",
              " 0.7930135378995528,\n",
              " 0.7820093689055961,\n",
              " 0.7887325104696784,\n",
              " 0.7856376876944953,\n",
              " 0.7591084285224134,\n",
              " 0.7181780211764206,\n",
              " 0.7993550688204384,\n",
              " 0.8090047639597919,\n",
              " 0.7704354347276611,\n",
              " 0.7896009293444082,\n",
              " 0.7645437753351904,\n",
              " 0.8174426270191144,\n",
              " 0.7778763928801535,\n",
              " 0.7745011051481825,\n",
              " 0.7578500877341273,\n",
              " 0.7655764936147484,\n",
              " 0.7405887271871797,\n",
              " 0.7593952556882271,\n",
              " 0.8101047374397228,\n",
              " 0.9349209906179051,\n",
              " 0.8866087224725093,\n",
              " 0.7817998552261519,\n",
              " 0.7480062141539943,\n",
              " 0.7875217936828331,\n",
              " 0.8028414024707452,\n",
              " 0.8334857677165529,\n",
              " 0.821166469958547,\n",
              " 0.803835184341591,\n",
              " 0.8318786761960313,\n",
              " 0.8380346410248395,\n",
              " 0.805962332164402,\n",
              " 0.7165994932651585,\n",
              " 0.7931131618949928,\n",
              " 0.7774210958821279,\n",
              " 0.7850277093181789,\n",
              " 0.8146326007423761,\n",
              " 0.8046852554510256,\n",
              " 0.8193346739721442,\n",
              " 0.7886250958511214,\n",
              " 0.8399203320537874,\n",
              " 0.6996580082471702,\n",
              " 0.7484990625263519,\n",
              " 0.8047016551620662,\n",
              " 0.8184472058079375,\n",
              " 0.8321844571674109,\n",
              " 0.802862099483253,\n",
              " 0.8415829221532422,\n",
              " 0.8185095229618637,\n",
              " 0.8498842502368379,\n",
              " 0.8156579574331015,\n",
              " 0.7628994195084521,\n",
              " 0.7223173117246359,\n",
              " 0.7679520647647847,\n",
              " 0.7776731349000802,\n",
              " 0.7999555433048896,\n",
              " 0.760624718519964,\n",
              " 0.7987756890990579,\n",
              " 0.7820062188315661,\n",
              " 0.8399845520775293,\n",
              " 0.8311610134847855,\n",
              " 0.7960273444444596,\n",
              " 0.8084706087782642,\n",
              " 0.807461147437863,\n",
              " 0.7106465826023646,\n",
              " 0.7669742096810647,\n",
              " 0.7935984713847477,\n",
              " 0.8232823138887189,\n",
              " 0.6991149247394715,\n",
              " 0.7363093810816644,\n",
              " 0.754521100734219,\n",
              " 0.6997986417097092,\n",
              " 0.732419894065328,\n",
              " 0.7243706475458319,\n",
              " 0.5881221185168639,\n",
              " 0.6718117847438737,\n",
              " 0.6125374733120256,\n",
              " 0.6828850333769291,\n",
              " 0.6645902576222268,\n",
              " 0.6731755640065235,\n",
              " 0.5866430612537812,\n",
              " 0.6189799323953535,\n",
              " 0.6530652305375063,\n",
              " 0.523460329831519,\n",
              " 0.5978322632206778,\n",
              " 0.6135235186929207,\n",
              " 0.5950364571778166,\n",
              " 0.557963315847702,\n",
              " 0.8005393074935347,\n",
              " 0.7794829432885965,\n",
              " 0.7920361927410778,\n",
              " 0.8092031261287332,\n",
              " 0.7783529934079709,\n",
              " 0.7584106526021707,\n",
              " 0.7733945501941272,\n",
              " 0.7474453377175517,\n",
              " 0.7725351153498671,\n",
              " 0.8505017717160325,\n",
              " 0.8406368102369342,\n",
              " 0.7856011156160212,\n",
              " 0.7593291789193166,\n",
              " 0.8175161110764096,\n",
              " 0.7441685858267056,\n",
              " 0.7470624037474748,\n",
              " 0.8053535444600716,\n",
              " 0.8463453439071559,\n",
              " 0.8201325433145104,\n",
              " 0.8061870587902167,\n",
              " 0.7948146701117293,\n",
              " 0.8025710801475193,\n",
              " 0.8016006041281097,\n",
              " 0.791197708090353,\n",
              " 0.7963651422366732,\n",
              " 0.8166919056620088,\n",
              " 0.772095459143098,\n",
              " 0.8008769374788569,\n",
              " 0.8084792743880405,\n",
              " 0.9264461228990024,\n",
              " 0.8108187788602488,\n",
              " 0.7836433932614826,\n",
              " 0.81380264485215,\n",
              " 0.8351071753651164,\n",
              " 0.8489123002902162,\n",
              " 0.846574551772008,\n",
              " 0.8224083138622384,\n",
              " 0.8345516298263592,\n",
              " 0.8548997556838829,\n",
              " 0.8221314892937165,\n",
              " 0.7114624726667659,\n",
              " 0.7979209338313548,\n",
              " 0.7841155180136636,\n",
              " 0.8128403994787665,\n",
              " 0.8481396495152953,\n",
              " 0.8160370244291211,\n",
              " 0.8392737303951406,\n",
              " 0.7975972256764118,\n",
              " 0.8534304033186023,\n",
              " 0.7263406115123466,\n",
              " 0.770406776063472,\n",
              " 0.8446436540851885,\n",
              " 0.8301808263333483,\n",
              " 0.8465562453810258,\n",
              " 0.8232776110974365,\n",
              " 0.8472434843975415,\n",
              " 0.8357724614405803,\n",
              " 0.8421895938659317,\n",
              " 0.8207289812467612,\n",
              " 0.7875229707318472,\n",
              " 0.7449659299464325,\n",
              " 0.8086809189239356,\n",
              " 0.8120376597037287,\n",
              " 0.8356032907839367,\n",
              " 0.7979244384532495,\n",
              " 0.8199648074247664,\n",
              " 0.8212559558183307,\n",
              " 0.8706385305335999,\n",
              " 0.8300289220856659,\n",
              " 0.7815614177697117,\n",
              " 0.8094646297953539,\n",
              " 0.8068068989209617,\n",
              " 0.7336961137862876,\n",
              " 0.7814264785540407,\n",
              " 0.7749997601749004,\n",
              " 0.8118262953372979,\n",
              " 0.7120730057796186,\n",
              " 0.7297530032707816,\n",
              " 0.7425297802430011,\n",
              " 0.6951937895479371,\n",
              " 0.7246486055369431,\n",
              " 0.728222716702313,\n",
              " 0.6289131602028207,\n",
              " 0.708389615939002,\n",
              " 0.6569832594678414,\n",
              " 0.6962096134673446,\n",
              " 0.706774837092776,\n",
              " 0.7111204897351331,\n",
              " 0.5945588367106331,\n",
              " 0.6448528324725878,\n",
              " 0.6907811954731106,\n",
              " 0.5561663232121814,\n",
              " 0.6312767892416468,\n",
              " 0.6556114731182769,\n",
              " 0.6453949495140411,\n",
              " 0.5998236307914682,\n",
              " 0.8192388813505829,\n",
              " 0.7857611992092326,\n",
              " 0.8154572533923254,\n",
              " 0.8523001238819446,\n",
              " 0.7783578586348785,\n",
              " 0.7573167618963357,\n",
              " 0.7646764488835757,\n",
              " 0.7592726281640423,\n",
              " 0.8031662050255944,\n",
              " 0.8754428791199108,\n",
              " 0.8641996504556955,\n",
              " 0.8219881614702695,\n",
              " 0.7930690031945664,\n",
              " 0.8062790220196075,\n",
              " 0.7321897439767827,\n",
              " 0.7619682516016291,\n",
              " 0.8555400145530747,\n",
              " 0.8681908812043972,\n",
              " 0.8329407956402182,\n",
              " 0.8357296663904896,\n",
              " 0.8106093134552208,\n",
              " 0.8339597591029893,\n",
              " 0.8297335072640711,\n",
              " 0.8150535567834006,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Edge_Data"
      ],
      "metadata": {
        "id": "gEhjcqG4_p2s",
        "outputId": "7d5e0c38-b580-4f13-c7b4-6af0480a099e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "id": "gEhjcqG4_p2s",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Src Ids                                       Src_features  \\\n",
              "0            0  [-0.04845981, 0.08475658, 0.0034967388, -0.087...   \n",
              "1            0  [-0.06572274, 0.025389247, -0.025804423, -0.13...   \n",
              "2            0  [-0.045841996, 0.040817425, -0.028310552, -0.1...   \n",
              "3            0  [-0.049970612, 0.03240784, -0.0054141465, -0.0...   \n",
              "4            0  [-0.045510203, 0.020915393, -0.008356429, -0.1...   \n",
              "...        ...                                                ...   \n",
              "11231      105  [-0.060479827, 0.032145437, 0.005485486, -0.08...   \n",
              "11232      105  [-0.05515463, -0.00077195856, -0.013739326, -0...   \n",
              "11233      105  [-0.040351998, -0.013974475, -0.021250768, -0....   \n",
              "11234      105  [-0.051737767, 0.011169992, -0.012154045, -0.0...   \n",
              "11235      105  [-0.051058244, 0.0287344, -0.0021639222, -0.12...   \n",
              "\n",
              "                                            Dst_features  Dst Ids  \\\n",
              "0      [-0.04845981, 0.08475658, 0.0034967388, -0.087...        0   \n",
              "1      [-0.04845981, 0.08475658, 0.0034967388, -0.087...        1   \n",
              "2      [-0.04845981, 0.08475658, 0.0034967388, -0.087...        2   \n",
              "3      [-0.04845981, 0.08475658, 0.0034967388, -0.087...        3   \n",
              "4      [-0.04845981, 0.08475658, 0.0034967388, -0.087...        4   \n",
              "...                                                  ...      ...   \n",
              "11231  [-0.051058244, 0.0287344, -0.0021639222, -0.12...      101   \n",
              "11232  [-0.051058244, 0.0287344, -0.0021639222, -0.12...      102   \n",
              "11233  [-0.051058244, 0.0287344, -0.0021639222, -0.12...      103   \n",
              "11234  [-0.051058244, 0.0287344, -0.0021639222, -0.12...      104   \n",
              "11235  [-0.051058244, 0.0287344, -0.0021639222, -0.12...      105   \n",
              "\n",
              "       edge weights  \n",
              "0          1.000000  \n",
              "1          0.779641  \n",
              "2          0.832802  \n",
              "3          0.705524  \n",
              "4          0.778326  \n",
              "...             ...  \n",
              "11231      0.815028  \n",
              "11232      0.842930  \n",
              "11233      0.818225  \n",
              "11234      0.873431  \n",
              "11235      1.000000  \n",
              "\n",
              "[11236 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a88c3a10-ef44-437d-8bac-1023f9f0fc36\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Src Ids</th>\n",
              "      <th>Src_features</th>\n",
              "      <th>Dst_features</th>\n",
              "      <th>Dst Ids</th>\n",
              "      <th>edge weights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[-0.04845981, 0.08475658, 0.0034967388, -0.087...</td>\n",
              "      <td>[-0.04845981, 0.08475658, 0.0034967388, -0.087...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>[-0.06572274, 0.025389247, -0.025804423, -0.13...</td>\n",
              "      <td>[-0.04845981, 0.08475658, 0.0034967388, -0.087...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.779641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>[-0.045841996, 0.040817425, -0.028310552, -0.1...</td>\n",
              "      <td>[-0.04845981, 0.08475658, 0.0034967388, -0.087...</td>\n",
              "      <td>2</td>\n",
              "      <td>0.832802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>[-0.049970612, 0.03240784, -0.0054141465, -0.0...</td>\n",
              "      <td>[-0.04845981, 0.08475658, 0.0034967388, -0.087...</td>\n",
              "      <td>3</td>\n",
              "      <td>0.705524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>[-0.045510203, 0.020915393, -0.008356429, -0.1...</td>\n",
              "      <td>[-0.04845981, 0.08475658, 0.0034967388, -0.087...</td>\n",
              "      <td>4</td>\n",
              "      <td>0.778326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11231</th>\n",
              "      <td>105</td>\n",
              "      <td>[-0.060479827, 0.032145437, 0.005485486, -0.08...</td>\n",
              "      <td>[-0.051058244, 0.0287344, -0.0021639222, -0.12...</td>\n",
              "      <td>101</td>\n",
              "      <td>0.815028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11232</th>\n",
              "      <td>105</td>\n",
              "      <td>[-0.05515463, -0.00077195856, -0.013739326, -0...</td>\n",
              "      <td>[-0.051058244, 0.0287344, -0.0021639222, -0.12...</td>\n",
              "      <td>102</td>\n",
              "      <td>0.842930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11233</th>\n",
              "      <td>105</td>\n",
              "      <td>[-0.040351998, -0.013974475, -0.021250768, -0....</td>\n",
              "      <td>[-0.051058244, 0.0287344, -0.0021639222, -0.12...</td>\n",
              "      <td>103</td>\n",
              "      <td>0.818225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11234</th>\n",
              "      <td>105</td>\n",
              "      <td>[-0.051737767, 0.011169992, -0.012154045, -0.0...</td>\n",
              "      <td>[-0.051058244, 0.0287344, -0.0021639222, -0.12...</td>\n",
              "      <td>104</td>\n",
              "      <td>0.873431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11235</th>\n",
              "      <td>105</td>\n",
              "      <td>[-0.051058244, 0.0287344, -0.0021639222, -0.12...</td>\n",
              "      <td>[-0.051058244, 0.0287344, -0.0021639222, -0.12...</td>\n",
              "      <td>105</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11236 rows Ã— 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a88c3a10-ef44-437d-8bac-1023f9f0fc36')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a88c3a10-ef44-437d-8bac-1023f9f0fc36 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a88c3a10-ef44-437d-8bac-1023f9f0fc36');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6b8bea3f-80f2-45fb-845e-543ad1b2be29\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6b8bea3f-80f2-45fb-845e-543ad1b2be29')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6b8bea3f-80f2-45fb-845e-543ad1b2be29 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b29a8008-b90d-41e4-a848-f148acc6acc4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('Edge_Data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b29a8008-b90d-41e4-a848-f148acc6acc4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('Edge_Data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "Edge_Data",
              "summary": "{\n  \"name\": \"Edge_Data\",\n  \"rows\": 11236,\n  \"fields\": [\n    {\n      \"column\": \"Src Ids\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30,\n        \"min\": 0,\n        \"max\": 105,\n        \"num_unique_values\": 106,\n        \"samples\": [\n          100,\n          10,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Src_features\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dst_features\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dst Ids\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30,\n        \"min\": 0,\n        \"max\": 105,\n        \"num_unique_values\": 106,\n        \"samples\": [\n          100,\n          10,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"edge weights\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08639586871311042,\n        \"min\": 0.4642524192687743,\n        \"max\": 1.0000000000000002,\n        \"num_unique_values\": 5569,\n        \"samples\": [\n          0.6938582613570896,\n          0.7658411459884465,\n          0.7514832700742373\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Edge_Data[Edge_Data['Dst Ids']==7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "Z_tRiN8c7_Th",
        "outputId": "c1d5746f-da81-498d-9cac-6a61224137d6"
      },
      "id": "Z_tRiN8c7_Th",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Src Ids                                       Src_features  \\\n",
              "7            0  [-0.058395185, 0.055415377, -0.010777363, -0.1...   \n",
              "113          1  [-0.058395185, 0.055415377, -0.010777363, -0.1...   \n",
              "219          2  [-0.058395185, 0.055415377, -0.010777363, -0.1...   \n",
              "325          3  [-0.058395185, 0.055415377, -0.010777363, -0.1...   \n",
              "431          4  [-0.058395185, 0.055415377, -0.010777363, -0.1...   \n",
              "...        ...                                                ...   \n",
              "10713      101  [-0.058395185, 0.055415377, -0.010777363, -0.1...   \n",
              "10819      102  [-0.058395185, 0.055415377, -0.010777363, -0.1...   \n",
              "10925      103  [-0.058395185, 0.055415377, -0.010777363, -0.1...   \n",
              "11031      104  [-0.058395185, 0.055415377, -0.010777363, -0.1...   \n",
              "11137      105  [-0.058395185, 0.055415377, -0.010777363, -0.1...   \n",
              "\n",
              "                                            Dst_features  Dst Ids  \\\n",
              "7      [-0.04845981, 0.08475658, 0.0034967388, -0.087...        7   \n",
              "113    [-0.06572274, 0.025389247, -0.025804423, -0.13...        7   \n",
              "219    [-0.045841996, 0.040817425, -0.028310552, -0.1...        7   \n",
              "325    [-0.049970612, 0.03240784, -0.0054141465, -0.0...        7   \n",
              "431    [-0.045510203, 0.020915393, -0.008356429, -0.1...        7   \n",
              "...                                                  ...      ...   \n",
              "10713  [-0.060479827, 0.032145437, 0.005485486, -0.08...        7   \n",
              "10819  [-0.05515463, -0.00077195856, -0.013739326, -0...        7   \n",
              "10925  [-0.040351998, -0.013974475, -0.021250768, -0....        7   \n",
              "11031  [-0.051737767, 0.011169992, -0.012154045, -0.0...        7   \n",
              "11137  [-0.051058244, 0.0287344, -0.0021639222, -0.12...        7   \n",
              "\n",
              "       edge weights  \n",
              "7          0.797193  \n",
              "113        0.890542  \n",
              "219        0.758308  \n",
              "325        0.802029  \n",
              "431        0.868480  \n",
              "...             ...  \n",
              "10713      0.757850  \n",
              "10819      0.765576  \n",
              "10925      0.740589  \n",
              "11031      0.759395  \n",
              "11137      0.810105  \n",
              "\n",
              "[106 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3d182f20-1205-453a-ae9d-0793e3a3e1f7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Src Ids</th>\n",
              "      <th>Src_features</th>\n",
              "      <th>Dst_features</th>\n",
              "      <th>Dst Ids</th>\n",
              "      <th>edge weights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>[-0.058395185, 0.055415377, -0.010777363, -0.1...</td>\n",
              "      <td>[-0.04845981, 0.08475658, 0.0034967388, -0.087...</td>\n",
              "      <td>7</td>\n",
              "      <td>0.797193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>1</td>\n",
              "      <td>[-0.058395185, 0.055415377, -0.010777363, -0.1...</td>\n",
              "      <td>[-0.06572274, 0.025389247, -0.025804423, -0.13...</td>\n",
              "      <td>7</td>\n",
              "      <td>0.890542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>2</td>\n",
              "      <td>[-0.058395185, 0.055415377, -0.010777363, -0.1...</td>\n",
              "      <td>[-0.045841996, 0.040817425, -0.028310552, -0.1...</td>\n",
              "      <td>7</td>\n",
              "      <td>0.758308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>325</th>\n",
              "      <td>3</td>\n",
              "      <td>[-0.058395185, 0.055415377, -0.010777363, -0.1...</td>\n",
              "      <td>[-0.049970612, 0.03240784, -0.0054141465, -0.0...</td>\n",
              "      <td>7</td>\n",
              "      <td>0.802029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>4</td>\n",
              "      <td>[-0.058395185, 0.055415377, -0.010777363, -0.1...</td>\n",
              "      <td>[-0.045510203, 0.020915393, -0.008356429, -0.1...</td>\n",
              "      <td>7</td>\n",
              "      <td>0.868480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10713</th>\n",
              "      <td>101</td>\n",
              "      <td>[-0.058395185, 0.055415377, -0.010777363, -0.1...</td>\n",
              "      <td>[-0.060479827, 0.032145437, 0.005485486, -0.08...</td>\n",
              "      <td>7</td>\n",
              "      <td>0.757850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10819</th>\n",
              "      <td>102</td>\n",
              "      <td>[-0.058395185, 0.055415377, -0.010777363, -0.1...</td>\n",
              "      <td>[-0.05515463, -0.00077195856, -0.013739326, -0...</td>\n",
              "      <td>7</td>\n",
              "      <td>0.765576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10925</th>\n",
              "      <td>103</td>\n",
              "      <td>[-0.058395185, 0.055415377, -0.010777363, -0.1...</td>\n",
              "      <td>[-0.040351998, -0.013974475, -0.021250768, -0....</td>\n",
              "      <td>7</td>\n",
              "      <td>0.740589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11031</th>\n",
              "      <td>104</td>\n",
              "      <td>[-0.058395185, 0.055415377, -0.010777363, -0.1...</td>\n",
              "      <td>[-0.051737767, 0.011169992, -0.012154045, -0.0...</td>\n",
              "      <td>7</td>\n",
              "      <td>0.759395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11137</th>\n",
              "      <td>105</td>\n",
              "      <td>[-0.058395185, 0.055415377, -0.010777363, -0.1...</td>\n",
              "      <td>[-0.051058244, 0.0287344, -0.0021639222, -0.12...</td>\n",
              "      <td>7</td>\n",
              "      <td>0.810105</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>106 rows Ã— 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d182f20-1205-453a-ae9d-0793e3a3e1f7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3d182f20-1205-453a-ae9d-0793e3a3e1f7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3d182f20-1205-453a-ae9d-0793e3a3e1f7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4bd85793-0a6e-4c16-942f-9ebcd474f72e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4bd85793-0a6e-4c16-942f-9ebcd474f72e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4bd85793-0a6e-4c16-942f-9ebcd474f72e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"Edge_Data[Edge_Data['Dst Ids']==7]\",\n  \"rows\": 106,\n  \"fields\": [\n    {\n      \"column\": \"Src Ids\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30,\n        \"min\": 0,\n        \"max\": 105,\n        \"num_unique_values\": 106,\n        \"samples\": [\n          100,\n          10,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Src_features\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dst_features\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dst Ids\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 7,\n        \"max\": 7,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"edge weights\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07391689930836207,\n        \"min\": 0.5087605435321274,\n        \"max\": 1.0,\n        \"num_unique_values\": 106,\n        \"samples\": [\n          0.7745011051481825\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ff=Edge_Data[Edge_Data['Src Ids']!=0 ]"
      ],
      "metadata": {
        "id": "0sIwXywH8aF6"
      },
      "id": "0sIwXywH8aF6",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ff[ff['edge weights']>=0.329]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "0tlrtE5p6UFZ",
        "outputId": "74cbdd8d-df46-42e7-8c88-a945ccc77eed"
      },
      "id": "0tlrtE5p6UFZ",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Src Ids                                       Src_features  \\\n",
              "106          1  [-0.04845981, 0.08475658, 0.0034967388, -0.087...   \n",
              "107          1  [-0.06572274, 0.025389247, -0.025804423, -0.13...   \n",
              "108          1  [-0.045841996, 0.040817425, -0.028310552, -0.1...   \n",
              "109          1  [-0.049970612, 0.03240784, -0.0054141465, -0.0...   \n",
              "110          1  [-0.045510203, 0.020915393, -0.008356429, -0.1...   \n",
              "...        ...                                                ...   \n",
              "11231      105  [-0.060479827, 0.032145437, 0.005485486, -0.08...   \n",
              "11232      105  [-0.05515463, -0.00077195856, -0.013739326, -0...   \n",
              "11233      105  [-0.040351998, -0.013974475, -0.021250768, -0....   \n",
              "11234      105  [-0.051737767, 0.011169992, -0.012154045, -0.0...   \n",
              "11235      105  [-0.051058244, 0.0287344, -0.0021639222, -0.12...   \n",
              "\n",
              "                                            Dst_features  Dst Ids  \\\n",
              "106    [-0.06572274, 0.025389247, -0.025804423, -0.13...        0   \n",
              "107    [-0.06572274, 0.025389247, -0.025804423, -0.13...        1   \n",
              "108    [-0.06572274, 0.025389247, -0.025804423, -0.13...        2   \n",
              "109    [-0.06572274, 0.025389247, -0.025804423, -0.13...        3   \n",
              "110    [-0.06572274, 0.025389247, -0.025804423, -0.13...        4   \n",
              "...                                                  ...      ...   \n",
              "11231  [-0.051058244, 0.0287344, -0.0021639222, -0.12...      101   \n",
              "11232  [-0.051058244, 0.0287344, -0.0021639222, -0.12...      102   \n",
              "11233  [-0.051058244, 0.0287344, -0.0021639222, -0.12...      103   \n",
              "11234  [-0.051058244, 0.0287344, -0.0021639222, -0.12...      104   \n",
              "11235  [-0.051058244, 0.0287344, -0.0021639222, -0.12...      105   \n",
              "\n",
              "       edge weights  \n",
              "106        0.779641  \n",
              "107        1.000000  \n",
              "108        0.704318  \n",
              "109        0.794569  \n",
              "110        0.823870  \n",
              "...             ...  \n",
              "11231      0.815028  \n",
              "11232      0.842930  \n",
              "11233      0.818225  \n",
              "11234      0.873431  \n",
              "11235      1.000000  \n",
              "\n",
              "[11130 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-81eafc37-0bb2-432d-a65f-5bdeceaa3b93\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Src Ids</th>\n",
              "      <th>Src_features</th>\n",
              "      <th>Dst_features</th>\n",
              "      <th>Dst Ids</th>\n",
              "      <th>edge weights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>1</td>\n",
              "      <td>[-0.04845981, 0.08475658, 0.0034967388, -0.087...</td>\n",
              "      <td>[-0.06572274, 0.025389247, -0.025804423, -0.13...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.779641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>1</td>\n",
              "      <td>[-0.06572274, 0.025389247, -0.025804423, -0.13...</td>\n",
              "      <td>[-0.06572274, 0.025389247, -0.025804423, -0.13...</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>1</td>\n",
              "      <td>[-0.045841996, 0.040817425, -0.028310552, -0.1...</td>\n",
              "      <td>[-0.06572274, 0.025389247, -0.025804423, -0.13...</td>\n",
              "      <td>2</td>\n",
              "      <td>0.704318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>1</td>\n",
              "      <td>[-0.049970612, 0.03240784, -0.0054141465, -0.0...</td>\n",
              "      <td>[-0.06572274, 0.025389247, -0.025804423, -0.13...</td>\n",
              "      <td>3</td>\n",
              "      <td>0.794569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>1</td>\n",
              "      <td>[-0.045510203, 0.020915393, -0.008356429, -0.1...</td>\n",
              "      <td>[-0.06572274, 0.025389247, -0.025804423, -0.13...</td>\n",
              "      <td>4</td>\n",
              "      <td>0.823870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11231</th>\n",
              "      <td>105</td>\n",
              "      <td>[-0.060479827, 0.032145437, 0.005485486, -0.08...</td>\n",
              "      <td>[-0.051058244, 0.0287344, -0.0021639222, -0.12...</td>\n",
              "      <td>101</td>\n",
              "      <td>0.815028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11232</th>\n",
              "      <td>105</td>\n",
              "      <td>[-0.05515463, -0.00077195856, -0.013739326, -0...</td>\n",
              "      <td>[-0.051058244, 0.0287344, -0.0021639222, -0.12...</td>\n",
              "      <td>102</td>\n",
              "      <td>0.842930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11233</th>\n",
              "      <td>105</td>\n",
              "      <td>[-0.040351998, -0.013974475, -0.021250768, -0....</td>\n",
              "      <td>[-0.051058244, 0.0287344, -0.0021639222, -0.12...</td>\n",
              "      <td>103</td>\n",
              "      <td>0.818225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11234</th>\n",
              "      <td>105</td>\n",
              "      <td>[-0.051737767, 0.011169992, -0.012154045, -0.0...</td>\n",
              "      <td>[-0.051058244, 0.0287344, -0.0021639222, -0.12...</td>\n",
              "      <td>104</td>\n",
              "      <td>0.873431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11235</th>\n",
              "      <td>105</td>\n",
              "      <td>[-0.051058244, 0.0287344, -0.0021639222, -0.12...</td>\n",
              "      <td>[-0.051058244, 0.0287344, -0.0021639222, -0.12...</td>\n",
              "      <td>105</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11130 rows Ã— 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-81eafc37-0bb2-432d-a65f-5bdeceaa3b93')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-81eafc37-0bb2-432d-a65f-5bdeceaa3b93 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-81eafc37-0bb2-432d-a65f-5bdeceaa3b93');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-337d1a67-eb3f-47a4-9255-288cb0bb0f82\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-337d1a67-eb3f-47a4-9255-288cb0bb0f82')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-337d1a67-eb3f-47a4-9255-288cb0bb0f82 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"ff[ff['edge weights']>=0\",\n  \"rows\": 11130,\n  \"fields\": [\n    {\n      \"column\": \"Src Ids\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30,\n        \"min\": 1,\n        \"max\": 105,\n        \"num_unique_values\": 105,\n        \"samples\": [\n          31,\n          66,\n          65\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Src_features\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dst_features\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dst Ids\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30,\n        \"min\": 0,\n        \"max\": 105,\n        \"num_unique_values\": 106,\n        \"samples\": [\n          100,\n          10,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"edge weights\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08643160780029656,\n        \"min\": 0.4642524192687743,\n        \"max\": 1.0000000000000002,\n        \"num_unique_values\": 5569,\n        \"samples\": [\n          0.7634911167136536,\n          0.8232823138887189,\n          0.8016854706215321\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "beb96876-26f6-4e13-9109-12ac34c05812",
      "metadata": {
        "id": "beb96876-26f6-4e13-9109-12ac34c05812",
        "outputId": "9a570992-34c6-4e66-ad5d-68889911e956",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    5565.000000\n",
            "mean        0.753111\n",
            "std         0.083432\n",
            "min         0.464252\n",
            "25%         0.700562\n",
            "50%         0.759550\n",
            "75%         0.811976\n",
            "max         0.989819\n",
            "Name: edge weights, dtype: float64\n",
            "95th percentile of edge weights column: 0.8765416432055043\n"
          ]
        }
      ],
      "source": [
        "print(Edge['edge weights'].describe())\n",
        "ninetyfive_percentile = Edge['edge weights'].quantile(0.95)\n",
        "print(\"95th percentile of edge weights column:\", ninetyfive_percentile)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# percentiles=[round(0.852597,4),round(0.879359,4),round(0.900892,4),round(0.9330570901928656,4)]"
      ],
      "metadata": {
        "id": "Nmm7y74Lw_l_"
      },
      "id": "Nmm7y74Lw_l_",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Sample list of values\n",
        "\n",
        "# Calculate the 25th, 75th, and 95th percentiles\n",
        "percentiles = np.percentile(edge_weight, [25, 75, 95])\n",
        "percentile_labels = ['25th Percentile', '75th Percentile', '95th Percentile']\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(edge_weight, marker='o', linestyle='-', color='b', label='Data')\n",
        "\n",
        "# Mark the percentiles\n",
        "for i, perc in enumerate(percentiles):\n",
        "    plt.axhline(y=perc, color='r', linestyle='--')\n",
        "    plt.text(0, perc, f'{percentile_labels[i]}: {perc}', color='r', ha='right', va='bottom')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Data with 25th, 75th, and 95th Percentiles')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "G_rZrPZmt7QT",
        "outputId": "50d4f38e-00a1-4804-bb25-4512863e2ea1"
      },
      "id": "G_rZrPZmt7QT",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edge_weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "b8WbybKvw-Ew",
        "outputId": "4aac09b4-2c10-47b7-e256-6304a81febc4"
      },
      "id": "b8WbybKvw-Ew",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7796408548409581,\n",
              " 0.8328019906396308,\n",
              " 0.7055243988750663,\n",
              " 0.7783259481411773,\n",
              " 0.7303962666612972,\n",
              " 0.7733989820689057,\n",
              " 0.7971934674773552,\n",
              " 0.8466760112771331,\n",
              " 0.8313724121864834,\n",
              " 0.7746195572377169,\n",
              " 0.778993676050981,\n",
              " 0.8064765268039201,\n",
              " 0.8200837856769234,\n",
              " 0.8228050526582315,\n",
              " 0.8505785026801473,\n",
              " 0.8468109835186194,\n",
              " 0.8391249935748182,\n",
              " 0.8594215058992217,\n",
              " 0.8551725073774077,\n",
              " 0.7560562241592254,\n",
              " 0.7408735903624087,\n",
              " 0.8131463158331481,\n",
              " 0.7552695852073049,\n",
              " 0.794431209754382,\n",
              " 0.8188852631308657,\n",
              " 0.8099809697076356,\n",
              " 0.7832908870742487,\n",
              " 0.7501994611411947,\n",
              " 0.802441650726862,\n",
              " 0.6773879384917223,\n",
              " 0.7402084274347283,\n",
              " 0.826395132973987,\n",
              " 0.8640246938569972,\n",
              " 0.8615927779123032,\n",
              " 0.8457298347467002,\n",
              " 0.8767925723939515,\n",
              " 0.864314541154144,\n",
              " 0.8516803080583467,\n",
              " 0.8753059310810704,\n",
              " 0.7597401378730584,\n",
              " 0.6989821107853115,\n",
              " 0.7589187790192264,\n",
              " 0.7377170566296561,\n",
              " 0.7661386361877413,\n",
              " 0.7563664462256254,\n",
              " 0.7862697630233826,\n",
              " 0.7816503644684977,\n",
              " 0.8104640517430881,\n",
              " 0.9005533052885634,\n",
              " 0.8066993992761121,\n",
              " 0.8646650261405682,\n",
              " 0.826110645776907,\n",
              " 0.7863523079447828,\n",
              " 0.801795323567958,\n",
              " 0.8311028311849292,\n",
              " 0.9231921739419817,\n",
              " 0.701927725469513,\n",
              " 0.7480461951921289,\n",
              " 0.7318169417321649,\n",
              " 0.7311724930486684,\n",
              " 0.7678019375628048,\n",
              " 0.7338151232112742,\n",
              " 0.582963933717195,\n",
              " 0.6440186961901881,\n",
              " 0.609786700120657,\n",
              " 0.6983756967535225,\n",
              " 0.6695488044855187,\n",
              " 0.6662753941285272,\n",
              " 0.589769352188224,\n",
              " 0.6568451085461169,\n",
              " 0.6692340445169382,\n",
              " 0.5164021231544749,\n",
              " 0.5894602244527993,\n",
              " 0.6035259975346919,\n",
              " 0.5914965179203857,\n",
              " 0.5462622254909415,\n",
              " 0.8501324027455971,\n",
              " 0.784562355691085,\n",
              " 0.7926639806467298,\n",
              " 0.7904140905799373,\n",
              " 0.7465172911628222,\n",
              " 0.7943379070449321,\n",
              " 0.795757796663388,\n",
              " 0.7487296994442062,\n",
              " 0.733650739122923,\n",
              " 0.8244301689527013,\n",
              " 0.8376633178093935,\n",
              " 0.7229772109592808,\n",
              " 0.745655904758662,\n",
              " 0.8310580311671001,\n",
              " 0.7617467142316542,\n",
              " 0.7170694458634003,\n",
              " 0.7998831805236735,\n",
              " 0.8148819236241078,\n",
              " 0.8314929244833594,\n",
              " 0.7741103863184542,\n",
              " 0.8183901009685742,\n",
              " 0.7918763310486708,\n",
              " 0.785279177278468,\n",
              " 0.7720885494759069,\n",
              " 0.7135651274381434,\n",
              " 0.8610296940177036,\n",
              " 0.7468043103030909,\n",
              " 0.8234180346757355,\n",
              " 0.8140402277562184,\n",
              " 0.7043182283697969,\n",
              " 0.7945689324203579,\n",
              " 0.8238695640466569,\n",
              " 0.7378326367044925,\n",
              " 0.8279850015150197,\n",
              " 0.8905422121689223,\n",
              " 0.8535519911696143,\n",
              " 0.8669770612418826,\n",
              " 0.9220879478925426,\n",
              " 0.9079165252095924,\n",
              " 0.7874424648290822,\n",
              " 0.7972024843175424,\n",
              " 0.7980493277124873,\n",
              " 0.7736913400423552,\n",
              " 0.7813470292125493,\n",
              " 0.7620943077416037,\n",
              " 0.7474900366884655,\n",
              " 0.7550124941596341,\n",
              " 0.765590219895693,\n",
              " 0.6417633753229154,\n",
              " 0.7062503843283134,\n",
              " 0.7000788940289494,\n",
              " 0.7077249952232366,\n",
              " 0.7452493207111037,\n",
              " 0.6913208998534498,\n",
              " 0.7329057092690549,\n",
              " 0.7128482262060637,\n",
              " 0.7544179924838603,\n",
              " 0.6559257247755071,\n",
              " 0.6747870173998596,\n",
              " 0.7083705869103651,\n",
              " 0.7309853443143213,\n",
              " 0.7359127136779353,\n",
              " 0.6885593078221709,\n",
              " 0.7181017887967254,\n",
              " 0.711212973098403,\n",
              " 0.7106612018980699,\n",
              " 0.7040638672234119,\n",
              " 0.7104727408043112,\n",
              " 0.6222899236032637,\n",
              " 0.6818072247680057,\n",
              " 0.7303037953381937,\n",
              " 0.6983102429261427,\n",
              " 0.671784916109972,\n",
              " 0.6979883194427167,\n",
              " 0.7122961143614315,\n",
              " 0.7977344127958569,\n",
              " 0.7386306119985538,\n",
              " 0.6883520531575276,\n",
              " 0.7069048602250312,\n",
              " 0.6874059236426305,\n",
              " 0.6338068131273489,\n",
              " 0.6688730009363885,\n",
              " 0.6766951112033465,\n",
              " 0.7328458253329594,\n",
              " 0.7511410729082052,\n",
              " 0.7231578711452215,\n",
              " 0.7311257884497295,\n",
              " 0.7269541095680747,\n",
              " 0.7327290620655362,\n",
              " 0.7660847958665348,\n",
              " 0.5464359009243581,\n",
              " 0.6321717829951726,\n",
              " 0.5633822109598705,\n",
              " 0.622371349203903,\n",
              " 0.6126151517127456,\n",
              " 0.6188316454161453,\n",
              " 0.5928447647212531,\n",
              " 0.5678801414059707,\n",
              " 0.592873156677486,\n",
              " 0.48961147503584834,\n",
              " 0.5431895389262921,\n",
              " 0.5725890211223287,\n",
              " 0.5714110037920168,\n",
              " 0.5354143739705751,\n",
              " 0.6891852562464457,\n",
              " 0.6639810281625584,\n",
              " 0.7063581651685781,\n",
              " 0.7681497307841858,\n",
              " 0.712709134262905,\n",
              " 0.6830018620918125,\n",
              " 0.661633643638749,\n",
              " 0.7455634856857472,\n",
              " 0.7067831197517371,\n",
              " 0.783614024068581,\n",
              " 0.7590810022323742,\n",
              " 0.7742106869941692,\n",
              " 0.8048329614456398,\n",
              " 0.7653547526673475,\n",
              " 0.738111005370426,\n",
              " 0.7385073201605175,\n",
              " 0.7930032009820891,\n",
              " 0.7799774008249044,\n",
              " 0.7652764990446782,\n",
              " 0.7928665670100131,\n",
              " 0.7691656911021193,\n",
              " 0.7959428599770599,\n",
              " 0.7562087036058213,\n",
              " 0.7759876320939467,\n",
              " 0.7883855704065416,\n",
              " 0.7499891737783142,\n",
              " 0.7173585695277792,\n",
              " 0.7653447632973381,\n",
              " 0.8031221162388871,\n",
              " 0.7819142107239305,\n",
              " 0.8098790605749524,\n",
              " 0.8673275909559576,\n",
              " 0.8022278312346367,\n",
              " 0.758308128471693,\n",
              " 0.8034384568769855,\n",
              " 0.817856011037931,\n",
              " 0.7391528738316714,\n",
              " 0.6674632535870382,\n",
              " 0.7115366379491589,\n",
              " 0.753250682880097,\n",
              " 0.7676241241447184,\n",
              " 0.7845441075251838,\n",
              " 0.8025735169218885,\n",
              " 0.7840672556914079,\n",
              " 0.8079144634856105,\n",
              " 0.8336096585203234,\n",
              " 0.7740663559519269,\n",
              " 0.7213734096925096,\n",
              " 0.8054596384348836,\n",
              " 0.7811244609321455,\n",
              " 0.8194886082843298,\n",
              " 0.8266412687602237,\n",
              " 0.8285363414097773,\n",
              " 0.7889435686455287,\n",
              " 0.7522106397087659,\n",
              " 0.7983223760859801,\n",
              " 0.7030074367047824,\n",
              " 0.7361768796762834,\n",
              " 0.826243891759127,\n",
              " 0.8327012698939832,\n",
              " 0.8461084756989947,\n",
              " 0.8465794238320969,\n",
              " 0.8492485117461749,\n",
              " 0.8463197726815169,\n",
              " 0.8557828218546704,\n",
              " 0.8252657097985691,\n",
              " 0.7932749868678299,\n",
              " 0.8226520544030647,\n",
              " 0.805139923871425,\n",
              " 0.7682480823422009,\n",
              " 0.8494647173051625,\n",
              " 0.8050135390565515,\n",
              " 0.8354490674852666,\n",
              " 0.8068884687146141,\n",
              " 0.7678402678227985,\n",
              " 0.8407524549205578,\n",
              " 0.8308251931680408,\n",
              " 0.8234213937042941,\n",
              " 0.858840744271672,\n",
              " 0.8325742848982051,\n",
              " 0.8505078631518016,\n",
              " 0.8426252924909106,\n",
              " 0.8104233974587359,\n",
              " 0.68249552675531,\n",
              " 0.7351113404634572,\n",
              " 0.732480009857706,\n",
              " 0.6833139907347278,\n",
              " 0.7241626128431303,\n",
              " 0.691963740769763,\n",
              " 0.6152017229238934,\n",
              " 0.6542416541788967,\n",
              " 0.6500600370165072,\n",
              " 0.6864169982724725,\n",
              " 0.679529126631472,\n",
              " 0.6945726895283636,\n",
              " 0.5696328127866066,\n",
              " 0.6586506056917136,\n",
              " 0.6894483443450433,\n",
              " 0.5526658791370651,\n",
              " 0.6155837475160424,\n",
              " 0.6425563675706236,\n",
              " 0.6316914882571371,\n",
              " 0.5680421487529929,\n",
              " 0.8401168891437754,\n",
              " 0.8346258837186765,\n",
              " 0.780744714731232,\n",
              " 0.7670032159514235,\n",
              " 0.7135392400427135,\n",
              " 0.7475108742224474,\n",
              " 0.793728536342278,\n",
              " 0.7163123240438435,\n",
              " 0.8004532472439464,\n",
              " 0.8403030308371078,\n",
              " 0.8619143697914856,\n",
              " 0.7340977581728623,\n",
              " 0.7015114010880334,\n",
              " 0.7813074378417437,\n",
              " 0.6838453727646819,\n",
              " 0.665751738336515,\n",
              " 0.7964294752408835,\n",
              " 0.8078319127503449,\n",
              " 0.8118549941576886,\n",
              " 0.7443448140658557,\n",
              " 0.7592421963489718,\n",
              " 0.800128209870191,\n",
              " 0.8103720092035219,\n",
              " 0.762227193127944,\n",
              " 0.7103764286453783,\n",
              " 0.8211180868578911,\n",
              " 0.7682514758653601,\n",
              " 0.8187001293439565,\n",
              " 0.8203678573076313,\n",
              " 0.8203683894479178,\n",
              " 0.8181357458837047,\n",
              " 0.8414099876117639,\n",
              " 0.8020292988035892,\n",
              " 0.8502581528087921,\n",
              " 0.8645900596351361,\n",
              " 0.8306615062539618,\n",
              " 0.7211744871484741,\n",
              " 0.6724966986767674,\n",
              " 0.728234909420139,\n",
              " 0.7302266366234073,\n",
              " 0.74196293402585,\n",
              " 0.7550104709730896,\n",
              " 0.726299790931795,\n",
              " 0.7382851019947807,\n",
              " 0.7728877071371711,\n",
              " 0.7877732367952381,\n",
              " 0.6541656459036808,\n",
              " 0.7400823763633092,\n",
              " 0.7563810592819181,\n",
              " 0.7602428347650833,\n",
              " 0.8006239334079148,\n",
              " 0.7583724465469204,\n",
              " 0.7692239831883143,\n",
              " 0.7680986632142712,\n",
              " 0.7857395010976745,\n",
              " 0.7143222678310446,\n",
              " 0.738591088627783,\n",
              " 0.7738639380804332,\n",
              " 0.7158563475856756,\n",
              " 0.7511729547232544,\n",
              " 0.7393368303527306,\n",
              " 0.7471111999648126,\n",
              " 0.7478996005049093,\n",
              " 0.756219812698325,\n",
              " 0.7043060872663005,\n",
              " 0.7320515161374832,\n",
              " 0.7396001091532612,\n",
              " 0.7401240348950414,\n",
              " 0.7660608363057178,\n",
              " 0.8041618090247367,\n",
              " 0.7566257835914113,\n",
              " 0.7671642196113841,\n",
              " 0.7604439146252389,\n",
              " 0.7748382365640962,\n",
              " 0.7177134140668385,\n",
              " 0.7464720571416501,\n",
              " 0.7197246122391063,\n",
              " 0.7595499088087303,\n",
              " 0.6981385215466984,\n",
              " 0.7204481891287972,\n",
              " 0.7292292352906647,\n",
              " 0.6906816718946854,\n",
              " 0.6874317962143875,\n",
              " 0.6977244873285983,\n",
              " 0.723591227088177,\n",
              " 0.6536218933355666,\n",
              " 0.6966314038741975,\n",
              " 0.6766347804713653,\n",
              " 0.6233482323207133,\n",
              " 0.6801128668560009,\n",
              " 0.6473868157695593,\n",
              " 0.6614751288271861,\n",
              " 0.6937661760220584,\n",
              " 0.6987152796956092,\n",
              " 0.598565180115992,\n",
              " 0.6262204873585809,\n",
              " 0.6563299630393186,\n",
              " 0.567770506745488,\n",
              " 0.6083833930037443,\n",
              " 0.6411210225653897,\n",
              " 0.6365347148663217,\n",
              " 0.59012215577652,\n",
              " 0.7292118895036059,\n",
              " 0.7490704596701636,\n",
              " 0.764236180890249,\n",
              " 0.7982514467869561,\n",
              " 0.7450131423246361,\n",
              " 0.6989769557062985,\n",
              " 0.6840238099313164,\n",
              " 0.719339230917878,\n",
              " 0.8029992431036517,\n",
              " 0.8085579258614534,\n",
              " 0.7972541010726272,\n",
              " 0.7803101145950091,\n",
              " 0.7241212962530239,\n",
              " 0.7379378133139145,\n",
              " 0.6565236753101816,\n",
              " 0.7060954458897051,\n",
              " 0.7831795817928272,\n",
              " 0.7988958900921039,\n",
              " 0.771653346350104,\n",
              " 0.7860608663675935,\n",
              " 0.7433189891305189,\n",
              " 0.7624555397770884,\n",
              " 0.7890442157839412,\n",
              " 0.75516037340041,\n",
              " 0.8051655146096137,\n",
              " 0.7461495146666903,\n",
              " 0.7621017672364911,\n",
              " 0.7642660197221436,\n",
              " 0.7771481650271642,\n",
              " 0.832672776051502,\n",
              " 0.8612771549655625,\n",
              " 0.868479808940139,\n",
              " 0.8276903941015935,\n",
              " 0.8442740100287736,\n",
              " 0.819309498047678,\n",
              " 0.7612308832420611,\n",
              " 0.6834239372984381,\n",
              " 0.7169848703921871,\n",
              " 0.7305462341955048,\n",
              " 0.7396837618258029,\n",
              " 0.7435732282618882,\n",
              " 0.7186372327945746,\n",
              " 0.765352398037433,\n",
              " 0.8128122655028848,\n",
              " 0.8193684100894776,\n",
              " 0.6837616753151811,\n",
              " 0.7611301953747157,\n",
              " 0.7776700267104282,\n",
              " 0.8038854183261324,\n",
              " 0.8116096325472146,\n",
              " 0.7747041587329203,\n",
              " 0.7716266932618152,\n",
              " 0.7518411606555941,\n",
              " 0.7629905535488107,\n",
              " 0.6749098796458848,\n",
              " 0.7000737645399656,\n",
              " 0.7798007507338803,\n",
              " 0.750171225809552,\n",
              " 0.7565595382758802,\n",
              " 0.7270005191516776,\n",
              " 0.7532079256878533,\n",
              " 0.7458677663779528,\n",
              " 0.744949467301159,\n",
              " 0.7213924405201904,\n",
              " 0.7722417498441334,\n",
              " 0.6963018759650738,\n",
              " 0.7498059939616618,\n",
              " 0.7777054259878032,\n",
              " 0.8111882613779319,\n",
              " 0.7358510585615151,\n",
              " 0.7725860621713309,\n",
              " 0.7717746146526924,\n",
              " 0.8058016140231438,\n",
              " 0.8009820532391314,\n",
              " 0.810804074042036,\n",
              " 0.7790481027715094,\n",
              " 0.79326912950281,\n",
              " 0.7399628955945298,\n",
              " 0.7514832700742373,\n",
              " 0.7800890949274092,\n",
              " 0.7785733422601313,\n",
              " 0.7408170610685165,\n",
              " 0.7774704620944334,\n",
              " 0.772255148419318,\n",
              " 0.765895293937422,\n",
              " 0.7447493222435685,\n",
              " 0.7311209036525036,\n",
              " 0.6066434033815421,\n",
              " 0.6877610160788058,\n",
              " 0.6327292319745575,\n",
              " 0.6849741417376805,\n",
              " 0.6801256857690893,\n",
              " 0.6962986163401221,\n",
              " 0.6760424829365561,\n",
              " 0.6449148527130071,\n",
              " 0.6679849919067969,\n",
              " 0.5485923596831205,\n",
              " 0.6010078320318764,\n",
              " 0.6389164862527301,\n",
              " 0.6623516980063404,\n",
              " 0.5976217955529145,\n",
              " 0.7416249824328747,\n",
              " 0.7483268514566241,\n",
              " 0.7394839530070262,\n",
              " 0.7606870031935549,\n",
              " 0.7272811806815289,\n",
              " 0.7109964771440028,\n",
              " 0.713452858625786,\n",
              " 0.7513981030629493,\n",
              " 0.7819117926902115,\n",
              " 0.8348142883008173,\n",
              " 0.8240810002618121,\n",
              " 0.8033906824159947,\n",
              " 0.7681390053311424,\n",
              " 0.7752080537391219,\n",
              " 0.7770546630302403,\n",
              " 0.7305109050577603,\n",
              " 0.8030239811580883,\n",
              " 0.8097445171321587,\n",
              " 0.8107816343108912,\n",
              " 0.8124335624490008,\n",
              " 0.7621452752913276,\n",
              " 0.8279862460857393,\n",
              " 0.8086160767158471,\n",
              " 0.8149393299216567,\n",
              " 0.782896345998558,\n",
              " 0.8047615481245517,\n",
              " 0.7909472319252803,\n",
              " 0.7856589222935011,\n",
              " 0.8428483903270162,\n",
              " 0.8069428205433734,\n",
              " 0.7667848885486834,\n",
              " 0.8016056146216356,\n",
              " 0.8382470104378406,\n",
              " 0.7666587865543137,\n",
              " 0.687985265600981,\n",
              " 0.6852090850786862,\n",
              " 0.7336347972017363,\n",
              " 0.7486675426223315,\n",
              " 0.7691582331616057,\n",
              " 0.7723791007100221,\n",
              " 0.7386561802625657,\n",
              " 0.7442532919049983,\n",
              " 0.8015398215624363,\n",
              " 0.7748063589586306,\n",
              " 0.663883572050242,\n",
              " 0.7526403550460624,\n",
              " 0.7724873860511486,\n",
              " 0.7963736432140659,\n",
              " 0.8155363161483012,\n",
              " 0.7785966678962862,\n",
              " 0.7647870088330906,\n",
              " 0.752082520046471,\n",
              " 0.7512986409899138,\n",
              " 0.690267646079751,\n",
              " 0.7098753464111379,\n",
              " 0.7930158616640269,\n",
              " 0.7768962352121915,\n",
              " 0.7961467281916543,\n",
              " 0.7793879860105922,\n",
              " 0.7979068222879503,\n",
              " 0.7962606870165071,\n",
              " 0.8039587083743732,\n",
              " 0.7618762236769436,\n",
              " 0.8069751703640037,\n",
              " 0.8285772399407565,\n",
              " 0.8202671027488909,\n",
              " 0.8104738672031002,\n",
              " 0.8587177569387558,\n",
              " 0.808675443196502,\n",
              " 0.8354651986738023,\n",
              " 0.8302366448053468,\n",
              " 0.7788867464458508,\n",
              " 0.7691021066381383,\n",
              " 0.8086849551658961,\n",
              " 0.7625978206699189,\n",
              " 0.8026131206947129,\n",
              " 0.7785730778612796,\n",
              " 0.8016854706215321,\n",
              " 0.7937880817499885,\n",
              " 0.7255057814063598,\n",
              " 0.7062563885006518,\n",
              " 0.7282118293086701,\n",
              " 0.747371432557249,\n",
              " 0.6795248413303105,\n",
              " 0.7179394644850464,\n",
              " 0.6951860881140202,\n",
              " 0.6347899149943264,\n",
              " 0.6817086457726951,\n",
              " 0.6730003498807436,\n",
              " 0.668748311490143,\n",
              " 0.699873672425879,\n",
              " 0.6987604710543063,\n",
              " 0.6207542834204899,\n",
              " 0.6409981792382189,\n",
              " 0.6849010278593853,\n",
              " 0.5940908154351225,\n",
              " 0.6344188599311535,\n",
              " 0.657897002397233,\n",
              " 0.6498464177360698,\n",
              " 0.6009792507657892,\n",
              " 0.7757651289155223,\n",
              " 0.7920055374937607,\n",
              " 0.7547639168471507,\n",
              " 0.762443646479824,\n",
              " 0.7065843900591016,\n",
              " 0.703512946661949,\n",
              " 0.7432028667022214,\n",
              " 0.6975609839000776,\n",
              " 0.8261979170027323,\n",
              " 0.8467743677118004,\n",
              " 0.8632490133434483,\n",
              " 0.7776615133071396,\n",
              " 0.725789408655366,\n",
              " 0.7639395196013011,\n",
              " 0.6698512646854449,\n",
              " 0.6889334425071377,\n",
              " 0.795059673043035,\n",
              " 0.7947303612775362,\n",
              " 0.8026325447440346,\n",
              " 0.7729270004403276,\n",
              " 0.7651349886486525,\n",
              " 0.8122329585265539,\n",
              " 0.8260034536150871,\n",
              " 0.7939509782664274,\n",
              " 0.7770433698224259,\n",
              " 0.7781898621884864,\n",
              " 0.800128432339837,\n",
              " 0.813665571612758,\n",
              " 0.8087250386604751,\n",
              " 0.8970884376999109,\n",
              " 0.8731631404474539,\n",
              " 0.8877820781634138,\n",
              " 0.8769691864849093,\n",
              " 0.7273853311719766,\n",
              " 0.6987308029787452,\n",
              " 0.7257806049640217,\n",
              " 0.7398707284874119,\n",
              " 0.7547718200605161,\n",
              " 0.7610194196412357,\n",
              " 0.7449403819441367,\n",
              " 0.8025197770729845,\n",
              " 0.8148588596771721,\n",
              " 0.8136226225540643,\n",
              " 0.7026285590667343,\n",
              " 0.7714795570669154,\n",
              " 0.7862156297184292,\n",
              " 0.8011741024247577,\n",
              " 0.8021987897927787,\n",
              " 0.7973570953654875,\n",
              " 0.8141946821202893,\n",
              " 0.7883418908283676,\n",
              " 0.8186766505734427,\n",
              " 0.7349585361934013,\n",
              " 0.7751724484878817,\n",
              " 0.8153404748909177,\n",
              " 0.7618528834290207,\n",
              " 0.7820848362063887,\n",
              " 0.7607627268283841,\n",
              " 0.785463672073137,\n",
              " 0.7799734195418773,\n",
              " 0.7968677972883934,\n",
              " 0.7401053427497816,\n",
              " 0.7445371856884205,\n",
              " 0.6918641051507641,\n",
              " 0.7528753145904832,\n",
              " 0.7638759765617256,\n",
              " 0.8040525825395494,\n",
              " 0.7270076447523676,\n",
              " 0.7804482453291076,\n",
              " 0.7535066877850591,\n",
              " 0.7759891627426719,\n",
              " 0.7702889898274061,\n",
              " 0.7696901748067435,\n",
              " 0.7686152200668395,\n",
              " 0.7901036209426405,\n",
              " 0.7044207460827775,\n",
              " 0.7600862229690293,\n",
              " 0.7575507157635137,\n",
              " 0.7534965889993459,\n",
              " 0.674246475747489,\n",
              " 0.7246479037022342,\n",
              " 0.7126428032260373,\n",
              " 0.6923832069997845,\n",
              " 0.6981550918665844,\n",
              " 0.68619327910487,\n",
              " 0.630307440568584,\n",
              " 0.7150547176195629,\n",
              " 0.6522326808342834,\n",
              " 0.6804200580452868,\n",
              " 0.6852097919896168,\n",
              " 0.6981728114459154,\n",
              " 0.5920931459431668,\n",
              " 0.6509267168174372,\n",
              " 0.6785583457416479,\n",
              " 0.5780015245849971,\n",
              " 0.638287121690462,\n",
              " 0.6464888689906777,\n",
              " 0.6446769050761103,\n",
              " 0.6144301793252624,\n",
              " 0.7688757237231637,\n",
              " 0.7443107077739678,\n",
              " 0.7719684513271502,\n",
              " 0.8260813557473471,\n",
              " 0.7430851991498536,\n",
              " 0.7556546117272768,\n",
              " 0.7410235070865674,\n",
              " 0.7198492606948397,\n",
              " 0.7954695090137566,\n",
              " 0.8203865896988749,\n",
              " 0.8055313583188666,\n",
              " 0.7905775733053482,\n",
              " 0.7399603296533763,\n",
              " 0.7779643256857082,\n",
              " 0.7144746361902784,\n",
              " 0.68483896338132,\n",
              " 0.8030722734226621,\n",
              " 0.8226886429986967,\n",
              " 0.7679585551795659,\n",
              " 0.7710177719613425,\n",
              " 0.7495096904905505,\n",
              " 0.7893986830665537,\n",
              " 0.7799711329356266,\n",
              " 0.7533620127927667,\n",
              " 0.7419723972596588,\n",
              " 0.7785416467419048,\n",
              " 0.7687930306550859,\n",
              " 0.7491050311889523,\n",
              " 0.7813461571627957,\n",
              " 0.8577894079137202,\n",
              " 0.8874736338146365,\n",
              " 0.9014931499706103,\n",
              " 0.8182381162938449,\n",
              " 0.7471256974830096,\n",
              " 0.7524458462615515,\n",
              " 0.7605429208858046,\n",
              " 0.7470232254058232,\n",
              " 0.7612201528672011,\n",
              " 0.7530670640029512,\n",
              " 0.7786318612731339,\n",
              " 0.7983225915382216,\n",
              " 0.7950683724695524,\n",
              " 0.6731458240547962,\n",
              " 0.7422956414725957,\n",
              " 0.7545012009389748,\n",
              " 0.7703006752829259,\n",
              " 0.7831719832903382,\n",
              " 0.7505649571793946,\n",
              " 0.7705682582606124,\n",
              " 0.739942703492127,\n",
              " 0.7838847598862267,\n",
              " 0.6755290772186483,\n",
              " 0.7131181863196173,\n",
              " 0.7708024348439962,\n",
              " 0.7600573838346024,\n",
              " 0.7658511800697257,\n",
              " 0.7292697015221794,\n",
              " 0.7496028624588216,\n",
              " 0.7472586977481398,\n",
              " 0.7482817816096449,\n",
              " 0.7281375234042423,\n",
              " 0.7313344948128545,\n",
              " 0.6582749365469057,\n",
              " 0.7105332168930816,\n",
              " 0.7440987963007443,\n",
              " 0.7423538186997026,\n",
              " 0.6905542594318262,\n",
              " 0.7367844747360832,\n",
              " 0.7307552396462785,\n",
              " 0.7892350512963463,\n",
              " 0.7719307520159673,\n",
              " 0.7347181102054934,\n",
              " 0.7497546716421672,\n",
              " 0.7509431417191046,\n",
              " 0.6827691690542823,\n",
              " 0.7218204762429696,\n",
              " 0.7102674942803028,\n",
              " 0.7658411459884465,\n",
              " 0.7296504383817374,\n",
              " 0.7485446992533167,\n",
              " 0.7209462521740192,\n",
              " 0.7382250859035794,\n",
              " 0.7309057354215838,\n",
              " 0.7458548700470004,\n",
              " 0.572811190959619,\n",
              " 0.6645242952559982,\n",
              " 0.5943386637941409,\n",
              " 0.6485110991869228,\n",
              " 0.6358660204509103,\n",
              " 0.6588571592004201,\n",
              " 0.6109171653920701,\n",
              " 0.6108818529590857,\n",
              " 0.6250019619399849,\n",
              " 0.5087605435321274,\n",
              " 0.5792180216067901,\n",
              " 0.5960108078077365,\n",
              " 0.6088074639849892,\n",
              " 0.568920978284382,\n",
              " 0.7331399882397509,\n",
              " 0.705441117206935,\n",
              " 0.7464294284787014,\n",
              " 0.7873178130838226,\n",
              " 0.7170712525216586,\n",
              " 0.7236051139026191,\n",
              " 0.7093752779927263,\n",
              " 0.7351475514792818,\n",
              " 0.7445801578116269,\n",
              " 0.8076090792963858,\n",
              " 0.7930135378995528,\n",
              " 0.7820093689055961,\n",
              " 0.7887325104696784,\n",
              " 0.7856376876944953,\n",
              " 0.7591084285224134,\n",
              " 0.7181780211764206,\n",
              " 0.7993550688204384,\n",
              " 0.8090047639597919,\n",
              " 0.7704354347276611,\n",
              " 0.7896009293444082,\n",
              " 0.7645437753351904,\n",
              " 0.8174426270191144,\n",
              " 0.7778763928801535,\n",
              " 0.7745011051481825,\n",
              " 0.7578500877341273,\n",
              " 0.7655764936147484,\n",
              " 0.7405887271871797,\n",
              " 0.7593952556882271,\n",
              " 0.8101047374397228,\n",
              " 0.9349209906179051,\n",
              " 0.8866087224725093,\n",
              " 0.7817998552261519,\n",
              " 0.7480062141539943,\n",
              " 0.7875217936828331,\n",
              " 0.8028414024707452,\n",
              " 0.8334857677165529,\n",
              " 0.821166469958547,\n",
              " 0.803835184341591,\n",
              " 0.8318786761960313,\n",
              " 0.8380346410248395,\n",
              " 0.805962332164402,\n",
              " 0.7165994932651585,\n",
              " 0.7931131618949928,\n",
              " 0.7774210958821279,\n",
              " 0.7850277093181789,\n",
              " 0.8146326007423761,\n",
              " 0.8046852554510256,\n",
              " 0.8193346739721442,\n",
              " 0.7886250958511214,\n",
              " 0.8399203320537874,\n",
              " 0.6996580082471702,\n",
              " 0.7484990625263519,\n",
              " 0.8047016551620662,\n",
              " 0.8184472058079375,\n",
              " 0.8321844571674109,\n",
              " 0.802862099483253,\n",
              " 0.8415829221532422,\n",
              " 0.8185095229618637,\n",
              " 0.8498842502368379,\n",
              " 0.8156579574331015,\n",
              " 0.7628994195084521,\n",
              " 0.7223173117246359,\n",
              " 0.7679520647647847,\n",
              " 0.7776731349000802,\n",
              " 0.7999555433048896,\n",
              " 0.760624718519964,\n",
              " 0.7987756890990579,\n",
              " 0.7820062188315661,\n",
              " 0.8399845520775293,\n",
              " 0.8311610134847855,\n",
              " 0.7960273444444596,\n",
              " 0.8084706087782642,\n",
              " 0.807461147437863,\n",
              " 0.7106465826023646,\n",
              " 0.7669742096810647,\n",
              " 0.7935984713847477,\n",
              " 0.8232823138887189,\n",
              " 0.6991149247394715,\n",
              " 0.7363093810816644,\n",
              " 0.754521100734219,\n",
              " 0.6997986417097092,\n",
              " 0.732419894065328,\n",
              " 0.7243706475458319,\n",
              " 0.5881221185168639,\n",
              " 0.6718117847438737,\n",
              " 0.6125374733120256,\n",
              " 0.6828850333769291,\n",
              " 0.6645902576222268,\n",
              " 0.6731755640065235,\n",
              " 0.5866430612537812,\n",
              " 0.6189799323953535,\n",
              " 0.6530652305375063,\n",
              " 0.523460329831519,\n",
              " 0.5978322632206778,\n",
              " 0.6135235186929207,\n",
              " 0.5950364571778166,\n",
              " 0.557963315847702,\n",
              " 0.8005393074935347,\n",
              " 0.7794829432885965,\n",
              " 0.7920361927410778,\n",
              " 0.8092031261287332,\n",
              " 0.7783529934079709,\n",
              " 0.7584106526021707,\n",
              " 0.7733945501941272,\n",
              " 0.7474453377175517,\n",
              " 0.7725351153498671,\n",
              " 0.8505017717160325,\n",
              " 0.8406368102369342,\n",
              " 0.7856011156160212,\n",
              " 0.7593291789193166,\n",
              " 0.8175161110764096,\n",
              " 0.7441685858267056,\n",
              " 0.7470624037474748,\n",
              " 0.8053535444600716,\n",
              " 0.8463453439071559,\n",
              " 0.8201325433145104,\n",
              " 0.8061870587902167,\n",
              " 0.7948146701117293,\n",
              " 0.8025710801475193,\n",
              " 0.8016006041281097,\n",
              " 0.791197708090353,\n",
              " 0.7963651422366732,\n",
              " 0.8166919056620088,\n",
              " 0.772095459143098,\n",
              " 0.8008769374788569,\n",
              " 0.8084792743880405,\n",
              " 0.9264461228990024,\n",
              " 0.8108187788602488,\n",
              " 0.7836433932614826,\n",
              " 0.81380264485215,\n",
              " 0.8351071753651164,\n",
              " 0.8489123002902162,\n",
              " 0.846574551772008,\n",
              " 0.8224083138622384,\n",
              " 0.8345516298263592,\n",
              " 0.8548997556838829,\n",
              " 0.8221314892937165,\n",
              " 0.7114624726667659,\n",
              " 0.7979209338313548,\n",
              " 0.7841155180136636,\n",
              " 0.8128403994787665,\n",
              " 0.8481396495152953,\n",
              " 0.8160370244291211,\n",
              " 0.8392737303951406,\n",
              " 0.7975972256764118,\n",
              " 0.8534304033186023,\n",
              " 0.7263406115123466,\n",
              " 0.770406776063472,\n",
              " 0.8446436540851885,\n",
              " 0.8301808263333483,\n",
              " 0.8465562453810258,\n",
              " 0.8232776110974365,\n",
              " 0.8472434843975415,\n",
              " 0.8357724614405803,\n",
              " 0.8421895938659317,\n",
              " 0.8207289812467612,\n",
              " 0.7875229707318472,\n",
              " 0.7449659299464325,\n",
              " 0.8086809189239356,\n",
              " 0.8120376597037287,\n",
              " 0.8356032907839367,\n",
              " 0.7979244384532495,\n",
              " 0.8199648074247664,\n",
              " 0.8212559558183307,\n",
              " 0.8706385305335999,\n",
              " 0.8300289220856659,\n",
              " 0.7815614177697117,\n",
              " 0.8094646297953539,\n",
              " 0.8068068989209617,\n",
              " 0.7336961137862876,\n",
              " 0.7814264785540407,\n",
              " 0.7749997601749004,\n",
              " 0.8118262953372979,\n",
              " 0.7120730057796186,\n",
              " 0.7297530032707816,\n",
              " 0.7425297802430011,\n",
              " 0.6951937895479371,\n",
              " 0.7246486055369431,\n",
              " 0.728222716702313,\n",
              " 0.6289131602028207,\n",
              " 0.708389615939002,\n",
              " 0.6569832594678414,\n",
              " 0.6962096134673446,\n",
              " 0.706774837092776,\n",
              " 0.7111204897351331,\n",
              " 0.5945588367106331,\n",
              " 0.6448528324725878,\n",
              " 0.6907811954731106,\n",
              " 0.5561663232121814,\n",
              " 0.6312767892416468,\n",
              " 0.6556114731182769,\n",
              " 0.6453949495140411,\n",
              " 0.5998236307914682,\n",
              " 0.8192388813505829,\n",
              " 0.7857611992092326,\n",
              " 0.8154572533923254,\n",
              " 0.8523001238819446,\n",
              " 0.7783578586348785,\n",
              " 0.7573167618963357,\n",
              " 0.7646764488835757,\n",
              " 0.7592726281640423,\n",
              " 0.8031662050255944,\n",
              " 0.8754428791199108,\n",
              " 0.8641996504556955,\n",
              " 0.8219881614702695,\n",
              " 0.7930690031945664,\n",
              " 0.8062790220196075,\n",
              " 0.7321897439767827,\n",
              " 0.7619682516016291,\n",
              " 0.8555400145530747,\n",
              " 0.8681908812043972,\n",
              " 0.8329407956402182,\n",
              " 0.8357296663904896,\n",
              " 0.8106093134552208,\n",
              " 0.8339597591029893,\n",
              " 0.8297335072640711,\n",
              " 0.8150535567834006,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Sample list of values\n",
        "\n",
        "# Fit data to a normal distribution\n",
        "mu, std = stats.norm.fit(edge_weight)\n",
        "\n",
        "# Generate a range of values for the x-axis\n",
        "xmin, xmax = min(edge_weight), max(edge_weight)\n",
        "x = np.linspace(xmin, xmax, 100)\n",
        "\n",
        "# Create the normal distribution curve\n",
        "p = stats.norm.pdf(x, mu, std)\n",
        "\n",
        "# Calculate the 25th, 75th, and 95th percentiles\n",
        "percentiles = np.percentile(edge_weight, [95])\n",
        "percentile_labels = ['95th Percentile']\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot the normal distribution curve\n",
        "plt.plot(x, p, 'k', linewidth=2)\n",
        "\n",
        "# Mark the percentiles\n",
        "for i, perc in enumerate(percentiles):\n",
        "    plt.axvline(x=perc, color='r', linestyle='--')\n",
        "    plt.text(perc, max(p)*0.5, f'{percentile_labels[i]}: {perc:.3f}', color='r', ha='center', va='bottom')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Cosine Similarity Value')\n",
        "plt.ylabel('Density')\n",
        "#plt.title('Cosine similarities with 95th Percentiles')\n",
        "plt.legend()\n",
        "#plt.ylim(0, 5)\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "X5-iHIQdvPM-",
        "outputId": "06bdb4ed-48c5-43c1-af35-55b7c97fe5f8"
      },
      "id": "X5-iHIQdvPM-",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "f9bf4c21-b02d-4b7f-8b61-18fa54283b2b",
      "metadata": {
        "id": "f9bf4c21-b02d-4b7f-8b61-18fa54283b2b",
        "outputId": "ae38494e-d272-4013-decf-446fa70eef94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph(num_nodes=106, num_edges=11236,\n",
            "      ndata_schemes={'feat': Scheme(shape=(862,), dtype=torch.float64), 'label': Scheme(shape=(), dtype=torch.int64), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}\n",
            "      edata_schemes={'weight': Scheme(shape=(), dtype=torch.float32)})\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import torch\n",
        "from dgl.data import DGLDataset\n",
        "edge_weight=[]\n",
        "\n",
        "\n",
        "class KarateClubDataset(DGLDataset):\n",
        "    def __init__(self, threshold):\n",
        "        self.threshold = threshold\n",
        "        super().__init__(name=\"karate_club\")\n",
        "\n",
        "    def process(self):\n",
        "        edge_remove = []\n",
        "        C = edge_weights\n",
        "        for i in range(len(C)):\n",
        "            if C[i] <= self.threshold:\n",
        "                edge_remove.append(i)\n",
        "            else:\n",
        "                edge_weight.append(C[i])\n",
        "\n",
        "        nodes_data = Nodes_Data\n",
        "        edges_data = Edge_Data\n",
        "\n",
        "        # Fix the non-writable tensor warning\n",
        "        features_array = np.array(nodes_data[\"features\"].tolist(), dtype=float)\n",
        "        node_features = torch.from_numpy(features_array).clone()  # Ensure it's writable\n",
        "\n",
        "        node_labels = torch.tensor(\n",
        "            nodes_data[\"label\"].astype(\"category\").cat.codes.to_numpy(),\n",
        "            dtype=torch.int64  # Ensure proper dtype\n",
        "        )\n",
        "\n",
        "        edge_features = torch.tensor(\n",
        "            edges_data[\"edge weights\"].to_numpy(),\n",
        "            dtype=torch.float32  # Edge weights can stay as float32\n",
        "        )\n",
        "\n",
        "        # Fix data type of edge indices\n",
        "        edges_src = torch.tensor(\n",
        "            edges_data[\"Src Ids\"].to_numpy(),\n",
        "            dtype=torch.int64\n",
        "        )\n",
        "        edges_dst = torch.tensor(\n",
        "            edges_data[\"Dst Ids\"].to_numpy(),\n",
        "            dtype=torch.int64\n",
        "        )\n",
        "\n",
        "        # Build the graph\n",
        "        self.graph = dgl.graph(\n",
        "            (edges_src, edges_dst), num_nodes=nodes_data.shape[0]\n",
        "        )\n",
        "\n",
        "        self.graph.ndata[\"feat\"] = node_features\n",
        "        self.graph.ndata[\"label\"] = node_labels\n",
        "        self.graph.edata[\"weight\"] = edge_features\n",
        "\n",
        "        # Remove edges below the threshold\n",
        "        self.graph = dgl.remove_edges(self.graph, torch.tensor(edge_remove, dtype=torch.int64))\n",
        "\n",
        "        # Create train, validation, and test masks\n",
        "        n_nodes = nodes_data.shape[0]\n",
        "        n_train = int(n_nodes * 0.6)\n",
        "        n_val = int(n_nodes * 0.2)\n",
        "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        train_mask[:n_train] = True\n",
        "        val_mask[n_train: n_train + n_val] = True\n",
        "        test_mask[n_train + n_val:] = True\n",
        "        self.graph.ndata[\"train_mask\"] = train_mask\n",
        "        self.graph.ndata[\"val_mask\"] = val_mask\n",
        "        self.graph.ndata[\"test_mask\"] = test_mask\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.graph\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "dataset = KarateClubDataset(0.231)\n",
        "g = dataset[0]\n",
        "\n",
        "print(g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "d34b2c91-acb3-4b15-b01c-568dfbec3e2b",
      "metadata": {
        "id": "d34b2c91-acb3-4b15-b01c-568dfbec3e2b"
      },
      "outputs": [],
      "source": [
        "new_g = dgl.compact_graphs(g)\n",
        "train_mask = g.ndata[\"train_mask\"]\n",
        "\n",
        "for i in range(106):\n",
        "    train_mask[i] = True\n",
        "\n",
        "indices_to_change = [4, 105, 84, 27, 98, 88, 18, 65, 9, 2, 5, 49, 99, 69, 86, 67, 7, 28, 78, 70, 18, 74]\n",
        "train_mask[indices_to_change] = False\n",
        "g.ndata[\"train_mask\"]=train_mask\n",
        "\n",
        "test_mask = ~train_mask\n",
        "g.ndata[\"test_mask\"]=test_mask\n",
        "\n",
        "Edge_Data_train = Edge_Data[~(Edge_Data['Src Ids'].isin(indices_to_change)) & ~(Edge_Data['Dst Ids'].isin(indices_to_change)) & Edge_Data['edge weights']>0.8258]\n",
        "Edge_Data_train = Edge_Data_train[Edge_Data_train['edge weights']>0.8258]\n",
        "\n",
        "\n",
        "Edge_Data_test = Edge_Data[Edge_Data['Src Ids'].isin(indices_to_change) & Edge_Data['Dst Ids'].isin(indices_to_change) & Edge_Data['edge weights']>0.8258]\n",
        "Edge_Data_test = Edge_Data_test[Edge_Data_test['edge weights']>0.8258]\n",
        "\n",
        "\n",
        "sg_train=dgl.node_subgraph(g, train_mask)\n",
        "\n",
        "sg_test = dgl.node_subgraph(g, test_mask)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "oSfyjv95d87v",
        "outputId": "5693aab6-48fa-4dea-d1f7-ccc5bc881528"
      },
      "id": "oSfyjv95d87v",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     index         0         1         2         3         4         5  \\\n",
              "0        0 -0.048460  0.084757  0.003497 -0.087329  0.059048 -0.209606   \n",
              "1        1 -0.065723  0.025389 -0.025804 -0.130403  0.077852 -0.120231   \n",
              "2        2 -0.045842  0.040817 -0.028311 -0.119993  0.082768 -0.169268   \n",
              "3        3 -0.049971  0.032408 -0.005414 -0.096241  0.066642 -0.137509   \n",
              "4        4 -0.045510  0.020915 -0.008356 -0.120176  0.046261 -0.174338   \n",
              "..     ...       ...       ...       ...       ...       ...       ...   \n",
              "101    101 -0.060480  0.032145  0.005485 -0.084974  0.062038 -0.145933   \n",
              "102    102 -0.055155 -0.000772 -0.013739 -0.069411  0.043931 -0.236069   \n",
              "103    103 -0.040352 -0.013974 -0.021251 -0.069300  0.054273 -0.207470   \n",
              "104    104 -0.051738  0.011170 -0.012154 -0.084336  0.054111 -0.143253   \n",
              "105    105 -0.051058  0.028734 -0.002164 -0.122716  0.081772 -0.161522   \n",
              "\n",
              "            6         7         8  ...       852       853       854  \\\n",
              "0   -0.006506  0.048831  0.017141  ...  0.245732  0.019818 -0.246347   \n",
              "1   -0.001798  0.034838  0.010572  ... -0.439034 -0.363662  0.125037   \n",
              "2   -0.019755  0.051325  0.019362  ... -0.171100  0.180284  0.044292   \n",
              "3   -0.046769  0.021914  0.060602  ...  0.026375  0.007201 -0.165179   \n",
              "4   -0.013932  0.018657  0.020323  ... -0.145272  0.047353 -0.150540   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "101 -0.047886  0.023430 -0.000973  ...  0.227745  0.008060 -0.163510   \n",
              "102  0.007162  0.073971 -0.005430  ... -0.044075  0.357937  0.251036   \n",
              "103 -0.014627  0.039035  0.016669  ... -0.011740  0.419290  0.062435   \n",
              "104 -0.028876  0.015378  0.012340  ... -0.069784  0.047647 -0.162943   \n",
              "105 -0.033399  0.069389 -0.016274  ...  0.070366 -0.196414  0.009736   \n",
              "\n",
              "          855       856       857       858       859       860       861  \n",
              "0   -0.334189  0.001933  0.099690  0.005821  0.098615  0.322466 -0.026514  \n",
              "1   -0.009941  0.000198  0.598846  0.399056 -0.448141 -0.133951  0.163589  \n",
              "2    0.000196  0.003606 -0.028537  0.019995  0.014549 -0.207424 -0.134106  \n",
              "3   -0.136537  0.035306 -0.042689 -0.063572 -0.160413  0.073046 -0.025301  \n",
              "4   -0.009131 -0.106808 -0.137046 -0.175852  0.152502  0.045290 -0.086046  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "101  0.093020 -0.086098  0.027652  0.021020  0.002824 -0.078523 -0.032950  \n",
              "102 -0.005687  0.141986 -0.258837  0.109927 -0.115133 -0.061322 -0.033643  \n",
              "103  0.191918  0.126068  0.287123 -0.040200  0.194610 -0.116119  0.169397  \n",
              "104 -0.053178  0.104861  0.008137 -0.253575 -0.041771 -0.019096  0.024908  \n",
              "105  0.119265  0.025462  0.260401  0.209111 -0.092553  0.108953 -0.142083  \n",
              "\n",
              "[106 rows x 863 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d788d9ee-4f8a-4e87-8629-c7fc1909321b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>852</th>\n",
              "      <th>853</th>\n",
              "      <th>854</th>\n",
              "      <th>855</th>\n",
              "      <th>856</th>\n",
              "      <th>857</th>\n",
              "      <th>858</th>\n",
              "      <th>859</th>\n",
              "      <th>860</th>\n",
              "      <th>861</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.048460</td>\n",
              "      <td>0.084757</td>\n",
              "      <td>0.003497</td>\n",
              "      <td>-0.087329</td>\n",
              "      <td>0.059048</td>\n",
              "      <td>-0.209606</td>\n",
              "      <td>-0.006506</td>\n",
              "      <td>0.048831</td>\n",
              "      <td>0.017141</td>\n",
              "      <td>...</td>\n",
              "      <td>0.245732</td>\n",
              "      <td>0.019818</td>\n",
              "      <td>-0.246347</td>\n",
              "      <td>-0.334189</td>\n",
              "      <td>0.001933</td>\n",
              "      <td>0.099690</td>\n",
              "      <td>0.005821</td>\n",
              "      <td>0.098615</td>\n",
              "      <td>0.322466</td>\n",
              "      <td>-0.026514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.065723</td>\n",
              "      <td>0.025389</td>\n",
              "      <td>-0.025804</td>\n",
              "      <td>-0.130403</td>\n",
              "      <td>0.077852</td>\n",
              "      <td>-0.120231</td>\n",
              "      <td>-0.001798</td>\n",
              "      <td>0.034838</td>\n",
              "      <td>0.010572</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.439034</td>\n",
              "      <td>-0.363662</td>\n",
              "      <td>0.125037</td>\n",
              "      <td>-0.009941</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.598846</td>\n",
              "      <td>0.399056</td>\n",
              "      <td>-0.448141</td>\n",
              "      <td>-0.133951</td>\n",
              "      <td>0.163589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.045842</td>\n",
              "      <td>0.040817</td>\n",
              "      <td>-0.028311</td>\n",
              "      <td>-0.119993</td>\n",
              "      <td>0.082768</td>\n",
              "      <td>-0.169268</td>\n",
              "      <td>-0.019755</td>\n",
              "      <td>0.051325</td>\n",
              "      <td>0.019362</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.171100</td>\n",
              "      <td>0.180284</td>\n",
              "      <td>0.044292</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.003606</td>\n",
              "      <td>-0.028537</td>\n",
              "      <td>0.019995</td>\n",
              "      <td>0.014549</td>\n",
              "      <td>-0.207424</td>\n",
              "      <td>-0.134106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-0.049971</td>\n",
              "      <td>0.032408</td>\n",
              "      <td>-0.005414</td>\n",
              "      <td>-0.096241</td>\n",
              "      <td>0.066642</td>\n",
              "      <td>-0.137509</td>\n",
              "      <td>-0.046769</td>\n",
              "      <td>0.021914</td>\n",
              "      <td>0.060602</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026375</td>\n",
              "      <td>0.007201</td>\n",
              "      <td>-0.165179</td>\n",
              "      <td>-0.136537</td>\n",
              "      <td>0.035306</td>\n",
              "      <td>-0.042689</td>\n",
              "      <td>-0.063572</td>\n",
              "      <td>-0.160413</td>\n",
              "      <td>0.073046</td>\n",
              "      <td>-0.025301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-0.045510</td>\n",
              "      <td>0.020915</td>\n",
              "      <td>-0.008356</td>\n",
              "      <td>-0.120176</td>\n",
              "      <td>0.046261</td>\n",
              "      <td>-0.174338</td>\n",
              "      <td>-0.013932</td>\n",
              "      <td>0.018657</td>\n",
              "      <td>0.020323</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.145272</td>\n",
              "      <td>0.047353</td>\n",
              "      <td>-0.150540</td>\n",
              "      <td>-0.009131</td>\n",
              "      <td>-0.106808</td>\n",
              "      <td>-0.137046</td>\n",
              "      <td>-0.175852</td>\n",
              "      <td>0.152502</td>\n",
              "      <td>0.045290</td>\n",
              "      <td>-0.086046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>101</td>\n",
              "      <td>-0.060480</td>\n",
              "      <td>0.032145</td>\n",
              "      <td>0.005485</td>\n",
              "      <td>-0.084974</td>\n",
              "      <td>0.062038</td>\n",
              "      <td>-0.145933</td>\n",
              "      <td>-0.047886</td>\n",
              "      <td>0.023430</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>...</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.008060</td>\n",
              "      <td>-0.163510</td>\n",
              "      <td>0.093020</td>\n",
              "      <td>-0.086098</td>\n",
              "      <td>0.027652</td>\n",
              "      <td>0.021020</td>\n",
              "      <td>0.002824</td>\n",
              "      <td>-0.078523</td>\n",
              "      <td>-0.032950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>102</td>\n",
              "      <td>-0.055155</td>\n",
              "      <td>-0.000772</td>\n",
              "      <td>-0.013739</td>\n",
              "      <td>-0.069411</td>\n",
              "      <td>0.043931</td>\n",
              "      <td>-0.236069</td>\n",
              "      <td>0.007162</td>\n",
              "      <td>0.073971</td>\n",
              "      <td>-0.005430</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.044075</td>\n",
              "      <td>0.357937</td>\n",
              "      <td>0.251036</td>\n",
              "      <td>-0.005687</td>\n",
              "      <td>0.141986</td>\n",
              "      <td>-0.258837</td>\n",
              "      <td>0.109927</td>\n",
              "      <td>-0.115133</td>\n",
              "      <td>-0.061322</td>\n",
              "      <td>-0.033643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>103</td>\n",
              "      <td>-0.040352</td>\n",
              "      <td>-0.013974</td>\n",
              "      <td>-0.021251</td>\n",
              "      <td>-0.069300</td>\n",
              "      <td>0.054273</td>\n",
              "      <td>-0.207470</td>\n",
              "      <td>-0.014627</td>\n",
              "      <td>0.039035</td>\n",
              "      <td>0.016669</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.011740</td>\n",
              "      <td>0.419290</td>\n",
              "      <td>0.062435</td>\n",
              "      <td>0.191918</td>\n",
              "      <td>0.126068</td>\n",
              "      <td>0.287123</td>\n",
              "      <td>-0.040200</td>\n",
              "      <td>0.194610</td>\n",
              "      <td>-0.116119</td>\n",
              "      <td>0.169397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>104</td>\n",
              "      <td>-0.051738</td>\n",
              "      <td>0.011170</td>\n",
              "      <td>-0.012154</td>\n",
              "      <td>-0.084336</td>\n",
              "      <td>0.054111</td>\n",
              "      <td>-0.143253</td>\n",
              "      <td>-0.028876</td>\n",
              "      <td>0.015378</td>\n",
              "      <td>0.012340</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.069784</td>\n",
              "      <td>0.047647</td>\n",
              "      <td>-0.162943</td>\n",
              "      <td>-0.053178</td>\n",
              "      <td>0.104861</td>\n",
              "      <td>0.008137</td>\n",
              "      <td>-0.253575</td>\n",
              "      <td>-0.041771</td>\n",
              "      <td>-0.019096</td>\n",
              "      <td>0.024908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>105</td>\n",
              "      <td>-0.051058</td>\n",
              "      <td>0.028734</td>\n",
              "      <td>-0.002164</td>\n",
              "      <td>-0.122716</td>\n",
              "      <td>0.081772</td>\n",
              "      <td>-0.161522</td>\n",
              "      <td>-0.033399</td>\n",
              "      <td>0.069389</td>\n",
              "      <td>-0.016274</td>\n",
              "      <td>...</td>\n",
              "      <td>0.070366</td>\n",
              "      <td>-0.196414</td>\n",
              "      <td>0.009736</td>\n",
              "      <td>0.119265</td>\n",
              "      <td>0.025462</td>\n",
              "      <td>0.260401</td>\n",
              "      <td>0.209111</td>\n",
              "      <td>-0.092553</td>\n",
              "      <td>0.108953</td>\n",
              "      <td>-0.142083</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>106 rows Ã— 863 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d788d9ee-4f8a-4e87-8629-c7fc1909321b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d788d9ee-4f8a-4e87-8629-c7fc1909321b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d788d9ee-4f8a-4e87-8629-c7fc1909321b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1f3cf561-9fae-4ce1-93e6-e0204e5fe346\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1f3cf561-9fae-4ce1-93e6-e0204e5fe346')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1f3cf561-9fae-4ce1-93e6-e0204e5fe346 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_8c41428e-442f-4b06-8e7f-fea735a13610\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('X_train_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8c41428e-442f-4b06-8e7f-fea735a13610 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('X_train_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train_df"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g.ndata['feat']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N7xzsCvd_Xy",
        "outputId": "2d1a3319-6a98-495e-83f0-651eaba73e8a"
      },
      "id": "0N7xzsCvd_Xy",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0485,  0.0848,  0.0035,  ...,  0.0986,  0.3225, -0.0265],\n",
              "        [-0.0657,  0.0254, -0.0258,  ..., -0.4481, -0.1340,  0.1636],\n",
              "        [-0.0458,  0.0408, -0.0283,  ...,  0.0145, -0.2074, -0.1341],\n",
              "        ...,\n",
              "        [-0.0404, -0.0140, -0.0213,  ...,  0.1946, -0.1161,  0.1694],\n",
              "        [-0.0517,  0.0112, -0.0122,  ..., -0.0418, -0.0191,  0.0249],\n",
              "        [-0.0511,  0.0287, -0.0022,  ..., -0.0926,  0.1090, -0.1421]],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sg_train.ndata['feat']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nKst0lteHNu",
        "outputId": "ccdb0c56-4f0b-42f9-dd07-091133739c6d"
      },
      "id": "1nKst0lteHNu",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0485,  0.0848,  0.0035,  ...,  0.0986,  0.3225, -0.0265],\n",
              "        [-0.0657,  0.0254, -0.0258,  ..., -0.4481, -0.1340,  0.1636],\n",
              "        [-0.0500,  0.0324, -0.0054,  ..., -0.1604,  0.0730, -0.0253],\n",
              "        ...,\n",
              "        [-0.0552, -0.0008, -0.0137,  ..., -0.1151, -0.0613, -0.0336],\n",
              "        [-0.0404, -0.0140, -0.0213,  ...,  0.1946, -0.1161,  0.1694],\n",
              "        [-0.0517,  0.0112, -0.0122,  ..., -0.0418, -0.0191,  0.0249]],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sg_test.ndata['feat']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3qCwqljeLbl",
        "outputId": "8c4c6397-c069-40b7-83e4-d58a4a3dd6bc"
      },
      "id": "J3qCwqljeLbl",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0458,  0.0408, -0.0283,  ...,  0.0145, -0.2074, -0.1341],\n",
              "        [-0.0455,  0.0209, -0.0084,  ...,  0.1525,  0.0453, -0.0860],\n",
              "        [-0.0363,  0.0129, -0.0055,  ..., -0.0262,  0.2284, -0.0882],\n",
              "        ...,\n",
              "        [-0.0352,  0.0227, -0.0169,  ..., -0.0695, -0.0509, -0.0217],\n",
              "        [-0.0508,  0.0275, -0.0027,  ...,  0.0929,  0.0467,  0.0179],\n",
              "        [-0.0511,  0.0287, -0.0022,  ..., -0.0926,  0.1090, -0.1421]],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "57ba9e91-e07f-435e-aa83-2949c9909810",
      "metadata": {
        "id": "57ba9e91-e07f-435e-aa83-2949c9909810"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GraphConv(in_feats, h_feats,norm='both')\n",
        "        self.conv2 = GraphConv(h_feats, num_classes)\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        in_feat = in_feat.float()\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.elu(h)\n",
        "\n",
        "        h = self.conv2(g, h)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "\n",
        "\n",
        "los=[]\n",
        "train_accuracy=[]\n",
        "test_accuracy=[]\n",
        "epoch=[]\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "def train(sg_train,sg_test, model):\n",
        "    model.train()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "    best_train_acc = 0\n",
        "    best_test_acc = 0\n",
        "\n",
        "    features_train = sg_train.ndata[\"feat\"]\n",
        "    labels = g.ndata[\"label\"]\n",
        "    train_mask = g.ndata[\"train_mask\"]\n",
        "    test_mask = g.ndata[\"test_mask\"]\n",
        "\n",
        "\n",
        "    for e in range(600):\n",
        "        # Forward\n",
        "        logits = model(sg_train,features_train)\n",
        "\n",
        "        # Compute prediction\n",
        "\n",
        "        pred = logits.argmax(1)\n",
        "\n",
        "        labels = g.ndata[\"label\"].long()\n",
        "        loss = F.cross_entropy(logits, labels[train_mask])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            logits_test = model(sg_test,sg_test.ndata[\"feat\"])\n",
        "\n",
        "\n",
        "        pred_test = logits_test.argmax(1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        train_acc = (pred == labels[train_mask]).float().mean()\n",
        "        test_acc = (pred_test == labels[test_mask]).float().mean()\n",
        "\n",
        "        if best_train_acc < train_acc:\n",
        "            best_train_acc = train_acc\n",
        "        if best_test_acc < test_acc:\n",
        "            best_test_acc = test_acc\n",
        "\n",
        "        if e == 510:\n",
        "            # Save features h after the first layer at epoch 1430\n",
        "            import numpy as np\n",
        "\n",
        "            feat_train = features_train.clone()\n",
        "            feat_train = feat_train.float()\n",
        "            h_train = model.conv1(sg_train, feat_train)\n",
        "\n",
        "            h_train = F.elu(h_train)\n",
        "            h_train_numpy = h_train.detach().numpy()  # Convert tensor to NumPy array\n",
        "            np.savetxt('final_animal_train_features.csv',h_train_numpy, delimiter=',')  # Save NumPy array to CSV file\n",
        "            FileLink('final_animal_train_features.csv')\n",
        "\n",
        "        # Backward\n",
        "            graph_features = g.ndata[\"feat\"].clone()\n",
        "            graph_features = graph_features.float()\n",
        "            graph_features = model.conv1(g, graph_features)\n",
        "            graph_features = F.elu(graph_features)\n",
        "\n",
        "\n",
        "            feat_test =  sg_test.ndata[\"feat\"].clone()\n",
        "\n",
        "            feat_test = feat_test.float()\n",
        "            print(type(feat_test[0]))\n",
        "\n",
        "            print(feat_test[0])\n",
        "            print(feat_test[2])\n",
        "            print(feat_test.shape)\n",
        "            h_test = model.conv1(sg_test,feat_test)\n",
        "\n",
        "\n",
        "            feat_test_0_numpy = feat_test[0].detach().numpy()  # Convert tensor to NumPy array\n",
        "            np.savetxt('feat_test_0.csv', feat_test_0_numpy, delimiter=',')  # Save NumPy array to CSV file\n",
        "\n",
        "# To provide a link for downloading in Jupyter Notebook (if you are using it)\n",
        "            FileLink('feat_test_0.csv')\n",
        "\n",
        "            feat_train_0_numpy = feat_train[0].detach().numpy()  # Convert tensor to NumPy array\n",
        "            np.savetxt('feat_train_0.csv', feat_train_0_numpy, delimiter=',')  # Save NumPy array to CSV file\n",
        "\n",
        "# To provide a link for downloading in Jupyter Notebook (if you are using it)\n",
        "            FileLink('feat_train_0.csv')\n",
        "\n",
        "\n",
        "\n",
        "            print(h_test[0])\n",
        "            print(h_test[2])\n",
        "            print(feat_test.shape)\n",
        "            h_test = F.elu(h_test)\n",
        "            h_test_numpy = h_test.detach().numpy()  # Convert tensor to NumPy array\n",
        "            np.savetxt('final_animal_test_features.csv',h_test_numpy, delimiter=',')  # Save NumPy array to CSV file\n",
        "            FileLink('final_animal_test_features.csv')\n",
        "        # Backward\n",
        "\n",
        "\n",
        "# Assuming features_h is a PyTorch tensor\n",
        "            graph_features_numpy = graph_features.detach().numpy()  # Convert tensor to NumPy array\n",
        "            np.savetxt('graph_features.csv',graph_features_numpy, delimiter=',')  # Save NumPy array to CSV file\n",
        "            FileLink('graph_features.csv')\n",
        "\n",
        "\n",
        "            np.savetxt('animal_train_predictions.csv', pred, delimiter=',')\n",
        "            np.savetxt('animal_test_predictions.csv', pred_test, delimiter=',')\n",
        "\n",
        "            FileLink('animal_train_predictions.csv')\n",
        "            FileLink('animal_test_predictions.csv')\n",
        "            print(pred_test)\n",
        "            print(pred_test)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if e % 5 == 0:\n",
        "            epoch.append(e)\n",
        "            los.append(round(loss.item(),3))\n",
        "            train_accuracy.append(round(train_acc.item(),3))\n",
        "            test_accuracy.append(round(test_acc.item(),3))\n",
        "            print(\n",
        "                f\"In epoch {e}, loss: {loss:.3f}, train acc: {train_acc:.3f} (train {best_train_acc:.3f}), test acc: {test_acc:.3f} (best {best_test_acc:.3f})\"\n",
        "            )\n",
        "\n",
        "\n",
        "model = GCN(g.ndata[\"feat\"].shape[1],18, 2)\n",
        "train(sg_train,sg_test,model)"
      ],
      "metadata": {
        "id": "tKH22hLevpUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a9aef8-b7d4-4f2e-a9d0-d56a12e2010b"
      },
      "id": "tKH22hLevpUE",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/tensor.py:352: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  assert input.numel() == input.storage().size(), \"Cannot convert view \" \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In epoch 0, loss: 0.695, train acc: 0.353 (train 0.353), test acc: 0.333 (best 0.333)\n",
            "In epoch 5, loss: 0.678, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 10, loss: 1.380, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 15, loss: 1.110, train acc: 0.353 (train 0.647), test acc: 0.333 (best 0.667)\n",
            "In epoch 20, loss: 0.738, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 25, loss: 0.701, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 30, loss: 0.739, train acc: 0.353 (train 0.647), test acc: 0.333 (best 0.667)\n",
            "In epoch 35, loss: 0.723, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 40, loss: 0.682, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 45, loss: 0.660, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 50, loss: 0.651, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 55, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 60, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 65, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 70, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 75, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 80, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 85, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 90, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 95, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 100, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 105, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 110, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 115, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 120, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 125, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 130, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 135, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 140, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 145, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 150, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 155, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 160, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 165, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 170, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 175, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 180, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 185, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 190, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 195, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 200, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 205, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 210, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 215, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 220, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 225, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 230, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 235, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 240, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 245, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 250, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 255, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 260, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 265, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 270, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 275, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 280, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 285, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 290, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 295, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 300, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 305, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 310, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 315, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 320, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 325, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 330, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 335, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 340, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 345, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 350, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 355, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 360, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 365, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 370, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 375, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 380, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 385, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 390, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 395, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 400, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 405, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 410, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 415, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 420, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 425, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 430, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 435, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 440, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 445, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 450, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 455, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 460, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 465, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 470, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 475, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 480, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 485, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 490, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 495, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 500, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 505, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "<class 'torch.Tensor'>\n",
            "tensor([-4.5842e-02,  4.0817e-02, -2.8311e-02, -1.1999e-01,  8.2768e-02,\n",
            "        -1.6927e-01, -1.9755e-02,  5.1325e-02,  1.9362e-02, -8.2979e-02,\n",
            "        -2.9065e-02,  1.2529e-01,  8.0288e-02, -4.4089e-02,  5.7751e-03,\n",
            "         1.2078e-01, -2.6596e-02,  1.8899e-02,  3.2557e-02, -4.3596e-02,\n",
            "        -8.9118e-02,  3.3935e-02, -2.3321e-02,  8.6089e-02,  1.1712e-02,\n",
            "         1.8047e-02,  3.2403e-02,  8.6993e-02, -7.1517e-02,  2.0603e-02,\n",
            "         2.3303e-02, -2.2535e-02,  1.8156e-02, -3.8378e-02,  3.2014e-02,\n",
            "         5.9845e-02,  5.3781e-02, -1.6890e-02, -4.2748e-02,  2.2731e-02,\n",
            "        -4.2522e-02,  8.5985e-03,  3.8393e-02,  2.2256e-02,  6.7034e-02,\n",
            "        -5.0055e-03,  4.6079e-02,  6.8220e-02, -4.5106e-02, -3.3927e-02,\n",
            "         6.2754e-03,  7.3618e-02, -2.0539e-02, -1.6162e-02, -7.5176e-02,\n",
            "         2.0553e-02,  1.5165e-03,  1.0790e-01,  6.7132e-02, -1.9286e-02,\n",
            "         5.0896e-02, -1.2640e-01, -7.4049e-02,  1.8557e-02,  6.7925e-02,\n",
            "        -1.4312e-02, -3.3919e-02,  6.2641e-03,  3.4596e-02,  2.8648e-02,\n",
            "         1.3276e-01, -5.8336e-02,  8.5206e-02, -4.6256e-02, -1.5752e-02,\n",
            "        -8.7582e-03,  7.7067e-02,  3.2471e-01, -1.2564e-01,  2.3013e-02,\n",
            "         9.9357e-02, -1.4149e-02,  1.8729e-01, -8.4630e-03,  7.0376e-02,\n",
            "        -6.3835e-02,  6.2754e-02,  2.9939e-02,  6.0713e-02,  4.4027e-02,\n",
            "         7.1388e-02,  7.5963e-02, -5.8966e-02,  3.8452e-02, -1.5572e-02,\n",
            "         5.3886e-02,  8.8361e-03, -3.9730e-02, -3.9428e-02, -3.6200e-02,\n",
            "         2.2402e-02, -6.0340e-02,  5.2149e-02,  5.3460e-03,  1.7884e-02,\n",
            "        -1.0841e-02,  8.0111e-02, -1.3977e-02,  6.1288e-02, -4.2260e-02,\n",
            "        -9.1117e-03,  7.5556e-02, -3.2102e-02,  7.0333e-02,  2.1255e-02,\n",
            "        -8.1718e-02,  4.6873e-02, -2.4988e-02,  1.5436e-02,  3.0652e-02,\n",
            "         2.7896e-02,  7.9364e-02,  9.5206e-02, -3.4920e-02, -3.1817e-02,\n",
            "        -5.2013e-02,  1.4213e-02, -4.7726e-02, -4.9141e-02,  2.7613e-02,\n",
            "        -2.2237e-02, -3.1869e-02, -1.3144e-03,  7.5312e-02,  9.4191e-02,\n",
            "         2.6142e-02,  4.2267e-02, -1.5748e-02,  7.9504e-02, -3.6131e-02,\n",
            "        -3.7577e-02,  1.9152e-02,  9.3688e-02,  1.9828e-02,  8.8612e-02,\n",
            "         1.1281e-02,  3.9031e-02, -2.4450e-02, -2.9722e-02, -5.5357e-02,\n",
            "         6.9083e-02,  8.5925e-03, -1.5077e-02,  2.1463e-02, -3.1369e-02,\n",
            "         3.5788e-01,  5.7628e-02, -1.5830e-02, -2.7861e-02,  3.7876e-02,\n",
            "         1.4335e-01, -5.5432e-02, -6.6102e-03,  3.8161e-02, -3.2889e-02,\n",
            "         4.9558e-04, -2.9461e-02,  5.6774e-02,  9.9110e-02,  3.2555e-02,\n",
            "         8.9766e-02,  3.7298e-02,  8.9731e-04, -3.0968e-02, -1.5418e-02,\n",
            "         5.7755e-03, -1.9634e-02, -1.1913e-02, -9.9341e-02,  6.1938e-03,\n",
            "         1.2174e-02,  6.4162e-02, -3.4889e-02,  5.5220e-02, -1.6219e-02,\n",
            "        -1.8213e-02, -1.8758e-02, -7.8380e-03, -5.6454e-02, -2.0033e-02,\n",
            "         2.0201e-02, -5.4683e-02,  8.3427e-03, -2.3210e-02, -5.3446e-02,\n",
            "         7.4306e-02, -1.2774e-02, -2.8042e-02,  4.6005e-02, -6.4694e-02,\n",
            "        -1.7294e-02, -7.9301e-02,  5.1080e-02, -1.3559e-01,  3.9159e-02,\n",
            "        -2.3599e-02, -8.2554e-02,  5.3449e-02,  1.4668e-02, -8.1506e-02,\n",
            "        -9.2771e-02,  5.6016e-02,  6.3872e-02,  6.3201e-02,  6.9818e-02,\n",
            "        -2.6722e-02,  3.4710e-02,  5.8679e-02, -4.3603e-02, -3.8966e-02,\n",
            "         5.8208e-02,  4.7882e-02,  1.5538e-02,  6.1185e-02,  1.1914e-02,\n",
            "        -1.1792e-02,  1.3080e-02,  1.2756e-02,  4.3904e-03, -4.6826e-02,\n",
            "         5.7792e-03,  4.2581e-02,  7.4119e-02, -1.9908e-02,  6.1138e-02,\n",
            "        -1.5696e-01,  9.2306e-03, -1.5963e-02,  2.7830e-03,  3.0826e-02,\n",
            "        -1.4974e-01,  2.2118e-02,  1.8737e-02,  5.9110e-02,  3.3743e-02,\n",
            "         1.1585e-02,  2.4766e-03,  9.8526e-02, -7.7599e-02,  7.5786e-02,\n",
            "        -6.8259e-04,  1.7229e-02, -1.9149e-02,  1.1706e-02,  4.8096e-02,\n",
            "         4.0260e-03, -8.0829e-02, -3.6413e-03,  6.6767e-02, -4.9990e-03,\n",
            "        -3.8108e-02, -5.3124e-02, -3.3550e-03,  2.1730e-02, -8.1736e-02,\n",
            "        -5.6593e-02, -1.9901e-02,  1.2717e-02,  2.4231e-02, -8.4462e-02,\n",
            "        -1.6094e-02, -6.8574e-02, -5.6506e-02,  2.5413e-02, -1.5372e-02,\n",
            "         5.0478e-02, -2.9032e-02, -2.8129e-02, -2.4322e-02, -2.7895e-02,\n",
            "         8.4280e-02, -2.5548e-02, -1.4745e-02, -3.6642e-02, -9.8730e-03,\n",
            "         2.4058e-02,  2.0563e-02, -3.5957e-03,  3.0437e-02,  4.9026e-02,\n",
            "         1.5517e-02,  2.6026e-02, -1.2295e-02,  3.4923e-02, -2.5392e-02,\n",
            "         2.9590e-02,  6.1086e-02,  2.9150e-02,  8.7412e-02,  5.4112e-02,\n",
            "        -2.7688e-02, -1.0407e-01, -1.0282e-01,  2.5547e-02,  4.2050e-02,\n",
            "        -7.4656e-02, -2.9789e-02, -3.7669e-02,  6.0588e-02,  3.1297e-02,\n",
            "        -3.0744e-02,  1.1578e-02, -1.2192e-01,  9.6826e-02,  1.3014e-02,\n",
            "         8.0922e-03,  3.6262e-02, -6.7964e-02,  8.1158e-02,  4.6057e-02,\n",
            "        -1.9024e-02,  5.8451e-02,  3.6102e-02, -3.2015e-02, -3.0960e-02,\n",
            "         6.5251e-02,  2.4475e-02, -3.4203e-02,  4.5505e-02,  3.8765e-01,\n",
            "        -2.8529e-01,  2.8007e-02,  2.6798e-02,  6.3689e-03,  8.0817e-02,\n",
            "         3.2870e-02,  5.3771e-02,  3.1394e-02,  7.2500e-02,  1.9375e-02,\n",
            "        -1.5466e-02,  5.2337e-02,  1.9306e-02,  1.2301e-02,  6.6790e-02,\n",
            "         2.3724e-02, -6.8996e-02, -5.1847e-02,  5.2198e-02,  4.5447e-02,\n",
            "        -5.2751e-03, -1.4791e-02, -9.6402e-03, -6.6430e-02, -5.1789e-02,\n",
            "         1.8181e-02, -2.3454e-03, -4.8436e-03, -9.0817e-03,  3.5545e-03,\n",
            "         5.3245e-03,  2.9253e-02,  1.6695e-02, -2.0860e-03,  1.1337e-01,\n",
            "        -1.4224e-01, -2.9005e-02,  4.6028e-02, -2.8213e-02, -2.6996e-03,\n",
            "         4.3953e-02, -1.7445e-02, -7.4858e-03,  1.6567e-02,  4.1592e-02,\n",
            "         7.5721e-03, -3.2867e-02, -2.4677e-02,  5.7528e-03,  6.7027e-02,\n",
            "         6.1531e-02,  3.7157e-02, -4.0617e-02,  1.4412e-02,  9.2273e-02,\n",
            "        -1.5370e-03,  1.7922e-02,  1.6840e-03,  9.1532e-02,  2.1521e-02,\n",
            "        -6.8034e-03, -1.2631e-01, -2.5620e-02, -7.9819e-02,  9.4447e-02,\n",
            "        -1.2627e-02, -5.8940e-02, -2.7002e-01, -6.0160e-02, -7.6149e-02,\n",
            "         3.9379e-02,  5.9777e-02, -5.3643e-02,  1.0764e-02,  5.7855e-02,\n",
            "         3.4035e-02,  4.4661e-02, -5.8045e-02,  3.0042e-03, -4.7472e-02,\n",
            "         1.5678e-03,  3.1604e-02,  4.0454e-02, -2.0392e-02, -1.0234e-02,\n",
            "        -3.2530e-02,  3.1062e-02,  1.3908e-02,  3.5735e-02,  3.0707e-02,\n",
            "        -3.4697e-02,  4.6806e-02,  8.7995e-03,  6.3946e-02, -7.5322e-02,\n",
            "        -1.2987e-02,  2.4917e-03, -1.1184e-01, -5.8173e-03,  1.2832e-02,\n",
            "         8.3949e-03,  1.0388e-01, -8.7576e-02,  5.5723e-03,  7.2192e-03,\n",
            "         2.4759e-02,  1.2799e-02, -7.5116e-03, -1.0270e-01, -1.5527e-02,\n",
            "        -6.4052e-02, -3.1220e-02,  5.4505e-02, -6.3366e-02, -9.5832e-02,\n",
            "        -1.1000e-02,  9.4019e-02,  1.3754e-02, -7.1562e-02,  5.5273e-03,\n",
            "        -3.1780e-02,  5.0031e-02, -2.2355e-02, -6.7475e-01,  6.9557e-02,\n",
            "         5.8188e-02, -4.1054e-02, -1.0171e-02, -5.0548e-02, -2.1783e-02,\n",
            "         7.3617e-02,  6.1316e-02,  2.1774e-02, -4.5567e-02,  2.2805e-03,\n",
            "         4.0153e-02, -4.9539e-02,  5.7593e-02, -8.9209e-03, -2.4321e-02,\n",
            "         8.4002e-02,  1.2323e-02, -3.2996e-02, -5.3422e-02,  6.1713e-03,\n",
            "        -2.0251e-03, -6.0440e-02, -9.4574e-03, -3.9084e-02, -3.7668e-02,\n",
            "        -3.5784e-02,  6.7191e-02,  3.6997e-02, -2.7328e-02, -4.5830e-02,\n",
            "        -4.0266e-03,  1.9312e-02, -2.1589e-02,  9.2412e-02,  4.4405e-02,\n",
            "        -8.0393e-03, -8.5168e-03, -7.1086e-02,  5.4087e-02,  2.2213e-01,\n",
            "        -2.8405e-03,  1.5990e-01,  8.6247e-03, -6.2194e-02, -2.9768e-02,\n",
            "         5.9843e-03, -2.8260e-02, -3.3483e-03, -6.4838e-02, -4.6637e-02,\n",
            "        -3.4865e-02, -7.8291e-02, -2.9179e-02, -7.0269e-03,  5.2529e-02,\n",
            "        -8.1852e-02,  8.4867e-02,  1.0240e-01, -8.7548e-04,  4.1783e-02,\n",
            "         7.3637e-03,  6.3527e-03, -1.7973e-02,  1.0915e-02,  3.2441e-02,\n",
            "        -3.6488e-02,  4.2774e-03, -1.9610e-02,  3.0902e-03, -7.7361e-02,\n",
            "        -2.4048e-02,  1.6607e-03,  3.5846e-02,  5.5341e-02,  1.7953e-02,\n",
            "         4.6866e-02, -7.9507e-02,  6.4539e-02, -4.9704e-03,  8.0451e-02,\n",
            "         1.7811e-02,  4.2519e-02,  1.0921e-02, -9.1278e-03, -3.2679e-02,\n",
            "         2.5103e-02,  5.9905e-02,  1.0180e-01, -4.2575e-03,  9.4389e-02,\n",
            "         4.9124e-02,  1.2660e-03,  1.1851e-03,  6.6533e-02,  1.9181e-02,\n",
            "         3.4485e-03, -5.1769e-01, -6.5665e-02,  9.7710e-02,  3.1140e-02,\n",
            "        -8.9468e-03,  4.2890e-02, -4.4266e-02,  1.0228e-02,  7.6612e-02,\n",
            "        -4.3981e-02,  4.3873e-02,  8.0825e-03,  1.0541e-01, -1.1044e-01,\n",
            "        -5.6318e-02,  8.8146e-02, -2.7185e-02, -7.5423e-02,  3.3043e-02,\n",
            "        -2.9974e-01,  9.4118e-03, -4.9416e-02,  1.0276e-01,  3.7868e-02,\n",
            "         5.6931e-03,  3.7073e-02, -6.7966e-03,  3.7274e-02,  9.3212e-02,\n",
            "         1.0642e-01,  1.2176e-01,  3.6945e-02, -4.1434e-02, -4.2832e-04,\n",
            "        -5.9879e-02,  5.7845e-02,  5.4819e-02,  1.0878e+01, -1.7253e-02,\n",
            "         2.1957e-02, -2.4097e-02,  1.7051e-02, -7.4836e-02,  6.4270e-02,\n",
            "        -5.3710e-02,  2.5446e-02,  4.4139e-02, -2.8801e-03, -1.4938e-02,\n",
            "        -8.3028e-02, -4.9978e-02,  5.0698e-02,  1.3110e-02, -7.9395e-03,\n",
            "        -7.8745e-02,  4.4851e-02, -2.1307e-02,  3.2836e-02,  8.2177e-02,\n",
            "         1.7795e-02, -2.8188e-02, -3.4754e-02,  5.7067e-02,  5.8323e-02,\n",
            "         2.4212e-02, -1.5017e-02,  7.4638e-02,  5.3725e-02,  8.4673e-02,\n",
            "         5.3806e-02,  1.7871e-02,  5.8110e-02, -4.0252e-03, -1.3587e-01,\n",
            "         8.5789e-02,  6.9770e-02,  9.4244e-02,  3.4441e-02,  3.7708e-02,\n",
            "         2.3582e-02,  1.2110e-02,  2.2975e-02,  6.6539e-02, -8.2072e-02,\n",
            "         5.2900e-02, -8.3411e-04,  3.3096e-02,  4.6514e-02, -1.0147e-01,\n",
            "         4.7645e-02,  1.6469e-02,  1.3492e-02, -3.7328e-02,  7.5788e-02,\n",
            "        -2.5437e-02,  3.3202e-02,  2.0434e-02, -2.5335e-02,  7.8580e-02,\n",
            "        -2.2129e-02,  4.6057e-03, -1.1318e-01,  6.9247e-02,  1.1160e-01,\n",
            "         3.4737e-02, -3.1239e-03,  3.6244e-02,  3.1596e-02, -4.1006e-02,\n",
            "        -6.5433e-02, -4.6649e-02, -1.9822e-02, -1.8647e-02,  4.1133e-02,\n",
            "        -5.5677e-02,  3.3787e-02,  2.9441e-04,  5.3747e-02,  2.8171e-02,\n",
            "         2.2229e-03, -1.1367e-02,  1.0680e-02,  7.0569e-02,  1.7948e-02,\n",
            "        -8.1710e-03,  3.1374e-02,  3.0959e-02, -3.8686e-02,  6.1800e-03,\n",
            "        -4.7963e-02, -7.8745e-02,  1.3582e-02, -2.0322e-02,  1.9978e-02,\n",
            "        -8.1599e-02,  8.5668e-02,  7.2206e-02, -8.3497e-02, -4.5517e-05,\n",
            "         1.1214e-02,  7.0517e-02, -6.2947e-02,  5.0614e-02,  5.8092e-03,\n",
            "         7.6392e-03,  1.1820e-02, -1.3068e-02,  7.1413e-03,  1.0342e-02,\n",
            "        -1.7049e-02, -2.2476e-02, -1.0490e-02,  2.8629e-02,  1.4431e-02,\n",
            "         1.9399e-02,  1.4150e-02,  9.3488e-02, -7.1433e-02,  1.4647e-02,\n",
            "        -7.1402e-02, -4.4263e-02, -3.4333e-02, -3.6016e-02,  1.2534e-02,\n",
            "         2.8717e-02,  8.2092e-03, -9.9918e-03, -3.2753e-02,  3.5688e-02,\n",
            "         1.2569e-01,  4.4503e-02,  1.7947e-02,  9.9387e-03,  2.3260e-02,\n",
            "         1.0723e-01,  4.5045e-02,  5.7420e-02,  4.1497e-03,  3.0881e-02,\n",
            "         4.0582e-03, -2.7754e-02,  1.5143e-02, -6.0359e-02,  8.6427e-02,\n",
            "        -2.6001e-02,  3.0275e-02, -6.7229e-03,  3.3538e-02,  9.7908e-03,\n",
            "        -2.0384e-02,  8.2415e-04,  9.7329e-03, -3.8048e-02, -3.5926e-02,\n",
            "         9.7855e-02,  6.7750e-03, -2.7798e-02,  1.0018e-01,  2.2434e-02,\n",
            "         9.3926e-03,  2.7452e-03, -5.5982e-02, -1.1085e-02, -8.3688e-02,\n",
            "         4.7721e-02,  6.8386e-02, -3.1060e-02,  3.3639e-02,  1.9891e-02,\n",
            "        -1.3196e-01, -4.9223e-02, -1.1454e-03,  1.0079e-01,  7.2946e-02,\n",
            "        -1.6240e-01, -6.6192e-02,  1.1734e-02, -1.7394e+00, -1.2417e+00,\n",
            "        -1.3217e+00, -1.5109e+00, -7.7963e-01, -1.5431e+00,  1.4771e+00,\n",
            "        -1.3966e+00,  2.9576e-01, -5.5510e-02, -6.0588e-01,  1.2064e+00,\n",
            "        -8.4447e-01,  1.1756e+00, -6.2642e-01, -7.3714e-01, -5.3736e-01,\n",
            "        -6.2428e-01, -9.0248e-01,  8.2726e-01, -4.2465e-01, -1.1799e-01,\n",
            "         8.7121e-02, -7.3930e-02,  9.2788e-01, -4.4258e-01,  1.0139e+00,\n",
            "        -4.4185e-01,  2.9396e-01,  3.7085e-01, -3.6707e-02,  6.0057e-01,\n",
            "         5.5380e-01, -7.1950e-01,  1.1247e-01, -4.3315e-01,  4.0871e-01,\n",
            "        -6.7639e-01, -9.4700e-01, -3.7880e-01,  1.4009e-01,  6.3724e-01,\n",
            "        -6.1086e-01,  6.7011e-01,  4.2396e-01, -1.0866e-01,  5.2466e-02,\n",
            "        -3.2861e-01, -1.8834e-01,  1.8687e-01,  7.1808e-01, -5.7074e-01,\n",
            "        -4.5368e-01,  1.3658e-01, -3.9713e-01, -2.0259e-01, -3.2779e-01,\n",
            "        -8.4972e-01,  3.4593e-01,  3.8849e-01, -1.4748e-01,  9.8161e-02,\n",
            "        -3.9313e-01,  1.6717e-01, -9.8111e-02, -1.2764e-01, -2.0426e-01,\n",
            "        -3.7045e-01,  4.4760e-01,  4.1807e-01,  1.6646e-01,  3.6681e-04,\n",
            "        -2.0239e-01,  1.3383e-01,  2.1372e-01,  7.1051e-02, -1.4776e-01,\n",
            "         3.2518e-01,  3.7257e-02, -1.2485e-01,  5.5896e-02,  2.0450e-01,\n",
            "        -3.2705e-01,  3.8719e-04, -1.7110e-01,  1.8028e-01,  4.4292e-02,\n",
            "         1.9559e-04,  3.6056e-03, -2.8537e-02,  1.9995e-02,  1.4549e-02,\n",
            "        -2.0742e-01, -1.3411e-01])\n",
            "tensor([-3.6322e-02,  1.2869e-02, -5.5312e-03, -9.6341e-02,  4.2336e-02,\n",
            "        -1.4539e-01, -9.0484e-03,  5.7103e-02,  2.4687e-02, -8.1741e-02,\n",
            "        -3.0225e-02,  2.5579e-02,  5.8548e-02, -4.9867e-02,  5.0203e-02,\n",
            "         7.5650e-02, -4.1141e-02, -1.2829e-02,  7.5020e-02, -1.3189e-02,\n",
            "        -8.4840e-02,  5.1697e-02, -5.4878e-02,  5.3523e-02,  4.0153e-04,\n",
            "         1.5814e-02,  5.1111e-02,  6.3844e-02, -1.9836e-02,  6.0353e-02,\n",
            "        -1.0099e-02, -3.1788e-02,  3.3388e-02, -1.7897e-02,  6.1129e-02,\n",
            "         5.7113e-02,  3.7684e-02, -1.0427e-02, -9.7295e-02,  2.2019e-02,\n",
            "        -6.1538e-02,  6.2562e-04,  2.8678e-02, -1.0793e-02,  3.3706e-02,\n",
            "        -1.6157e-02,  6.6215e-02,  2.3399e-02, -5.1450e-02,  2.0376e-02,\n",
            "         3.1540e-02,  5.8350e-02, -1.4653e-02,  8.1390e-03, -5.0225e-02,\n",
            "        -2.1951e-02,  2.0207e-02,  1.1959e-01,  8.1463e-02, -1.8548e-02,\n",
            "         3.3364e-02, -9.5495e-02, -5.9264e-02,  1.6898e-03,  7.6944e-02,\n",
            "        -1.9413e-02, -4.9659e-02, -3.8033e-02,  2.8652e-02,  1.4563e-02,\n",
            "         1.2019e-01, -5.4261e-03,  8.5814e-02, -3.6949e-02, -2.9997e-02,\n",
            "        -2.0054e-05,  6.9948e-02,  3.8955e-01, -1.0227e-01, -1.3083e-02,\n",
            "         8.6164e-02, -1.8964e-02,  1.7921e-01,  8.2371e-03,  6.3320e-02,\n",
            "        -3.0482e-02,  6.3990e-02,  1.2819e-02,  4.2326e-02,  4.5568e-02,\n",
            "         4.0579e-02,  9.5651e-02, -4.7653e-02,  1.9072e-02,  3.1303e-02,\n",
            "         3.6434e-02, -1.2934e-02, -1.3756e-01, -2.4334e-02, -5.4745e-02,\n",
            "         3.2571e-02, -7.2704e-02,  3.1211e-02,  2.4106e-02,  2.6673e-02,\n",
            "         1.2149e-02,  2.7144e-02, -9.9355e-03,  7.6006e-02, -3.7298e-02,\n",
            "         4.4281e-03,  9.1099e-02, -1.2565e-02,  6.8632e-02, -2.9034e-03,\n",
            "        -5.9119e-02,  4.8411e-02, -5.1160e-03, -6.4826e-03,  2.1447e-02,\n",
            "         2.8027e-02,  6.8923e-02,  1.1383e-01, -7.3664e-02,  5.7064e-04,\n",
            "        -4.8633e-02, -2.0790e-02, -2.1333e-02, -6.3110e-02,  4.5132e-02,\n",
            "        -2.5033e-02, -4.3639e-02,  5.6835e-03,  2.5500e-02,  8.3732e-02,\n",
            "         6.3173e-02,  5.6496e-02,  5.6305e-02,  5.3646e-02, -2.6009e-03,\n",
            "        -6.1021e-02,  3.6931e-02,  9.2316e-02,  5.9727e-02,  8.7288e-02,\n",
            "         6.4462e-03,  4.7067e-02, -4.1283e-02,  1.5829e-02, -2.1120e-02,\n",
            "         4.8364e-02, -3.7329e-02, -3.2394e-03,  1.1407e-02, -4.1325e-03,\n",
            "         3.4622e-01, -8.6450e-03, -8.5468e-03, -7.2167e-02,  5.3349e-02,\n",
            "         1.3081e-01, -4.3549e-02,  3.4567e-02,  2.6777e-02, -8.8466e-03,\n",
            "         1.9290e-02, -9.4078e-03,  3.4583e-02,  9.4020e-02,  1.2232e-02,\n",
            "         6.0486e-02,  2.9533e-02,  3.8107e-02, -1.8406e-02, -3.9480e-02,\n",
            "        -1.5725e-02, -3.8294e-02,  2.8687e-02, -1.1825e-01, -3.2401e-03,\n",
            "         4.5625e-02,  3.0560e-02, -3.1201e-02,  2.5719e-02, -1.7676e-02,\n",
            "         1.8250e-02,  2.0831e-02,  1.8779e-02, -9.1807e-03, -6.3298e-02,\n",
            "         4.7421e-02, -4.7029e-02,  5.9571e-03, -1.8474e-02, -2.2924e-02,\n",
            "         5.1043e-02, -2.4320e-02, -3.6888e-02,  6.1044e-02, -5.8341e-02,\n",
            "        -2.2246e-02, -1.2967e-01,  8.2855e-02, -1.1675e-01,  3.6233e-02,\n",
            "        -2.4181e-02, -1.2525e-02,  7.4649e-02,  2.8840e-02, -1.6407e-02,\n",
            "        -7.3264e-02,  5.7127e-02,  6.8482e-03,  5.5084e-02,  4.1689e-02,\n",
            "        -2.0332e-02,  3.9136e-02,  1.2333e-01,  9.5746e-03, -2.9634e-02,\n",
            "         4.5093e-02,  5.2893e-02, -2.1535e-02,  7.2714e-02, -3.3714e-04,\n",
            "        -4.0052e-03, -3.6782e-02, -1.3995e-02, -4.6696e-04,  6.4219e-03,\n",
            "        -4.5222e-02,  6.8420e-02,  5.5741e-02, -3.3642e-02,  1.5213e-02,\n",
            "        -1.1793e-01,  1.9744e-03, -2.1123e-02, -1.2266e-02,  4.6773e-02,\n",
            "        -1.9257e-01,  2.0304e-02,  3.5179e-02,  4.9446e-02,  2.5478e-02,\n",
            "         6.8032e-02,  2.8815e-02,  9.9396e-02, -1.1813e-02,  4.9548e-02,\n",
            "        -2.2471e-02,  5.5978e-02, -6.0108e-02, -1.8313e-02,  1.0433e-02,\n",
            "        -1.2744e-02, -7.6866e-02,  1.2958e-02,  7.2957e-02,  2.3578e-02,\n",
            "        -2.0839e-02, -4.2590e-02, -2.5356e-02, -2.6740e-02, -8.9889e-02,\n",
            "        -4.7192e-02, -1.9083e-02,  1.8472e-02, -2.2759e-02, -7.4915e-02,\n",
            "         2.1731e-02, -4.7447e-02, -5.7152e-02,  1.9052e-02,  1.9421e-02,\n",
            "         5.1147e-02, -4.8389e-02, -3.3174e-02, -5.5901e-02, -4.1200e-02,\n",
            "         5.9445e-02,  1.4038e-02, -8.4396e-02,  2.1945e-02, -9.1490e-03,\n",
            "         1.3948e-02, -2.7819e-02, -3.5978e-03,  9.0871e-03,  4.2710e-02,\n",
            "         2.4648e-02,  5.0614e-02, -3.3942e-02,  2.8621e-02, -3.2189e-02,\n",
            "         3.3072e-02,  7.3912e-02, -1.3716e-03,  5.7244e-02,  2.3908e-02,\n",
            "        -2.2020e-02, -1.6527e-01, -8.5599e-02,  2.9256e-02,  6.7279e-02,\n",
            "        -8.0949e-02, -4.9962e-03,  2.4755e-03,  6.9027e-02,  1.8428e-02,\n",
            "        -3.5428e-02,  8.6001e-03, -1.3278e-01,  9.4407e-02,  1.5975e-02,\n",
            "         2.6569e-02,  3.9255e-02, -8.7216e-02,  4.6797e-02,  4.8426e-02,\n",
            "         2.3845e-02,  3.8762e-02,  4.9410e-02,  6.0106e-03, -4.6811e-02,\n",
            "         2.6385e-02,  5.5347e-03, -2.5811e-02,  5.3812e-02,  3.8275e-01,\n",
            "        -3.2558e-01,  3.8385e-02,  5.7874e-02, -7.4104e-03,  7.3526e-02,\n",
            "         2.6531e-02,  4.5465e-02,  2.0513e-02,  5.7604e-02,  3.7850e-02,\n",
            "        -9.2189e-03,  7.2112e-02, -4.3952e-03,  2.2269e-02,  5.8892e-02,\n",
            "         1.9217e-02, -6.9242e-02, -3.0528e-02, -1.2232e-03,  3.6192e-02,\n",
            "        -4.3637e-02, -1.4587e-02, -3.5987e-02, -4.7823e-02, -5.4289e-02,\n",
            "         7.5752e-03,  1.6092e-02,  1.4183e-02, -4.1704e-02, -6.4596e-03,\n",
            "         1.8658e-02,  3.8733e-02,  4.2076e-02,  3.1629e-02,  1.2592e-01,\n",
            "        -1.3609e-01, -3.8296e-02,  4.9696e-02,  2.5063e-02, -2.1946e-03,\n",
            "         4.7523e-02, -1.2830e-02, -1.1781e-02,  3.5702e-02,  7.7113e-02,\n",
            "         4.5458e-03, -4.2035e-02, -3.1120e-02,  6.7870e-02,  9.4403e-02,\n",
            "         4.4231e-02,  3.0055e-02, -5.8581e-02,  1.9276e-02,  1.1463e-01,\n",
            "        -2.2897e-03,  5.1869e-02,  2.7334e-02,  6.7668e-02,  5.0704e-03,\n",
            "        -3.1757e-02, -9.1450e-02, -2.7517e-02, -7.9393e-02,  9.6116e-02,\n",
            "         2.3899e-02, -8.5019e-03, -1.1955e-01, -4.2359e-02, -9.5152e-02,\n",
            "         6.5177e-02,  5.7711e-02, -4.5227e-02, -5.8121e-03,  1.3712e-02,\n",
            "        -1.0227e-02,  5.5551e-03, -7.4640e-02,  2.4722e-02, -3.2990e-02,\n",
            "         9.7654e-03,  9.0940e-02,  2.6634e-02,  1.2113e-02, -1.4218e-02,\n",
            "        -4.8199e-04,  2.4374e-02, -7.4725e-02,  1.8827e-02, -3.0178e-02,\n",
            "        -2.9852e-02,  1.8402e-02,  9.7525e-03,  2.6837e-02, -9.1716e-02,\n",
            "        -1.6121e-02,  2.4981e-02, -1.1908e-01,  3.4597e-02,  1.4785e-02,\n",
            "         2.3338e-02,  9.1643e-02, -5.7276e-02, -5.6817e-02,  4.1983e-03,\n",
            "        -1.3676e-02, -5.4127e-03,  8.8277e-04, -5.8726e-02,  1.8088e-02,\n",
            "        -5.4782e-02, -4.2959e-03,  5.7704e-02,  4.5941e-03, -1.0177e-01,\n",
            "         8.0435e-03,  1.0566e-01,  4.1112e-02, -9.9277e-02, -8.4366e-03,\n",
            "        -2.2504e-02,  3.2158e-02, -9.5616e-03, -7.0256e-01,  4.9932e-02,\n",
            "         4.8950e-02, -4.6480e-02,  2.9862e-02, -4.0977e-02, -9.5046e-03,\n",
            "         4.2369e-02,  5.5929e-02,  2.8835e-02, -5.8040e-02, -3.8722e-04,\n",
            "         4.2295e-02, -5.7366e-02,  3.8632e-02,  1.8671e-02, -3.2099e-02,\n",
            "         4.2332e-02, -1.7836e-03, -3.8902e-03, -3.7639e-02,  2.1009e-02,\n",
            "        -3.4994e-02, -1.5278e-02,  8.1929e-03, -6.9555e-02, -3.2298e-02,\n",
            "        -7.2499e-03,  8.3506e-02,  2.7084e-02, -5.5700e-02, -5.5680e-02,\n",
            "        -3.3762e-02, -2.8557e-02,  2.5373e-02,  4.4885e-02,  2.0335e-02,\n",
            "        -2.7362e-02,  2.2008e-02, -2.4878e-02,  2.6757e-02,  2.2348e-01,\n",
            "        -1.9747e-02,  1.3777e-01,  1.5308e-02, -1.4281e-02, -3.9142e-02,\n",
            "        -9.3916e-03, -2.0645e-02,  1.9456e-02,  7.2209e-03, -4.5962e-02,\n",
            "        -4.7489e-02, -3.3171e-02, -4.7620e-02, -2.7105e-02,  5.9946e-02,\n",
            "        -2.9455e-02,  5.4699e-02,  1.5935e-01,  3.0510e-03,  1.6858e-02,\n",
            "         5.2457e-02,  6.6040e-02, -2.0652e-02, -2.7342e-02,  9.9265e-03,\n",
            "        -4.7742e-02, -2.7498e-02, -1.5442e-02, -3.7787e-02, -6.2848e-02,\n",
            "        -5.1580e-02,  3.3165e-02,  4.0055e-02,  1.9463e-02,  3.5676e-02,\n",
            "         4.6308e-02,  2.0661e-02,  4.2663e-02,  5.4483e-03,  9.2025e-02,\n",
            "         3.0415e-02,  6.4098e-02,  1.0370e-02, -6.1593e-02, -2.5957e-02,\n",
            "         5.7281e-03, -3.3209e-03,  1.2973e-01, -6.7200e-03,  8.6707e-02,\n",
            "         6.2969e-02,  6.2961e-03, -1.6785e-02,  4.7459e-02,  6.7465e-02,\n",
            "        -2.9788e-02, -6.1643e-01, -8.7960e-02,  7.9061e-02,  2.6763e-02,\n",
            "         1.2506e-02,  5.4869e-02, -1.7504e-02, -2.3276e-02,  1.4701e-01,\n",
            "        -3.8309e-02,  3.5431e-02,  9.4987e-03,  1.1691e-01, -1.2312e-01,\n",
            "        -3.2399e-02,  7.8846e-02, -5.6094e-04, -6.8023e-02,  1.5209e-02,\n",
            "        -3.5934e-01,  2.0500e-02, -4.8624e-02,  6.5644e-02,  1.6256e-02,\n",
            "         4.2979e-02,  2.2939e-02, -2.5439e-02,  7.0173e-02,  1.0834e-01,\n",
            "         1.2824e-01,  9.0225e-02,  5.9989e-02, -7.0119e-02, -1.0260e-02,\n",
            "        -5.4651e-02,  1.4140e-01,  6.7652e-02,  1.0771e+01, -6.0884e-02,\n",
            "         2.2216e-02,  8.6350e-03, -3.8366e-02, -7.9767e-02,  7.7003e-02,\n",
            "        -6.6984e-02,  3.9964e-02,  9.8337e-02,  1.2899e-02, -1.5841e-02,\n",
            "        -1.0529e-01, -1.4766e-02,  3.5257e-02,  4.8249e-03, -3.3896e-02,\n",
            "        -7.2779e-02,  4.8911e-02,  3.1773e-04,  1.0771e-02,  1.4293e-02,\n",
            "         6.4734e-03, -5.2589e-02, -2.7245e-02,  5.5450e-02,  2.7375e-02,\n",
            "        -1.6613e-02, -2.9289e-02,  5.7553e-02,  2.9543e-02,  2.0733e-02,\n",
            "         4.1005e-02,  3.3263e-02,  2.6662e-02, -4.4479e-02, -8.8926e-02,\n",
            "         7.3524e-02,  5.5843e-02,  5.1860e-02,  7.9489e-02,  1.3874e-02,\n",
            "         1.3935e-02,  2.7866e-02,  3.1512e-02,  6.1451e-02, -5.0313e-02,\n",
            "         6.2019e-02,  1.3979e-02,  2.4732e-02,  3.1946e-02, -6.5249e-02,\n",
            "         5.5547e-02, -1.1367e-02,  1.0244e-02, -2.4453e-02,  8.0586e-02,\n",
            "        -3.3154e-02,  4.7224e-02,  3.3173e-02,  2.5314e-03,  5.8377e-02,\n",
            "        -1.3017e-02,  6.1784e-02, -1.1869e-01,  8.3740e-02,  6.2502e-02,\n",
            "         8.0431e-02,  9.0860e-03,  8.2020e-02,  3.4728e-02, -4.0853e-02,\n",
            "        -2.8695e-02, -6.9271e-02, -1.5890e-02,  1.9573e-03,  9.1296e-03,\n",
            "        -5.7387e-02,  3.6904e-02, -1.5574e-02,  5.0020e-02,  3.1650e-02,\n",
            "        -4.3890e-02,  1.1176e-02, -4.0637e-02,  5.4011e-02,  2.2679e-02,\n",
            "        -1.8415e-02,  3.1756e-02,  4.9665e-02, -5.2075e-02,  6.5514e-03,\n",
            "        -4.7654e-03, -6.5296e-02,  2.6535e-02, -1.2431e-02, -5.4362e-02,\n",
            "        -1.1205e-01,  6.2456e-02,  3.6198e-02, -6.2439e-02,  3.6384e-02,\n",
            "        -2.1701e-02,  4.9398e-02, -3.7431e-02,  2.1957e-02,  1.9059e-02,\n",
            "        -1.1235e-02,  3.1760e-02, -2.0449e-02,  1.7884e-02,  3.6163e-02,\n",
            "         3.0138e-02,  1.3085e-03, -1.7408e-02,  1.3580e-02,  3.6237e-02,\n",
            "         2.5265e-02, -6.2052e-04,  9.2951e-02, -7.5020e-02,  2.8750e-02,\n",
            "        -4.3356e-02, -2.9244e-02, -2.4325e-02, -4.8438e-02, -3.9904e-02,\n",
            "        -2.1230e-02,  2.5526e-02,  1.5238e-02, -3.6857e-02,  4.1159e-02,\n",
            "         7.1275e-02,  3.7291e-02,  1.1167e-02, -8.1453e-03,  4.6607e-03,\n",
            "         9.6828e-02,  1.0373e-02,  6.3819e-02, -3.5641e-02,  2.4343e-03,\n",
            "         7.5513e-03, -1.0044e-01,  3.3797e-02,  1.6339e-02,  1.0122e-01,\n",
            "         5.9193e-03,  1.3590e-02,  4.4146e-02,  7.9529e-02,  3.4504e-02,\n",
            "         2.5246e-02,  1.5167e-02,  7.2386e-03,  7.6383e-03, -2.2288e-02,\n",
            "         6.4652e-02,  2.4022e-02, -2.0539e-02,  9.2867e-02, -6.0486e-02,\n",
            "         5.9137e-03,  4.9252e-02, -9.4271e-02,  1.9136e-03, -7.2209e-02,\n",
            "         5.1211e-02,  1.7238e-03, -2.8163e-02,  4.2723e-02,  2.8751e-02,\n",
            "        -1.2767e-01, -7.9602e-02, -2.5974e-02,  6.0759e-02,  6.1441e-02,\n",
            "        -1.3963e-01, -4.5193e-02,  2.1861e-04, -1.1315e+00,  2.2370e-01,\n",
            "         3.6841e-01, -1.8416e+00, -3.6665e-01, -3.5075e+00,  5.8646e-01,\n",
            "        -1.0198e+00, -7.9014e-03, -2.9026e-01,  1.2908e-01,  1.1236e+00,\n",
            "        -2.5018e-01, -6.2732e-02, -4.5930e-01,  8.1175e-02, -7.3332e-01,\n",
            "         6.5280e-02, -1.0459e+00,  5.8055e-01, -2.9075e-01, -6.6924e-01,\n",
            "        -9.7246e-01, -9.0157e-01, -2.6440e-02, -1.9146e+00,  6.1611e-01,\n",
            "        -8.4131e-01, -2.3132e-01,  3.1075e-01, -7.7542e-01, -6.7073e-01,\n",
            "        -3.4340e-01,  2.9549e-01, -7.5244e-01, -7.9901e-01,  5.6373e-01,\n",
            "         1.8701e-01, -1.3371e-01,  7.7219e-01, -6.4455e-01, -1.3683e-01,\n",
            "        -2.9212e-01, -6.2009e-02,  3.6573e-01,  3.4455e-01, -7.9620e-01,\n",
            "         4.8772e-01,  5.8165e-01, -3.8373e-01, -5.9276e-02,  4.9546e-01,\n",
            "         4.7261e-01,  1.1697e-01,  2.5211e-01,  2.4636e-01,  3.1195e-01,\n",
            "         1.2676e-01,  6.6951e-02,  1.7885e-01,  2.3511e-01,  1.0321e-01,\n",
            "         1.9683e-01, -5.2836e-01, -4.1421e-01,  2.6498e-02,  4.3029e-01,\n",
            "        -1.8760e-01, -3.2229e-01, -1.0610e-01, -1.5717e-01,  3.1812e-02,\n",
            "         2.6450e-02, -8.7288e-02, -1.1942e-01, -1.2040e-01,  1.0141e-01,\n",
            "        -2.5261e-01,  5.8486e-02,  5.5056e-02, -1.4116e-01, -6.6773e-02,\n",
            "         1.1225e-01,  9.6879e-03,  1.8960e-01, -1.1596e-01, -1.3301e-01,\n",
            "        -6.3879e-02,  1.4310e-01,  2.0425e-02, -1.2429e-01, -2.6170e-02,\n",
            "         2.2843e-01, -8.8222e-02])\n",
            "torch.Size([21, 862])\n",
            "tensor([ -6.5737, -20.8618, -20.5174, -21.0002, -20.6820, -19.8695, -20.2932,\n",
            "         -5.2061, -20.1578, -17.0817, -20.9314, -20.1848,  -4.6071, -20.9741,\n",
            "        -10.6089, -20.4322, -20.6442, -19.9205], grad_fn=<SelectBackward0>)\n",
            "tensor([ -6.5737, -20.8618, -20.5174, -21.0002, -20.6820, -19.8695, -20.2932,\n",
            "         -5.2061, -20.1578, -17.0817, -20.9314, -20.1848,  -4.6071, -20.9741,\n",
            "        -10.6089, -20.4322, -20.6442, -19.9205], grad_fn=<SelectBackward0>)\n",
            "torch.Size([21, 862])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "In epoch 510, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 515, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 520, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 525, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 530, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 535, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 540, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 545, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 550, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 555, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 560, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 565, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 570, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 575, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 580, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 585, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 590, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n",
            "In epoch 595, loss: 0.649, train acc: 0.647 (train 0.647), test acc: 0.667 (best 0.667)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import torch\n",
        "from dgl.data import DGLDataset\n",
        "\n",
        "\n",
        "class KarateClubDataset(DGLDataset):\n",
        "    def __init__(self,params):\n",
        "        self.params=params\n",
        "        #self.threshold=threshold\n",
        "        super().__init__(name=\"karate_club\")\n",
        "        self.process()\n",
        "    def process(self):\n",
        "        edge_remove=[]\n",
        "        C=edge_weights\n",
        "        for i in range(0,len(C)):\n",
        "            if C[i]<=self.params['percentile']:\n",
        "                edge_remove.append(i)\n",
        "        nodes_data = Nodes_Data\n",
        "        edges_data = Edge_Data\n",
        "        features_array = np.array(nodes_data[\"features\"].tolist(), dtype=float)\n",
        "        node_features = torch.from_numpy(features_array)\n",
        "        node_labels = torch.from_numpy(\n",
        "                      nodes_data[\"label\"].astype(\"category\").cat.codes.to_numpy()\n",
        "                       ).clone().detach()\n",
        "        edge_features = torch.from_numpy(edges_data[\"edge weights\"].to_numpy())\n",
        "        edges_src = torch.from_numpy(edges_data[\"Src Ids\"].to_numpy())\n",
        "        edges_dst = torch.from_numpy(edges_data[\"Dst Ids\"].to_numpy())\n",
        "        self.graph = dgl.graph(\n",
        "            (edges_src, edges_dst), num_nodes=nodes_data.shape[0]\n",
        "        )\n",
        "        self.graph.ndata[\"feat\"] = node_features\n",
        "        self.graph.ndata[\"label\"] = node_labels\n",
        "        self.graph.edata[\"weight\"] = edge_features\n",
        "\n",
        "        self.graph=dgl.remove_edges(self.graph, torch.tensor(edge_remove))\n",
        "        n_nodes = nodes_data.shape[0]\n",
        "        n_train = int(n_nodes * 0.6)\n",
        "        n_val = int(n_nodes * 0.2)\n",
        "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        train_mask[:n_train] = True\n",
        "        val_mask[n_train : n_train + n_val] = True\n",
        "        test_mask[n_train + n_val :] = True\n",
        "        self.graph.ndata[\"train_mask\"] = train_mask\n",
        "        self.graph.ndata[\"val_mask\"] = val_mask\n",
        "        self.graph.ndata[\"test_mask\"] = test_mask\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.graph\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1"
      ],
      "metadata": {
        "id": "vOnTCKaZwBfm"
      },
      "id": "vOnTCKaZwBfm",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self,in_feats,params,num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GraphConv(in_feats, params['output_features'])\n",
        "        self.conv2 = GraphConv(params['output_features'], num_classes)\n",
        "\n",
        "    def forward(self, g,in_feat,params):\n",
        "        in_feat = in_feat.float()\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = getattr(F, params['activation'])(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h"
      ],
      "metadata": {
        "id": "s98khP4RwRm4"
      },
      "id": "s98khP4RwRm4",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "los=[]\n",
        "train_accuracy=[]\n",
        "test_accuracy=[]\n",
        "TT=[]\n",
        "CT=[]\n",
        "epoch=[]\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "def train(g,sg_train,sg_test, model,params):\n",
        "    model.train()\n",
        "    optimizer = getattr(optim, params['optimizer'])(model.parameters(), lr= params['learning_rate'])\n",
        "    best_train_acc = 0\n",
        "    best_test_acc = 0\n",
        "    corresponding_test=0\n",
        "\n",
        "    features_train = sg_train.ndata[\"feat\"]\n",
        "    labels = g.ndata[\"label\"]\n",
        "    train_mask = g.ndata[\"train_mask\"]\n",
        "    test_mask = g.ndata[\"test_mask\"]\n",
        "\n",
        "\n",
        "    for e in range(100):\n",
        "        # Forward\n",
        "        logits = model(sg_train,features_train,params)\n",
        "\n",
        "\n",
        "        # Compute prediction\n",
        "        pred = logits.argmax(1)\n",
        "\n",
        "\n",
        "        labels = g.ndata[\"label\"].long()\n",
        "        if params['loss'] == 'binary_cross_entropy':\n",
        "            loss = getattr(F, params['loss'])(torch.sigmoid(logits), labels[train_mask])\n",
        "        else:\n",
        "            loss = getattr(F, params['loss'])(logits, labels[train_mask])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            logits_test = model(sg_test,sg_test.ndata[\"feat\"],params)\n",
        "\n",
        "\n",
        "        pred_test = logits_test.argmax(1)\n",
        "\n",
        "        train_acc = (pred == labels[train_mask]).float().mean()\n",
        "        test_acc = (pred_test == labels[test_mask]).float().mean()\n",
        "\n",
        "        if best_train_acc < train_acc:\n",
        "            best_train_acc = train_acc\n",
        "            corresponding_test=test_acc\n",
        "        if best_test_acc < test_acc:\n",
        "            best_test_acc = test_acc\n",
        "\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if e % 5 == 0:\n",
        "            epoch.append(e)\n",
        "            los.append(round(loss.item(),3))\n",
        "            train_accuracy.append(round(train_acc.item(),3))\n",
        "\n",
        "    TT.append({'train_accuracy':best_train_acc})\n",
        "    CT.append({'test_accuracy': corresponding_test})\n",
        "\n",
        "    print(f'Train Accuracy: {best_train_acc}, Test Accuracy: {corresponding_test}')\n",
        "    return best_train_acc"
      ],
      "metadata": {
        "id": "ls8Rsgc5wSEd"
      },
      "id": "ls8Rsgc5wSEd",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "import optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgGHl72bwikI",
        "outputId": "3ae01619-d87d-4bf7-af65-616313119785"
      },
      "id": "fgGHl72bwikI",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.6 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percentiles=[round(0.8765416432055043,4)]\n",
        "# percentiles=[round(0.360498,4),round(0.424333,4),round(0.497295,4),round(0.6061553838915714,4)]"
      ],
      "metadata": {
        "id": "TEeEjr-gwipT"
      },
      "id": "TEeEjr-gwipT",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    percentile_values=[percentiles[0]]\n",
        "    learning_rate=[0.01,0.1,1,10,100,200]\n",
        "    output_features=[10,12,14,16,18,20,22]\n",
        "    params={'output_features':trial.suggest_categorical('output_features',output_features),\n",
        "           'optimizer':trial.suggest_categorical('optimizer',[\"Adam\", \"RMSprop\", \"SGD\"]),\n",
        "           'percentile':trial.suggest_categorical('percentile',percentile_values),\n",
        "           'learning_rate':trial.suggest_categorical('learning_rate',learning_rate),\n",
        "           'loss':trial.suggest_categorical('loss',['cross_entropy']),\n",
        "           'activation':trial.suggest_categorical('activation',['relu','selu','elu','leaky_relu','tanh'])}\n",
        "    dataset = KarateClubDataset(params)\n",
        "    g = dataset[0]\n",
        "    train_mask = g.ndata[\"train_mask\"]\n",
        "    for i in range(106):\n",
        "        train_mask[i] = True\n",
        "\n",
        "    indices_to_change = [4, 105, 84, 27, 98, 88, 18, 65, 9, 2, 5, 49, 99, 69, 86, 67, 7, 28, 78, 70, 18, 74]\n",
        "    train_mask[indices_to_change] = False\n",
        "    g.ndata[\"train_mask\"]=train_mask\n",
        "    test_mask = ~train_mask\n",
        "    g.ndata[\"test_mask\"]=test_mask\n",
        "    sg_train=dgl.node_subgraph(g, train_mask)\n",
        "    sg_test = dgl.node_subgraph(g, test_mask)\n",
        "    model=GCN(g.ndata[\"feat\"].shape[1],params, 2)\n",
        "    accuracy=train(g,sg_train,sg_test,model,params)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "rSyGDfHgwy1t"
      },
      "id": "rSyGDfHgwy1t",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# study = optuna.create_study(direction=\"maximize\")\n",
        "# study.optimize(objective, n_trials=630)\n",
        "\n",
        "import time\n",
        "\n",
        "# Initialize the study\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "\n",
        "# Measure the total time for all trials\n",
        "start_time = time.time()  # Start time before optimization\n",
        "\n",
        "# Perform the optimization\n",
        "study.optimize(objective, n_trials=630)\n",
        "\n",
        "end_time = time.time()  # End time after optimization\n",
        "\n",
        "# Calculate total compute time\n",
        "total_compute_time = end_time - start_time\n",
        "\n",
        "print(f\"Total computation time for all 630 trials: {total_compute_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpRxquLowy5P",
        "outputId": "b3b2fa53-f8a2-4d77-a5fd-75da4483a1fc"
      },
      "id": "bpRxquLowy5P",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:06:56,854] A new study created in memory with name: no-name-dca3c5e3-8e90-4f62-8ac9-5b19360e698c\n",
            "<ipython-input-48-a8f79a39848d>:25: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "  node_labels = torch.from_numpy(\n",
            "[I 2024-11-23 22:06:58,150] Trial 0 finished with value: 0.7411764860153198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 0 with value: 0.7411764860153198.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:06:59,509] Trial 1 finished with value: 0.7529411911964417 and parameters: {'output_features': 14, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 1 with value: 0.7529411911964417.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7529411911964417, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:01,085] Trial 2 finished with value: 0.6470588445663452 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 1 with value: 0.7529411911964417.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:02,968] Trial 3 finished with value: 0.929411768913269 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:04,550] Trial 4 finished with value: 0.6470588445663452 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:06,032] Trial 5 finished with value: 0.729411780834198 and parameters: {'output_features': 16, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:09,132] Trial 6 finished with value: 0.7058823704719543 and parameters: {'output_features': 14, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:11,049] Trial 7 finished with value: 0.6470588445663452 and parameters: {'output_features': 22, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:12,489] Trial 8 finished with value: 0.7529411911964417 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7529411911964417, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:13,775] Trial 9 finished with value: 0.658823549747467 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.658823549747467, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:15,490] Trial 10 finished with value: 0.8941176533699036 and parameters: {'output_features': 10, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:16,846] Trial 11 finished with value: 0.8823529481887817 and parameters: {'output_features': 10, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:18,509] Trial 12 finished with value: 0.8941176533699036 and parameters: {'output_features': 10, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:20,368] Trial 13 finished with value: 0.8588235378265381 and parameters: {'output_features': 10, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:22,821] Trial 14 finished with value: 0.8941176533699036 and parameters: {'output_features': 14, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:25,384] Trial 15 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:27,236] Trial 16 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:29,589] Trial 17 finished with value: 0.6470588445663452 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:31,114] Trial 18 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:32,731] Trial 19 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:34,628] Trial 20 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:37,829] Trial 21 finished with value: 0.9058823585510254 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:40,794] Trial 22 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:41,610] Trial 23 finished with value: 0.929411768913269 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:42,430] Trial 24 finished with value: 0.8941176533699036 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:43,179] Trial 25 finished with value: 0.6470588445663452 and parameters: {'output_features': 14, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:44,015] Trial 26 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:44,820] Trial 27 finished with value: 0.8941176533699036 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:45,639] Trial 28 finished with value: 0.9176470637321472 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:46,451] Trial 29 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:47,186] Trial 30 finished with value: 0.6941176652908325 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:47,992] Trial 31 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:48,790] Trial 32 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:50,038] Trial 33 finished with value: 0.7529411911964417 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7529411911964417, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:51,084] Trial 34 finished with value: 0.6470588445663452 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:52,218] Trial 35 finished with value: 0.8705882430076599 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:53,268] Trial 36 finished with value: 0.6470588445663452 and parameters: {'output_features': 18, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:54,211] Trial 37 finished with value: 0.6470588445663452 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:55,024] Trial 38 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:55,829] Trial 39 finished with value: 0.6470588445663452 and parameters: {'output_features': 16, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:56,606] Trial 40 finished with value: 0.8705882430076599 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:57,431] Trial 41 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:58,210] Trial 42 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:59,008] Trial 43 finished with value: 0.8823529481887817 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:07:59,812] Trial 44 finished with value: 0.8823529481887817 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:00,581] Trial 45 finished with value: 0.658823549747467 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.658823549747467, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:01,369] Trial 46 finished with value: 0.8941176533699036 and parameters: {'output_features': 14, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:02,183] Trial 47 finished with value: 0.8823529481887817 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:03,009] Trial 48 finished with value: 0.7529411911964417 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7529411911964417, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:03,774] Trial 49 finished with value: 0.8352941274642944 and parameters: {'output_features': 10, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8352941274642944, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:04,885] Trial 50 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:06,023] Trial 51 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:07,158] Trial 52 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:07,963] Trial 53 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:08,759] Trial 54 finished with value: 0.8823529481887817 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:09,592] Trial 55 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:10,398] Trial 56 finished with value: 0.7411764860153198 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:11,152] Trial 57 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:11,973] Trial 58 finished with value: 0.8588235378265381 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:12,797] Trial 59 finished with value: 0.8705882430076599 and parameters: {'output_features': 10, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:13,609] Trial 60 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:14,418] Trial 61 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:15,236] Trial 62 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:16,070] Trial 63 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:16,865] Trial 64 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:17,935] Trial 65 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:19,011] Trial 66 finished with value: 0.6470588445663452 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:20,269] Trial 67 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:21,161] Trial 68 finished with value: 0.7647058963775635 and parameters: {'output_features': 22, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7647058963775635, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:21,949] Trial 69 finished with value: 0.8941176533699036 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:22,875] Trial 70 finished with value: 0.729411780834198 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:23,742] Trial 71 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:24,559] Trial 72 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:25,391] Trial 73 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:26,226] Trial 74 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:27,098] Trial 75 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:27,999] Trial 76 finished with value: 0.7176470756530762 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:28,795] Trial 77 finished with value: 0.8941176533699036 and parameters: {'output_features': 18, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:29,632] Trial 78 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:30,423] Trial 79 finished with value: 0.658823549747467 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.658823549747467, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:31,547] Trial 80 finished with value: 0.8588235378265381 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:32,704] Trial 81 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:33,913] Trial 82 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:35,007] Trial 83 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:35,835] Trial 84 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:36,662] Trial 85 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:37,410] Trial 86 finished with value: 0.7176470756530762 and parameters: {'output_features': 10, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:38,415] Trial 87 finished with value: 0.7411764860153198 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:39,234] Trial 88 finished with value: 0.6470588445663452 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:40,068] Trial 89 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:40,874] Trial 90 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:41,729] Trial 91 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:42,596] Trial 92 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:43,442] Trial 93 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:44,293] Trial 94 finished with value: 0.8705882430076599 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:45,270] Trial 95 finished with value: 0.9176470637321472 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:46,369] Trial 96 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:47,797] Trial 97 finished with value: 0.729411780834198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:48,667] Trial 98 finished with value: 0.8823529481887817 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:49,503] Trial 99 finished with value: 0.8941176533699036 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:50,270] Trial 100 finished with value: 0.6470588445663452 and parameters: {'output_features': 22, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:51,151] Trial 101 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:51,991] Trial 102 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:52,841] Trial 103 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:53,680] Trial 104 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:54,486] Trial 105 finished with value: 0.8705882430076599 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:55,334] Trial 106 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:56,132] Trial 107 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:56,918] Trial 108 finished with value: 0.6470588445663452 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:57,718] Trial 109 finished with value: 0.7411764860153198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:58,837] Trial 110 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:08:59,958] Trial 111 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:01,204] Trial 112 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:02,041] Trial 113 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:02,864] Trial 114 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:03,676] Trial 115 finished with value: 0.9176470637321472 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:04,501] Trial 116 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:05,305] Trial 117 finished with value: 0.8705882430076599 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:06,123] Trial 118 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:06,913] Trial 119 finished with value: 0.6823529601097107 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:07,708] Trial 120 finished with value: 0.8941176533699036 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:08,537] Trial 121 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:09,386] Trial 122 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:11,089] Trial 123 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:12,138] Trial 124 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:13,251] Trial 125 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:14,438] Trial 126 finished with value: 0.6823529601097107 and parameters: {'output_features': 22, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:15,398] Trial 127 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:16,233] Trial 128 finished with value: 0.6470588445663452 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:17,050] Trial 129 finished with value: 0.8941176533699036 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:17,825] Trial 130 finished with value: 0.929411768913269 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:18,615] Trial 131 finished with value: 0.8823529481887817 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:19,430] Trial 132 finished with value: 0.8941176533699036 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:20,222] Trial 133 finished with value: 0.9058823585510254 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:21,035] Trial 134 finished with value: 0.8823529481887817 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:21,879] Trial 135 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:22,691] Trial 136 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:23,502] Trial 137 finished with value: 0.8235294222831726 and parameters: {'output_features': 10, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8235294222831726, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:24,333] Trial 138 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:25,318] Trial 139 finished with value: 0.6470588445663452 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:26,402] Trial 140 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:27,621] Trial 141 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:28,568] Trial 142 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:29,394] Trial 143 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:30,182] Trial 144 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:30,998] Trial 145 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:31,794] Trial 146 finished with value: 0.8941176533699036 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:32,581] Trial 147 finished with value: 0.9176470637321472 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:33,318] Trial 148 finished with value: 0.3529411852359772 and parameters: {'output_features': 14, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.3529411852359772, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:34,140] Trial 149 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:34,960] Trial 150 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:35,773] Trial 151 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:36,582] Trial 152 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:37,411] Trial 153 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:38,352] Trial 154 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:39,434] Trial 155 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:40,578] Trial 156 finished with value: 0.9176470637321472 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.8095238208770752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:41,634] Trial 157 finished with value: 0.658823549747467 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.658823549747467, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:42,440] Trial 158 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:43,269] Trial 159 finished with value: 0.9058823585510254 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:44,084] Trial 160 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:44,891] Trial 161 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:45,703] Trial 162 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:46,516] Trial 163 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:47,373] Trial 164 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:48,171] Trial 165 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:48,968] Trial 166 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:49,767] Trial 167 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:50,986] Trial 168 finished with value: 0.729411780834198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:52,044] Trial 169 finished with value: 0.7058823704719543 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:53,150] Trial 170 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:54,340] Trial 171 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:55,458] Trial 172 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:56,284] Trial 173 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:57,143] Trial 174 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:57,934] Trial 175 finished with value: 0.8823529481887817 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:58,753] Trial 176 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:09:59,558] Trial 177 finished with value: 0.8941176533699036 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:00,316] Trial 178 finished with value: 0.6470588445663452 and parameters: {'output_features': 22, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:01,142] Trial 179 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:02,070] Trial 180 finished with value: 0.729411780834198 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:02,889] Trial 181 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:03,718] Trial 182 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:04,546] Trial 183 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:05,385] Trial 184 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:06,473] Trial 185 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:07,608] Trial 186 finished with value: 0.8823529481887817 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:08,758] Trial 187 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:09,743] Trial 188 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:10,541] Trial 189 finished with value: 0.8823529481887817 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:11,385] Trial 190 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:12,222] Trial 191 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:13,053] Trial 192 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:13,869] Trial 193 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:14,687] Trial 194 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:15,532] Trial 195 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:16,351] Trial 196 finished with value: 0.658823549747467 and parameters: {'output_features': 22, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.658823549747467, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:17,171] Trial 197 finished with value: 0.8823529481887817 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:17,970] Trial 198 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:18,786] Trial 199 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:19,823] Trial 200 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:20,999] Trial 201 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:22,239] Trial 202 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:23,366] Trial 203 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:24,218] Trial 204 finished with value: 0.9176470637321472 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:25,077] Trial 205 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:25,913] Trial 206 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:26,770] Trial 207 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:27,594] Trial 208 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:28,427] Trial 209 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:29,230] Trial 210 finished with value: 0.9058823585510254 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:30,069] Trial 211 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:30,899] Trial 212 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:31,728] Trial 213 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:32,552] Trial 214 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:33,469] Trial 215 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:34,572] Trial 216 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:35,663] Trial 217 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:36,789] Trial 218 finished with value: 0.8117647171020508 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:37,599] Trial 219 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:38,391] Trial 220 finished with value: 0.8941176533699036 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:39,225] Trial 221 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:40,053] Trial 222 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:40,888] Trial 223 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:41,780] Trial 224 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:42,592] Trial 225 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:43,420] Trial 226 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:44,246] Trial 227 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:45,099] Trial 228 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:45,922] Trial 229 finished with value: 0.9058823585510254 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:46,821] Trial 230 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:47,922] Trial 231 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:49,128] Trial 232 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:50,285] Trial 233 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:51,237] Trial 234 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:52,106] Trial 235 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:52,909] Trial 236 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:53,692] Trial 237 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:54,513] Trial 238 finished with value: 0.658823549747467 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.658823549747467, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:55,301] Trial 239 finished with value: 0.9058823585510254 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:56,161] Trial 240 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:57,803] Trial 241 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:58,734] Trial 242 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:10:59,562] Trial 243 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:00,382] Trial 244 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:01,411] Trial 245 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:02,478] Trial 246 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:03,663] Trial 247 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:04,705] Trial 248 finished with value: 0.8941176533699036 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:05,521] Trial 249 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:06,358] Trial 250 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:07,157] Trial 251 finished with value: 0.7529411911964417 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7529411911964417, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:07,895] Trial 252 finished with value: 0.729411780834198 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:08,701] Trial 253 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:09,559] Trial 254 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:10,360] Trial 255 finished with value: 0.9176470637321472 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:11,171] Trial 256 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:11,964] Trial 257 finished with value: 0.8705882430076599 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:12,809] Trial 258 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:13,663] Trial 259 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:14,660] Trial 260 finished with value: 0.6941176652908325 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:15,755] Trial 261 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:16,879] Trial 262 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:18,002] Trial 263 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:18,805] Trial 264 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:19,638] Trial 265 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:20,460] Trial 266 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:21,269] Trial 267 finished with value: 0.9058823585510254 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:22,102] Trial 268 finished with value: 0.658823549747467 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.658823549747467, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:22,922] Trial 269 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:23,727] Trial 270 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:24,559] Trial 271 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:25,344] Trial 272 finished with value: 0.929411768913269 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:26,114] Trial 273 finished with value: 0.8823529481887817 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:26,915] Trial 274 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:27,653] Trial 275 finished with value: 0.6941176652908325 and parameters: {'output_features': 14, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:28,696] Trial 276 finished with value: 0.8941176533699036 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:29,912] Trial 277 finished with value: 0.7529411911964417 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7529411911964417, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:31,068] Trial 278 finished with value: 0.6470588445663452 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:32,222] Trial 279 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:33,043] Trial 280 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:33,866] Trial 281 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:34,687] Trial 282 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:35,512] Trial 283 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.8571428656578064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:36,346] Trial 284 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:37,150] Trial 285 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:38,038] Trial 286 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:38,861] Trial 287 finished with value: 0.8705882430076599 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:39,670] Trial 288 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:40,499] Trial 289 finished with value: 0.8941176533699036 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:41,355] Trial 290 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:42,262] Trial 291 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:43,385] Trial 292 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:44,631] Trial 293 finished with value: 0.8705882430076599 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:45,777] Trial 294 finished with value: 0.8823529481887817 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:46,682] Trial 295 finished with value: 0.9176470637321472 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:47,514] Trial 296 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:48,344] Trial 297 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:49,164] Trial 298 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:49,987] Trial 299 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:50,761] Trial 300 finished with value: 0.6470588445663452 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:51,512] Trial 301 finished with value: 0.6823529601097107 and parameters: {'output_features': 22, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:52,356] Trial 302 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:53,174] Trial 303 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:53,983] Trial 304 finished with value: 0.9058823585510254 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:54,839] Trial 305 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:55,640] Trial 306 finished with value: 0.7411764860153198 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:56,649] Trial 307 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:57,702] Trial 308 finished with value: 0.6470588445663452 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:58,852] Trial 309 finished with value: 0.8941176533699036 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:11:59,981] Trial 310 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:00,773] Trial 311 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:01,594] Trial 312 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:02,450] Trial 313 finished with value: 0.8941176533699036 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:03,243] Trial 314 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:04,088] Trial 315 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:04,920] Trial 316 finished with value: 0.8823529481887817 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:05,747] Trial 317 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:06,691] Trial 318 finished with value: 0.7411764860153198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:07,515] Trial 319 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:08,331] Trial 320 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:09,121] Trial 321 finished with value: 0.8823529481887817 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:10,011] Trial 322 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:11,054] Trial 323 finished with value: 0.8941176533699036 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:12,165] Trial 324 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:13,321] Trial 325 finished with value: 0.6705882549285889 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6705882549285889, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:14,077] Trial 326 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:14,938] Trial 327 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:15,779] Trial 328 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:16,565] Trial 329 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:17,373] Trial 330 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:18,195] Trial 331 finished with value: 0.8941176533699036 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:19,033] Trial 332 finished with value: 0.6823529601097107 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:19,825] Trial 333 finished with value: 0.8588235378265381 and parameters: {'output_features': 16, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:20,643] Trial 334 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:21,440] Trial 335 finished with value: 0.6705882549285889 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6705882549285889, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:22,278] Trial 336 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:23,094] Trial 337 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:24,141] Trial 338 finished with value: 0.8823529481887817 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:25,236] Trial 339 finished with value: 0.929411768913269 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:26,453] Trial 340 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:27,343] Trial 341 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:28,141] Trial 342 finished with value: 0.9058823585510254 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:28,965] Trial 343 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:29,796] Trial 344 finished with value: 0.7058823704719543 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:30,565] Trial 345 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:31,424] Trial 346 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:32,212] Trial 347 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:33,060] Trial 348 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:33,867] Trial 349 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:34,662] Trial 350 finished with value: 0.8941176533699036 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:35,829] Trial 351 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:36,912] Trial 352 finished with value: 0.658823549747467 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.658823549747467, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:38,493] Trial 353 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:40,360] Trial 354 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:41,927] Trial 355 finished with value: 0.8823529481887817 and parameters: {'output_features': 12, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:43,126] Trial 356 finished with value: 0.929411768913269 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:45,033] Trial 357 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:45,887] Trial 358 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:46,850] Trial 359 finished with value: 0.6470588445663452 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:47,701] Trial 360 finished with value: 0.6470588445663452 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:48,526] Trial 361 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:49,367] Trial 362 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:50,181] Trial 363 finished with value: 0.9176470637321472 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:51,023] Trial 364 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:51,939] Trial 365 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:53,064] Trial 366 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:54,289] Trial 367 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:55,374] Trial 368 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:56,326] Trial 369 finished with value: 0.8705882430076599 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:57,197] Trial 370 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:58,019] Trial 371 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:59,069] Trial 372 finished with value: 0.7529411911964417 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7529411911964417, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:12:59,924] Trial 373 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:00,772] Trial 374 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:01,598] Trial 375 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:02,422] Trial 376 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:03,230] Trial 377 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:04,023] Trial 378 finished with value: 0.8705882430076599 and parameters: {'output_features': 22, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:04,846] Trial 379 finished with value: 0.929411768913269 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:05,652] Trial 380 finished with value: 0.8705882430076599 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:06,755] Trial 381 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:07,894] Trial 382 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:09,160] Trial 383 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:10,085] Trial 384 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:11,058] Trial 385 finished with value: 0.658823549747467 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.658823549747467, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:11,892] Trial 386 finished with value: 0.7529411911964417 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7529411911964417, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:12,684] Trial 387 finished with value: 0.9058823585510254 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:13,519] Trial 388 finished with value: 0.9176470637321472 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:14,402] Trial 389 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:15,254] Trial 390 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:16,030] Trial 391 finished with value: 0.6470588445663452 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:16,843] Trial 392 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:17,707] Trial 393 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:18,537] Trial 394 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:19,335] Trial 395 finished with value: 0.8941176533699036 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:20,492] Trial 396 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:22,269] Trial 397 finished with value: 0.729411780834198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:23,354] Trial 398 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:24,190] Trial 399 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:24,993] Trial 400 finished with value: 0.8941176533699036 and parameters: {'output_features': 12, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:25,844] Trial 401 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:26,677] Trial 402 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:27,490] Trial 403 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:28,290] Trial 404 finished with value: 0.6470588445663452 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:29,109] Trial 405 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:29,917] Trial 406 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:30,727] Trial 407 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:31,514] Trial 408 finished with value: 0.8941176533699036 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:32,343] Trial 409 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:33,181] Trial 410 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:34,419] Trial 411 finished with value: 0.7529411911964417 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7529411911964417, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:35,585] Trial 412 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:36,706] Trial 413 finished with value: 0.7411764860153198 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:37,463] Trial 414 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:38,257] Trial 415 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:39,032] Trial 416 finished with value: 0.8823529481887817 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:39,881] Trial 417 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:40,705] Trial 418 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:41,528] Trial 419 finished with value: 0.9058823585510254 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:42,344] Trial 420 finished with value: 0.9176470637321472 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:43,168] Trial 421 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:44,124] Trial 422 finished with value: 0.729411780834198 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:44,964] Trial 423 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:45,825] Trial 424 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:46,650] Trial 425 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:47,765] Trial 426 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:48,881] Trial 427 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:50,032] Trial 428 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:51,017] Trial 429 finished with value: 0.9058823585510254 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:51,853] Trial 430 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:52,728] Trial 431 finished with value: 0.658823549747467 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.658823549747467, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:53,516] Trial 432 finished with value: 0.9058823585510254 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:54,367] Trial 433 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:55,212] Trial 434 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:56,019] Trial 435 finished with value: 0.7058823704719543 and parameters: {'output_features': 22, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:56,823] Trial 436 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:57,779] Trial 437 finished with value: 0.7411764860153198 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:58,608] Trial 438 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:13:59,401] Trial 439 finished with value: 0.6470588445663452 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:00,248] Trial 440 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:01,316] Trial 441 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:02,445] Trial 442 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:03,627] Trial 443 finished with value: 0.8941176533699036 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:04,596] Trial 444 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:05,404] Trial 445 finished with value: 0.9058823585510254 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:06,236] Trial 446 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:07,064] Trial 447 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:07,933] Trial 448 finished with value: 0.7411764860153198 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.2857142984867096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:08,743] Trial 449 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:09,564] Trial 450 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:10,369] Trial 451 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:11,189] Trial 452 finished with value: 0.8941176533699036 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:12,029] Trial 453 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:12,880] Trial 454 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:13,699] Trial 455 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:14,630] Trial 456 finished with value: 0.6470588445663452 and parameters: {'output_features': 22, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:15,761] Trial 457 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:16,963] Trial 458 finished with value: 0.9058823585510254 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:18,139] Trial 459 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:19,009] Trial 460 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:19,835] Trial 461 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:20,657] Trial 462 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:21,529] Trial 463 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:22,362] Trial 464 finished with value: 0.6470588445663452 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:23,404] Trial 465 finished with value: 0.7058823704719543 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:24,208] Trial 466 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:25,024] Trial 467 finished with value: 0.9058823585510254 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:25,871] Trial 468 finished with value: 0.9176470637321472 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:26,712] Trial 469 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:27,549] Trial 470 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:28,424] Trial 471 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:29,526] Trial 472 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:30,746] Trial 473 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:31,927] Trial 474 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:32,873] Trial 475 finished with value: 0.7529411911964417 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7529411911964417, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:33,697] Trial 476 finished with value: 0.929411768913269 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:34,520] Trial 477 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:35,289] Trial 478 finished with value: 0.6470588445663452 and parameters: {'output_features': 12, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:36,140] Trial 479 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:36,960] Trial 480 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:37,779] Trial 481 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:38,621] Trial 482 finished with value: 0.6823529601097107 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6823529601097107, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:39,453] Trial 483 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:40,287] Trial 484 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:41,122] Trial 485 finished with value: 0.8705882430076599 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:42,032] Trial 486 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:43,168] Trial 487 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:44,343] Trial 488 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:45,461] Trial 489 finished with value: 0.8941176533699036 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:46,486] Trial 490 finished with value: 0.729411780834198 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:47,306] Trial 491 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:48,138] Trial 492 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:48,995] Trial 493 finished with value: 0.6470588445663452 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:49,815] Trial 494 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:50,620] Trial 495 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:51,442] Trial 496 finished with value: 0.9058823585510254 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:52,280] Trial 497 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:53,139] Trial 498 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:53,936] Trial 499 finished with value: 0.8941176533699036 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:54,765] Trial 500 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:55,703] Trial 501 finished with value: 0.658823549747467 and parameters: {'output_features': 22, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.658823549747467, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:56,792] Trial 502 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:58,004] Trial 503 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:59,131] Trial 504 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:14:59,916] Trial 505 finished with value: 0.9176470637321472 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:00,744] Trial 506 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:01,596] Trial 507 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:02,417] Trial 508 finished with value: 0.658823549747467 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.658823549747467, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:03,250] Trial 509 finished with value: 0.8941176533699036 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:04,163] Trial 510 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:04,961] Trial 511 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:05,816] Trial 512 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:06,670] Trial 513 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:07,471] Trial 514 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:08,285] Trial 515 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:09,241] Trial 516 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:10,323] Trial 517 finished with value: 0.6941176652908325 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:11,516] Trial 518 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:12,634] Trial 519 finished with value: 0.929411768913269 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:13,495] Trial 520 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:14,318] Trial 521 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:15,131] Trial 522 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:15,877] Trial 523 finished with value: 0.7176470756530762 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:16,679] Trial 524 finished with value: 0.929411768913269 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:17,517] Trial 525 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:18,311] Trial 526 finished with value: 0.8823529481887817 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:19,674] Trial 527 finished with value: 0.7176470756530762 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7176470756530762, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:20,484] Trial 528 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:21,323] Trial 529 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:22,100] Trial 530 finished with value: 0.8941176533699036 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:23,139] Trial 531 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:24,240] Trial 532 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:25,382] Trial 533 finished with value: 0.9176470637321472 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:26,419] Trial 534 finished with value: 0.8117647171020508 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8117647171020508, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:27,220] Trial 535 finished with value: 0.8705882430076599 and parameters: {'output_features': 22, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:28,036] Trial 536 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:28,865] Trial 537 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:29,665] Trial 538 finished with value: 0.9176470637321472 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:30,453] Trial 539 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:31,271] Trial 540 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:32,162] Trial 541 finished with value: 0.7411764860153198 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7411764860153198, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:32,975] Trial 542 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:33,856] Trial 543 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:34,748] Trial 544 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:35,529] Trial 545 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:36,342] Trial 546 finished with value: 0.658823549747467 and parameters: {'output_features': 14, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.658823549747467, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:37,453] Trial 547 finished with value: 0.9058823585510254 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:38,622] Trial 548 finished with value: 0.8823529481887817 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:39,792] Trial 549 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:40,648] Trial 550 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:41,644] Trial 551 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:42,496] Trial 552 finished with value: 0.8705882430076599 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:43,290] Trial 553 finished with value: 0.8823529481887817 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:44,096] Trial 554 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:45,103] Trial 555 finished with value: 0.729411780834198 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:45,901] Trial 556 finished with value: 0.9176470637321472 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:46,710] Trial 557 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:47,513] Trial 558 finished with value: 0.8705882430076599 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:48,341] Trial 559 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:49,141] Trial 560 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:50,069] Trial 561 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:51,109] Trial 562 finished with value: 0.6470588445663452 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:52,272] Trial 563 finished with value: 0.8823529481887817 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:53,400] Trial 564 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:54,269] Trial 565 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:55,103] Trial 566 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:56,002] Trial 567 finished with value: 0.6470588445663452 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6470588445663452, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:56,824] Trial 568 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:57,657] Trial 569 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:58,453] Trial 570 finished with value: 0.8941176533699036 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:15:59,266] Trial 571 finished with value: 0.8941176533699036 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:00,106] Trial 572 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:00,936] Trial 573 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:01,762] Trial 574 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:02,557] Trial 575 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:03,456] Trial 576 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:04,565] Trial 577 finished with value: 0.8941176533699036 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:05,703] Trial 578 finished with value: 0.8823529481887817 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:06,814] Trial 579 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:08,017] Trial 580 finished with value: 0.7529411911964417 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7529411911964417, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:08,815] Trial 581 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:09,651] Trial 582 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:10,447] Trial 583 finished with value: 0.8823529481887817 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:11,293] Trial 584 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:12,111] Trial 585 finished with value: 0.8823529481887817 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8823529481887817, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:12,894] Trial 586 finished with value: 0.658823549747467 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.658823549747467, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:13,704] Trial 587 finished with value: 0.8941176533699036 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:14,547] Trial 588 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:15,328] Trial 589 finished with value: 0.7529411911964417 and parameters: {'output_features': 20, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7529411911964417, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:16,164] Trial 590 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:17,132] Trial 591 finished with value: 0.9176470637321472 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.2380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:18,198] Trial 592 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:19,398] Trial 593 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:21,048] Trial 594 finished with value: 0.6941176652908325 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:21,854] Trial 595 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:22,689] Trial 596 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:23,537] Trial 597 finished with value: 0.929411768913269 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:24,389] Trial 598 finished with value: 0.729411780834198 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:25,271] Trial 599 finished with value: 0.8941176533699036 and parameters: {'output_features': 20, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:26,072] Trial 600 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:26,895] Trial 601 finished with value: 0.8941176533699036 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:27,681] Trial 602 finished with value: 0.8941176533699036 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:28,510] Trial 603 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:29,322] Trial 604 finished with value: 0.9058823585510254 and parameters: {'output_features': 18, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:30,464] Trial 605 finished with value: 0.7529411911964417 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'selu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7529411911964417, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:31,529] Trial 606 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:32,669] Trial 607 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:34,007] Trial 608 finished with value: 0.9176470637321472 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:34,903] Trial 609 finished with value: 0.8705882430076599 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 10, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:35,692] Trial 610 finished with value: 0.8705882430076599 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8705882430076599, Test Accuracy: 0.7142857313156128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:36,475] Trial 611 finished with value: 0.6941176652908325 and parameters: {'output_features': 22, 'optimizer': 'SGD', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6941176652908325, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:37,315] Trial 612 finished with value: 0.929411768913269 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.929411768913269, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:38,112] Trial 613 finished with value: 0.658823549747467 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'tanh'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.658823549747467, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:38,919] Trial 614 finished with value: 0.9176470637321472 and parameters: {'output_features': 10, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6190476417541504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:39,746] Trial 615 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.3333333432674408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:40,556] Trial 616 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:41,381] Trial 617 finished with value: 0.9176470637321472 and parameters: {'output_features': 12, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:42,188] Trial 618 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.380952388048172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:43,127] Trial 619 finished with value: 0.729411780834198 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'elu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.729411780834198, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:43,954] Trial 620 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'RMSprop', 'percentile': 0.8765, 'learning_rate': 100, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:45,007] Trial 621 finished with value: 0.8941176533699036 and parameters: {'output_features': 16, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:46,152] Trial 622 finished with value: 0.7058823704719543 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7058823704719543, Test Accuracy: 0.4285714328289032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:47,536] Trial 623 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.4761904776096344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:48,410] Trial 624 finished with value: 0.9058823585510254 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 0.01, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.523809552192688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:49,260] Trial 625 finished with value: 0.8941176533699036 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8941176533699036, Test Accuracy: 0.6666666865348816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:50,095] Trial 626 finished with value: 0.9058823585510254 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:50,902] Trial 627 finished with value: 0.9058823585510254 and parameters: {'output_features': 14, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9058823585510254, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:51,714] Trial 628 finished with value: 0.8588235378265381 and parameters: {'output_features': 20, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 200, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8588235378265381, Test Accuracy: 0.5714285969734192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-23 22:16:52,542] Trial 629 finished with value: 0.9176470637321472 and parameters: {'output_features': 22, 'optimizer': 'Adam', 'percentile': 0.8765, 'learning_rate': 1, 'loss': 'cross_entropy', 'activation': 'leaky_relu'}. Best is trial 3 with value: 0.929411768913269.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9176470637321472, Test Accuracy: 0.4285714328289032\n",
            "Total computation time for all 630 trials: 595.6845 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "\n",
        "# Assuming CT is a list of dictionaries and each dictionary has a 'test_accuracy' key\n",
        "top_ten_max_results = heapq.nlargest(50, enumerate(CT), key=lambda x: x[1]['test_accuracy'])\n",
        "\n",
        "# Print the top ten results along with their indices\n",
        "for index, result in top_ten_max_results:\n",
        "    print(f'Index: {index}, Test Accuracy: {result[\"test_accuracy\"]}')"
      ],
      "metadata": {
        "id": "_M3gBwZJxu6I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca2bbcc1-5717-4312-f8b3-eca38a7d7ce1"
      },
      "id": "_M3gBwZJxu6I",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index: 283, Test Accuracy: 0.8571428656578064\n",
            "Index: 156, Test Accuracy: 0.8095238208770752\n",
            "Index: 101, Test Accuracy: 0.761904776096344\n",
            "Index: 115, Test Accuracy: 0.761904776096344\n",
            "Index: 121, Test Accuracy: 0.761904776096344\n",
            "Index: 127, Test Accuracy: 0.761904776096344\n",
            "Index: 176, Test Accuracy: 0.761904776096344\n",
            "Index: 179, Test Accuracy: 0.761904776096344\n",
            "Index: 215, Test Accuracy: 0.761904776096344\n",
            "Index: 246, Test Accuracy: 0.761904776096344\n",
            "Index: 247, Test Accuracy: 0.761904776096344\n",
            "Index: 309, Test Accuracy: 0.761904776096344\n",
            "Index: 330, Test Accuracy: 0.761904776096344\n",
            "Index: 346, Test Accuracy: 0.761904776096344\n",
            "Index: 356, Test Accuracy: 0.761904776096344\n",
            "Index: 370, Test Accuracy: 0.761904776096344\n",
            "Index: 376, Test Accuracy: 0.761904776096344\n",
            "Index: 402, Test Accuracy: 0.761904776096344\n",
            "Index: 497, Test Accuracy: 0.761904776096344\n",
            "Index: 528, Test Accuracy: 0.761904776096344\n",
            "Index: 559, Test Accuracy: 0.761904776096344\n",
            "Index: 20, Test Accuracy: 0.7142857313156128\n",
            "Index: 27, Test Accuracy: 0.7142857313156128\n",
            "Index: 41, Test Accuracy: 0.7142857313156128\n",
            "Index: 62, Test Accuracy: 0.7142857313156128\n",
            "Index: 63, Test Accuracy: 0.7142857313156128\n",
            "Index: 73, Test Accuracy: 0.7142857313156128\n",
            "Index: 93, Test Accuracy: 0.7142857313156128\n",
            "Index: 104, Test Accuracy: 0.7142857313156128\n",
            "Index: 105, Test Accuracy: 0.7142857313156128\n",
            "Index: 111, Test Accuracy: 0.7142857313156128\n",
            "Index: 114, Test Accuracy: 0.7142857313156128\n",
            "Index: 131, Test Accuracy: 0.7142857313156128\n",
            "Index: 138, Test Accuracy: 0.7142857313156128\n",
            "Index: 141, Test Accuracy: 0.7142857313156128\n",
            "Index: 152, Test Accuracy: 0.7142857313156128\n",
            "Index: 155, Test Accuracy: 0.7142857313156128\n",
            "Index: 163, Test Accuracy: 0.7142857313156128\n",
            "Index: 172, Test Accuracy: 0.7142857313156128\n",
            "Index: 173, Test Accuracy: 0.7142857313156128\n",
            "Index: 181, Test Accuracy: 0.7142857313156128\n",
            "Index: 202, Test Accuracy: 0.7142857313156128\n",
            "Index: 208, Test Accuracy: 0.7142857313156128\n",
            "Index: 214, Test Accuracy: 0.7142857313156128\n",
            "Index: 218, Test Accuracy: 0.7142857313156128\n",
            "Index: 243, Test Accuracy: 0.7142857313156128\n",
            "Index: 254, Test Accuracy: 0.7142857313156128\n",
            "Index: 255, Test Accuracy: 0.7142857313156128\n",
            "Index: 257, Test Accuracy: 0.7142857313156128\n",
            "Index: 259, Test Accuracy: 0.7142857313156128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "o_f6BtNOMIY8",
        "outputId": "e8b041ab-60f5-4bac-8b72-b34d117c146d"
      },
      "id": "o_f6BtNOMIY8",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     index         0         1         2         3         4         5  \\\n",
              "0        0 -0.048460  0.084757  0.003497 -0.087329  0.059048 -0.209606   \n",
              "1        1 -0.065723  0.025389 -0.025804 -0.130403  0.077852 -0.120231   \n",
              "2        2 -0.045842  0.040817 -0.028311 -0.119993  0.082768 -0.169268   \n",
              "3        3 -0.049971  0.032408 -0.005414 -0.096241  0.066642 -0.137509   \n",
              "4        4 -0.045510  0.020915 -0.008356 -0.120176  0.046261 -0.174338   \n",
              "..     ...       ...       ...       ...       ...       ...       ...   \n",
              "101    101 -0.060480  0.032145  0.005485 -0.084974  0.062038 -0.145933   \n",
              "102    102 -0.055155 -0.000772 -0.013739 -0.069411  0.043931 -0.236069   \n",
              "103    103 -0.040352 -0.013974 -0.021251 -0.069300  0.054273 -0.207470   \n",
              "104    104 -0.051738  0.011170 -0.012154 -0.084336  0.054111 -0.143253   \n",
              "105    105 -0.051058  0.028734 -0.002164 -0.122716  0.081772 -0.161522   \n",
              "\n",
              "            6         7         8  ...       852       853       854  \\\n",
              "0   -0.006506  0.048831  0.017141  ...  0.245732  0.019818 -0.246347   \n",
              "1   -0.001798  0.034838  0.010572  ... -0.439034 -0.363662  0.125037   \n",
              "2   -0.019755  0.051325  0.019362  ... -0.171100  0.180284  0.044292   \n",
              "3   -0.046769  0.021914  0.060602  ...  0.026375  0.007201 -0.165179   \n",
              "4   -0.013932  0.018657  0.020323  ... -0.145272  0.047353 -0.150540   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "101 -0.047886  0.023430 -0.000973  ...  0.227745  0.008060 -0.163510   \n",
              "102  0.007162  0.073971 -0.005430  ... -0.044075  0.357937  0.251036   \n",
              "103 -0.014627  0.039035  0.016669  ... -0.011740  0.419290  0.062435   \n",
              "104 -0.028876  0.015378  0.012340  ... -0.069784  0.047647 -0.162943   \n",
              "105 -0.033399  0.069389 -0.016274  ...  0.070366 -0.196414  0.009736   \n",
              "\n",
              "          855       856       857       858       859       860       861  \n",
              "0   -0.334189  0.001933  0.099690  0.005821  0.098615  0.322466 -0.026514  \n",
              "1   -0.009941  0.000198  0.598846  0.399056 -0.448141 -0.133951  0.163589  \n",
              "2    0.000196  0.003606 -0.028537  0.019995  0.014549 -0.207424 -0.134106  \n",
              "3   -0.136537  0.035306 -0.042689 -0.063572 -0.160413  0.073046 -0.025301  \n",
              "4   -0.009131 -0.106808 -0.137046 -0.175852  0.152502  0.045290 -0.086046  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "101  0.093020 -0.086098  0.027652  0.021020  0.002824 -0.078523 -0.032950  \n",
              "102 -0.005687  0.141986 -0.258837  0.109927 -0.115133 -0.061322 -0.033643  \n",
              "103  0.191918  0.126068  0.287123 -0.040200  0.194610 -0.116119  0.169397  \n",
              "104 -0.053178  0.104861  0.008137 -0.253575 -0.041771 -0.019096  0.024908  \n",
              "105  0.119265  0.025462  0.260401  0.209111 -0.092553  0.108953 -0.142083  \n",
              "\n",
              "[106 rows x 863 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1d7fab85-1e67-4180-a54a-8f2a076a16c5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>852</th>\n",
              "      <th>853</th>\n",
              "      <th>854</th>\n",
              "      <th>855</th>\n",
              "      <th>856</th>\n",
              "      <th>857</th>\n",
              "      <th>858</th>\n",
              "      <th>859</th>\n",
              "      <th>860</th>\n",
              "      <th>861</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.048460</td>\n",
              "      <td>0.084757</td>\n",
              "      <td>0.003497</td>\n",
              "      <td>-0.087329</td>\n",
              "      <td>0.059048</td>\n",
              "      <td>-0.209606</td>\n",
              "      <td>-0.006506</td>\n",
              "      <td>0.048831</td>\n",
              "      <td>0.017141</td>\n",
              "      <td>...</td>\n",
              "      <td>0.245732</td>\n",
              "      <td>0.019818</td>\n",
              "      <td>-0.246347</td>\n",
              "      <td>-0.334189</td>\n",
              "      <td>0.001933</td>\n",
              "      <td>0.099690</td>\n",
              "      <td>0.005821</td>\n",
              "      <td>0.098615</td>\n",
              "      <td>0.322466</td>\n",
              "      <td>-0.026514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.065723</td>\n",
              "      <td>0.025389</td>\n",
              "      <td>-0.025804</td>\n",
              "      <td>-0.130403</td>\n",
              "      <td>0.077852</td>\n",
              "      <td>-0.120231</td>\n",
              "      <td>-0.001798</td>\n",
              "      <td>0.034838</td>\n",
              "      <td>0.010572</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.439034</td>\n",
              "      <td>-0.363662</td>\n",
              "      <td>0.125037</td>\n",
              "      <td>-0.009941</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.598846</td>\n",
              "      <td>0.399056</td>\n",
              "      <td>-0.448141</td>\n",
              "      <td>-0.133951</td>\n",
              "      <td>0.163589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.045842</td>\n",
              "      <td>0.040817</td>\n",
              "      <td>-0.028311</td>\n",
              "      <td>-0.119993</td>\n",
              "      <td>0.082768</td>\n",
              "      <td>-0.169268</td>\n",
              "      <td>-0.019755</td>\n",
              "      <td>0.051325</td>\n",
              "      <td>0.019362</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.171100</td>\n",
              "      <td>0.180284</td>\n",
              "      <td>0.044292</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.003606</td>\n",
              "      <td>-0.028537</td>\n",
              "      <td>0.019995</td>\n",
              "      <td>0.014549</td>\n",
              "      <td>-0.207424</td>\n",
              "      <td>-0.134106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-0.049971</td>\n",
              "      <td>0.032408</td>\n",
              "      <td>-0.005414</td>\n",
              "      <td>-0.096241</td>\n",
              "      <td>0.066642</td>\n",
              "      <td>-0.137509</td>\n",
              "      <td>-0.046769</td>\n",
              "      <td>0.021914</td>\n",
              "      <td>0.060602</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026375</td>\n",
              "      <td>0.007201</td>\n",
              "      <td>-0.165179</td>\n",
              "      <td>-0.136537</td>\n",
              "      <td>0.035306</td>\n",
              "      <td>-0.042689</td>\n",
              "      <td>-0.063572</td>\n",
              "      <td>-0.160413</td>\n",
              "      <td>0.073046</td>\n",
              "      <td>-0.025301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-0.045510</td>\n",
              "      <td>0.020915</td>\n",
              "      <td>-0.008356</td>\n",
              "      <td>-0.120176</td>\n",
              "      <td>0.046261</td>\n",
              "      <td>-0.174338</td>\n",
              "      <td>-0.013932</td>\n",
              "      <td>0.018657</td>\n",
              "      <td>0.020323</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.145272</td>\n",
              "      <td>0.047353</td>\n",
              "      <td>-0.150540</td>\n",
              "      <td>-0.009131</td>\n",
              "      <td>-0.106808</td>\n",
              "      <td>-0.137046</td>\n",
              "      <td>-0.175852</td>\n",
              "      <td>0.152502</td>\n",
              "      <td>0.045290</td>\n",
              "      <td>-0.086046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>101</td>\n",
              "      <td>-0.060480</td>\n",
              "      <td>0.032145</td>\n",
              "      <td>0.005485</td>\n",
              "      <td>-0.084974</td>\n",
              "      <td>0.062038</td>\n",
              "      <td>-0.145933</td>\n",
              "      <td>-0.047886</td>\n",
              "      <td>0.023430</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>...</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.008060</td>\n",
              "      <td>-0.163510</td>\n",
              "      <td>0.093020</td>\n",
              "      <td>-0.086098</td>\n",
              "      <td>0.027652</td>\n",
              "      <td>0.021020</td>\n",
              "      <td>0.002824</td>\n",
              "      <td>-0.078523</td>\n",
              "      <td>-0.032950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>102</td>\n",
              "      <td>-0.055155</td>\n",
              "      <td>-0.000772</td>\n",
              "      <td>-0.013739</td>\n",
              "      <td>-0.069411</td>\n",
              "      <td>0.043931</td>\n",
              "      <td>-0.236069</td>\n",
              "      <td>0.007162</td>\n",
              "      <td>0.073971</td>\n",
              "      <td>-0.005430</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.044075</td>\n",
              "      <td>0.357937</td>\n",
              "      <td>0.251036</td>\n",
              "      <td>-0.005687</td>\n",
              "      <td>0.141986</td>\n",
              "      <td>-0.258837</td>\n",
              "      <td>0.109927</td>\n",
              "      <td>-0.115133</td>\n",
              "      <td>-0.061322</td>\n",
              "      <td>-0.033643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>103</td>\n",
              "      <td>-0.040352</td>\n",
              "      <td>-0.013974</td>\n",
              "      <td>-0.021251</td>\n",
              "      <td>-0.069300</td>\n",
              "      <td>0.054273</td>\n",
              "      <td>-0.207470</td>\n",
              "      <td>-0.014627</td>\n",
              "      <td>0.039035</td>\n",
              "      <td>0.016669</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.011740</td>\n",
              "      <td>0.419290</td>\n",
              "      <td>0.062435</td>\n",
              "      <td>0.191918</td>\n",
              "      <td>0.126068</td>\n",
              "      <td>0.287123</td>\n",
              "      <td>-0.040200</td>\n",
              "      <td>0.194610</td>\n",
              "      <td>-0.116119</td>\n",
              "      <td>0.169397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>104</td>\n",
              "      <td>-0.051738</td>\n",
              "      <td>0.011170</td>\n",
              "      <td>-0.012154</td>\n",
              "      <td>-0.084336</td>\n",
              "      <td>0.054111</td>\n",
              "      <td>-0.143253</td>\n",
              "      <td>-0.028876</td>\n",
              "      <td>0.015378</td>\n",
              "      <td>0.012340</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.069784</td>\n",
              "      <td>0.047647</td>\n",
              "      <td>-0.162943</td>\n",
              "      <td>-0.053178</td>\n",
              "      <td>0.104861</td>\n",
              "      <td>0.008137</td>\n",
              "      <td>-0.253575</td>\n",
              "      <td>-0.041771</td>\n",
              "      <td>-0.019096</td>\n",
              "      <td>0.024908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>105</td>\n",
              "      <td>-0.051058</td>\n",
              "      <td>0.028734</td>\n",
              "      <td>-0.002164</td>\n",
              "      <td>-0.122716</td>\n",
              "      <td>0.081772</td>\n",
              "      <td>-0.161522</td>\n",
              "      <td>-0.033399</td>\n",
              "      <td>0.069389</td>\n",
              "      <td>-0.016274</td>\n",
              "      <td>...</td>\n",
              "      <td>0.070366</td>\n",
              "      <td>-0.196414</td>\n",
              "      <td>0.009736</td>\n",
              "      <td>0.119265</td>\n",
              "      <td>0.025462</td>\n",
              "      <td>0.260401</td>\n",
              "      <td>0.209111</td>\n",
              "      <td>-0.092553</td>\n",
              "      <td>0.108953</td>\n",
              "      <td>-0.142083</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>106 rows Ã— 863 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d7fab85-1e67-4180-a54a-8f2a076a16c5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1d7fab85-1e67-4180-a54a-8f2a076a16c5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1d7fab85-1e67-4180-a54a-8f2a076a16c5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e3c288f4-4418-4dcc-b04e-19994cf268e1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e3c288f4-4418-4dcc-b04e-19994cf268e1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e3c288f4-4418-4dcc-b04e-19994cf268e1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b5dbd97f-b736-4475-9930-16f813b63530\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('X_train_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b5dbd97f-b736-4475-9930-16f813b63530 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('X_train_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train_df"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.read_csv(\"/AnimalLabels.csv\")\n",
        "labels['majority_vote'] = labels.mode(axis=1, numeric_only=True).astype(int)\n",
        "type(labels['majority_vote'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "TYzDuHntMIdk",
        "outputId": "9f4dea9e-d976-412e-8ff8-c85c51bdb0ac"
      },
      "id": "TYzDuHntMIdk",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pandas.core.series.Series</b><br/>def __init__(data=None, index=None, dtype: Dtype | None=None, name=None, copy: bool | None=None, fastpath: bool | lib.NoDefault=lib.no_default) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pandas/core/series.py</a>One-dimensional ndarray with axis labels (including time series).\n",
              "\n",
              "Labels need not be unique but must be a hashable type. The object\n",
              "supports both integer- and label-based indexing and provides a host of\n",
              "methods for performing operations involving the index. Statistical\n",
              "methods from ndarray have been overridden to automatically exclude\n",
              "missing data (currently represented as NaN).\n",
              "\n",
              "Operations between Series (+, -, /, \\*, \\*\\*) align values based on their\n",
              "associated index values-- they need not be the same length. The result\n",
              "index will be the sorted union of the two indexes.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "data : array-like, Iterable, dict, or scalar value\n",
              "    Contains data stored in Series. If data is a dict, argument order is\n",
              "    maintained.\n",
              "index : array-like or Index (1d)\n",
              "    Values must be hashable and have the same length as `data`.\n",
              "    Non-unique index values are allowed. Will default to\n",
              "    RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n",
              "    and index is None, then the keys in the data are used as the index. If the\n",
              "    index is not None, the resulting Series is reindexed with the index values.\n",
              "dtype : str, numpy.dtype, or ExtensionDtype, optional\n",
              "    Data type for the output Series. If not specified, this will be\n",
              "    inferred from `data`.\n",
              "    See the :ref:`user guide &lt;basics.dtypes&gt;` for more usages.\n",
              "name : Hashable, default None\n",
              "    The name to give to the Series.\n",
              "copy : bool, default False\n",
              "    Copy input data. Only affects Series or 1d ndarray input. See examples.\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Please reference the :ref:`User Guide &lt;basics.series&gt;` for more information.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "Constructing Series from a dictionary with an Index specified\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3}\n",
              "&gt;&gt;&gt; ser = pd.Series(data=d, index=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;])\n",
              "&gt;&gt;&gt; ser\n",
              "a   1\n",
              "b   2\n",
              "c   3\n",
              "dtype: int64\n",
              "\n",
              "The keys of the dictionary match with the Index values, hence the Index\n",
              "values have no effect.\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3}\n",
              "&gt;&gt;&gt; ser = pd.Series(data=d, index=[&#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;])\n",
              "&gt;&gt;&gt; ser\n",
              "x   NaN\n",
              "y   NaN\n",
              "z   NaN\n",
              "dtype: float64\n",
              "\n",
              "Note that the Index is first build with the keys from the dictionary.\n",
              "After this the Series is reindexed with the given Index values, hence we\n",
              "get all NaN as a result.\n",
              "\n",
              "Constructing Series from a list with `copy=False`.\n",
              "\n",
              "&gt;&gt;&gt; r = [1, 2]\n",
              "&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n",
              "&gt;&gt;&gt; ser.iloc[0] = 999\n",
              "&gt;&gt;&gt; r\n",
              "[1, 2]\n",
              "&gt;&gt;&gt; ser\n",
              "0    999\n",
              "1      2\n",
              "dtype: int64\n",
              "\n",
              "Due to input data type the Series has a `copy` of\n",
              "the original data even though `copy=False`, so\n",
              "the data is unchanged.\n",
              "\n",
              "Constructing Series from a 1d ndarray with `copy=False`.\n",
              "\n",
              "&gt;&gt;&gt; r = np.array([1, 2])\n",
              "&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n",
              "&gt;&gt;&gt; ser.iloc[0] = 999\n",
              "&gt;&gt;&gt; r\n",
              "array([999,   2])\n",
              "&gt;&gt;&gt; ser\n",
              "0    999\n",
              "1      2\n",
              "dtype: int64\n",
              "\n",
              "Due to input data type the Series has a `view` on\n",
              "the original data, so\n",
              "the data is changed as well.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 263);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df['label']=labels['majority_vote']\n",
        "y=X_train_df['label']\n",
        "X=X_train_df"
      ],
      "metadata": {
        "id": "h9Ab9ZOPMIgh"
      },
      "id": "h9Ab9ZOPMIgh",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.drop(columns=['label'],axis=1,inplace=True)\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "v9h6Qt0DMIjb",
        "outputId": "c81e16c9-de6e-41b9-92fb-9bdac2c09ac4"
      },
      "id": "v9h6Qt0DMIjb",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     index         0         1         2         3         4         5  \\\n",
              "0        0 -0.048460  0.084757  0.003497 -0.087329  0.059048 -0.209606   \n",
              "1        1 -0.065723  0.025389 -0.025804 -0.130403  0.077852 -0.120231   \n",
              "2        2 -0.045842  0.040817 -0.028311 -0.119993  0.082768 -0.169268   \n",
              "3        3 -0.049971  0.032408 -0.005414 -0.096241  0.066642 -0.137509   \n",
              "4        4 -0.045510  0.020915 -0.008356 -0.120176  0.046261 -0.174338   \n",
              "..     ...       ...       ...       ...       ...       ...       ...   \n",
              "101    101 -0.060480  0.032145  0.005485 -0.084974  0.062038 -0.145933   \n",
              "102    102 -0.055155 -0.000772 -0.013739 -0.069411  0.043931 -0.236069   \n",
              "103    103 -0.040352 -0.013974 -0.021251 -0.069300  0.054273 -0.207470   \n",
              "104    104 -0.051738  0.011170 -0.012154 -0.084336  0.054111 -0.143253   \n",
              "105    105 -0.051058  0.028734 -0.002164 -0.122716  0.081772 -0.161522   \n",
              "\n",
              "            6         7         8  ...       852       853       854  \\\n",
              "0   -0.006506  0.048831  0.017141  ...  0.245732  0.019818 -0.246347   \n",
              "1   -0.001798  0.034838  0.010572  ... -0.439034 -0.363662  0.125037   \n",
              "2   -0.019755  0.051325  0.019362  ... -0.171100  0.180284  0.044292   \n",
              "3   -0.046769  0.021914  0.060602  ...  0.026375  0.007201 -0.165179   \n",
              "4   -0.013932  0.018657  0.020323  ... -0.145272  0.047353 -0.150540   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "101 -0.047886  0.023430 -0.000973  ...  0.227745  0.008060 -0.163510   \n",
              "102  0.007162  0.073971 -0.005430  ... -0.044075  0.357937  0.251036   \n",
              "103 -0.014627  0.039035  0.016669  ... -0.011740  0.419290  0.062435   \n",
              "104 -0.028876  0.015378  0.012340  ... -0.069784  0.047647 -0.162943   \n",
              "105 -0.033399  0.069389 -0.016274  ...  0.070366 -0.196414  0.009736   \n",
              "\n",
              "          855       856       857       858       859       860       861  \n",
              "0   -0.334189  0.001933  0.099690  0.005821  0.098615  0.322466 -0.026514  \n",
              "1   -0.009941  0.000198  0.598846  0.399056 -0.448141 -0.133951  0.163589  \n",
              "2    0.000196  0.003606 -0.028537  0.019995  0.014549 -0.207424 -0.134106  \n",
              "3   -0.136537  0.035306 -0.042689 -0.063572 -0.160413  0.073046 -0.025301  \n",
              "4   -0.009131 -0.106808 -0.137046 -0.175852  0.152502  0.045290 -0.086046  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "101  0.093020 -0.086098  0.027652  0.021020  0.002824 -0.078523 -0.032950  \n",
              "102 -0.005687  0.141986 -0.258837  0.109927 -0.115133 -0.061322 -0.033643  \n",
              "103  0.191918  0.126068  0.287123 -0.040200  0.194610 -0.116119  0.169397  \n",
              "104 -0.053178  0.104861  0.008137 -0.253575 -0.041771 -0.019096  0.024908  \n",
              "105  0.119265  0.025462  0.260401  0.209111 -0.092553  0.108953 -0.142083  \n",
              "\n",
              "[106 rows x 863 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9fe67835-3ae2-41f2-8829-5cb6231e935a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>852</th>\n",
              "      <th>853</th>\n",
              "      <th>854</th>\n",
              "      <th>855</th>\n",
              "      <th>856</th>\n",
              "      <th>857</th>\n",
              "      <th>858</th>\n",
              "      <th>859</th>\n",
              "      <th>860</th>\n",
              "      <th>861</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.048460</td>\n",
              "      <td>0.084757</td>\n",
              "      <td>0.003497</td>\n",
              "      <td>-0.087329</td>\n",
              "      <td>0.059048</td>\n",
              "      <td>-0.209606</td>\n",
              "      <td>-0.006506</td>\n",
              "      <td>0.048831</td>\n",
              "      <td>0.017141</td>\n",
              "      <td>...</td>\n",
              "      <td>0.245732</td>\n",
              "      <td>0.019818</td>\n",
              "      <td>-0.246347</td>\n",
              "      <td>-0.334189</td>\n",
              "      <td>0.001933</td>\n",
              "      <td>0.099690</td>\n",
              "      <td>0.005821</td>\n",
              "      <td>0.098615</td>\n",
              "      <td>0.322466</td>\n",
              "      <td>-0.026514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.065723</td>\n",
              "      <td>0.025389</td>\n",
              "      <td>-0.025804</td>\n",
              "      <td>-0.130403</td>\n",
              "      <td>0.077852</td>\n",
              "      <td>-0.120231</td>\n",
              "      <td>-0.001798</td>\n",
              "      <td>0.034838</td>\n",
              "      <td>0.010572</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.439034</td>\n",
              "      <td>-0.363662</td>\n",
              "      <td>0.125037</td>\n",
              "      <td>-0.009941</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.598846</td>\n",
              "      <td>0.399056</td>\n",
              "      <td>-0.448141</td>\n",
              "      <td>-0.133951</td>\n",
              "      <td>0.163589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.045842</td>\n",
              "      <td>0.040817</td>\n",
              "      <td>-0.028311</td>\n",
              "      <td>-0.119993</td>\n",
              "      <td>0.082768</td>\n",
              "      <td>-0.169268</td>\n",
              "      <td>-0.019755</td>\n",
              "      <td>0.051325</td>\n",
              "      <td>0.019362</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.171100</td>\n",
              "      <td>0.180284</td>\n",
              "      <td>0.044292</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.003606</td>\n",
              "      <td>-0.028537</td>\n",
              "      <td>0.019995</td>\n",
              "      <td>0.014549</td>\n",
              "      <td>-0.207424</td>\n",
              "      <td>-0.134106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-0.049971</td>\n",
              "      <td>0.032408</td>\n",
              "      <td>-0.005414</td>\n",
              "      <td>-0.096241</td>\n",
              "      <td>0.066642</td>\n",
              "      <td>-0.137509</td>\n",
              "      <td>-0.046769</td>\n",
              "      <td>0.021914</td>\n",
              "      <td>0.060602</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026375</td>\n",
              "      <td>0.007201</td>\n",
              "      <td>-0.165179</td>\n",
              "      <td>-0.136537</td>\n",
              "      <td>0.035306</td>\n",
              "      <td>-0.042689</td>\n",
              "      <td>-0.063572</td>\n",
              "      <td>-0.160413</td>\n",
              "      <td>0.073046</td>\n",
              "      <td>-0.025301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-0.045510</td>\n",
              "      <td>0.020915</td>\n",
              "      <td>-0.008356</td>\n",
              "      <td>-0.120176</td>\n",
              "      <td>0.046261</td>\n",
              "      <td>-0.174338</td>\n",
              "      <td>-0.013932</td>\n",
              "      <td>0.018657</td>\n",
              "      <td>0.020323</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.145272</td>\n",
              "      <td>0.047353</td>\n",
              "      <td>-0.150540</td>\n",
              "      <td>-0.009131</td>\n",
              "      <td>-0.106808</td>\n",
              "      <td>-0.137046</td>\n",
              "      <td>-0.175852</td>\n",
              "      <td>0.152502</td>\n",
              "      <td>0.045290</td>\n",
              "      <td>-0.086046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>101</td>\n",
              "      <td>-0.060480</td>\n",
              "      <td>0.032145</td>\n",
              "      <td>0.005485</td>\n",
              "      <td>-0.084974</td>\n",
              "      <td>0.062038</td>\n",
              "      <td>-0.145933</td>\n",
              "      <td>-0.047886</td>\n",
              "      <td>0.023430</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>...</td>\n",
              "      <td>0.227745</td>\n",
              "      <td>0.008060</td>\n",
              "      <td>-0.163510</td>\n",
              "      <td>0.093020</td>\n",
              "      <td>-0.086098</td>\n",
              "      <td>0.027652</td>\n",
              "      <td>0.021020</td>\n",
              "      <td>0.002824</td>\n",
              "      <td>-0.078523</td>\n",
              "      <td>-0.032950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>102</td>\n",
              "      <td>-0.055155</td>\n",
              "      <td>-0.000772</td>\n",
              "      <td>-0.013739</td>\n",
              "      <td>-0.069411</td>\n",
              "      <td>0.043931</td>\n",
              "      <td>-0.236069</td>\n",
              "      <td>0.007162</td>\n",
              "      <td>0.073971</td>\n",
              "      <td>-0.005430</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.044075</td>\n",
              "      <td>0.357937</td>\n",
              "      <td>0.251036</td>\n",
              "      <td>-0.005687</td>\n",
              "      <td>0.141986</td>\n",
              "      <td>-0.258837</td>\n",
              "      <td>0.109927</td>\n",
              "      <td>-0.115133</td>\n",
              "      <td>-0.061322</td>\n",
              "      <td>-0.033643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>103</td>\n",
              "      <td>-0.040352</td>\n",
              "      <td>-0.013974</td>\n",
              "      <td>-0.021251</td>\n",
              "      <td>-0.069300</td>\n",
              "      <td>0.054273</td>\n",
              "      <td>-0.207470</td>\n",
              "      <td>-0.014627</td>\n",
              "      <td>0.039035</td>\n",
              "      <td>0.016669</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.011740</td>\n",
              "      <td>0.419290</td>\n",
              "      <td>0.062435</td>\n",
              "      <td>0.191918</td>\n",
              "      <td>0.126068</td>\n",
              "      <td>0.287123</td>\n",
              "      <td>-0.040200</td>\n",
              "      <td>0.194610</td>\n",
              "      <td>-0.116119</td>\n",
              "      <td>0.169397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>104</td>\n",
              "      <td>-0.051738</td>\n",
              "      <td>0.011170</td>\n",
              "      <td>-0.012154</td>\n",
              "      <td>-0.084336</td>\n",
              "      <td>0.054111</td>\n",
              "      <td>-0.143253</td>\n",
              "      <td>-0.028876</td>\n",
              "      <td>0.015378</td>\n",
              "      <td>0.012340</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.069784</td>\n",
              "      <td>0.047647</td>\n",
              "      <td>-0.162943</td>\n",
              "      <td>-0.053178</td>\n",
              "      <td>0.104861</td>\n",
              "      <td>0.008137</td>\n",
              "      <td>-0.253575</td>\n",
              "      <td>-0.041771</td>\n",
              "      <td>-0.019096</td>\n",
              "      <td>0.024908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>105</td>\n",
              "      <td>-0.051058</td>\n",
              "      <td>0.028734</td>\n",
              "      <td>-0.002164</td>\n",
              "      <td>-0.122716</td>\n",
              "      <td>0.081772</td>\n",
              "      <td>-0.161522</td>\n",
              "      <td>-0.033399</td>\n",
              "      <td>0.069389</td>\n",
              "      <td>-0.016274</td>\n",
              "      <td>...</td>\n",
              "      <td>0.070366</td>\n",
              "      <td>-0.196414</td>\n",
              "      <td>0.009736</td>\n",
              "      <td>0.119265</td>\n",
              "      <td>0.025462</td>\n",
              "      <td>0.260401</td>\n",
              "      <td>0.209111</td>\n",
              "      <td>-0.092553</td>\n",
              "      <td>0.108953</td>\n",
              "      <td>-0.142083</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>106 rows Ã— 863 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9fe67835-3ae2-41f2-8829-5cb6231e935a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9fe67835-3ae2-41f2-8829-5cb6231e935a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9fe67835-3ae2-41f2-8829-5cb6231e935a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c29356d4-b819-4451-bf72-1d41667289ec\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c29356d4-b819-4451-bf72-1d41667289ec')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c29356d4-b819-4451-bf72-1d41667289ec button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_0d050de2-5139-455f-8afd-5124aa82b35b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('X_train_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_0d050de2-5139-455f-8afd-5124aa82b35b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('X_train_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train_df"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices_to_change = [4, 105, 84, 27, 98, 88, 18, 65, 9, 2, 5, 49, 99, 69, 86, 67, 7, 28, 78, 70, 18, 74]\n",
        "\n",
        "# Create a boolean mask to identify indices for the test set\n",
        "test_mask = np.zeros(len(X), dtype=bool)\n",
        "test_mask[indices_to_change] = True\n",
        "\n",
        "# Split the dataset based on the boolean mask\n",
        "X_train = X[~test_mask]\n",
        "X_test = X[test_mask]\n",
        "y_train = y[~test_mask]\n",
        "y_test = y[test_mask]"
      ],
      "metadata": {
        "id": "3r7-XtBwMImf"
      },
      "id": "3r7-XtBwMImf",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all column names are strings to avoid data type issues.\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "\n",
        "# Modify the param_grid to avoid penalty=None with 'liblinear'\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [0.1, 2, 2.5, 3],\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'max_iter': [500]\n",
        "}\n",
        "\n",
        "LR = LogisticRegression(random_state=0)\n",
        "grid_search_LR = GridSearchCV(estimator=LR, param_grid=param_grid, cv=3)\n",
        "\n",
        "grid_search_LR.fit(X_train, y_train)\n",
        "print(\"Best parameters found: \", grid_search_LR.best_params_)\n",
        "print(\"Best Score: {}\".format(grid_search_LR.best_score_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXiWtPk2MIpF",
        "outputId": "3e0555b8-79be-40de-d724-be6262d35ae9"
      },
      "id": "zXiWtPk2MIpF",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'C': 0.1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Score: 0.5767651888341544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'penalty':['elasticnet',None],'C': [0.1,2,2.5,3],\n",
        "                   'solver':['saga'],'max_iter':[500],'l1_ratio':[0.5,0.2,0.7]}\n",
        "LR=LogisticRegression(random_state=0)\n",
        "grid_search_LR = GridSearchCV(estimator=LR,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=2)\n",
        "grid_search_LR.fit(X_train, y_train)\n",
        "print(\"Best parameters found: \", grid_search_LR.best_params_)\n",
        "print(\"Best Score: {}\".format(grid_search_LR.best_score_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81_6AOwdMIr9",
        "outputId": "e89d39d0-13d5-4f82-cb78-8879465996f8"
      },
      "id": "81_6AOwdMIr9",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=None)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'C': 0.1, 'l1_ratio': 0.5, 'max_iter': 500, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
            "Best Score: 0.6121262458471761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'penalty':['l2',None],'C': [0.1,2,2.5,3],\n",
        "                   'solver':['lbfgs','newton-cg','sag'],'max_iter':[500]}\n",
        "LR=LogisticRegression(random_state=0)\n",
        "grid_search_LR = GridSearchCV(estimator=LR,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=3\n",
        "                          )\n",
        "\n",
        "grid_search_LR.fit(X_train, y_train)\n",
        "print(\"Best parameters found: \", grid_search_LR.best_params_)\n",
        "print(\"Best Score: {}\".format(grid_search_LR.best_score_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3byHwhKMIu4",
        "outputId": "47e40a9a-4474-4002-ef2b-5df393268c08"
      },
      "id": "O3byHwhKMIu4",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'C': 0.1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Best Score: 0.5537766830870279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'penalty':['l2',None],'C': [0.1,2,2.5,3],\n",
        "                   'solver':['lbfgs','newton-cg','sag'],'max_iter':[500]}\n",
        "LR=LogisticRegression(random_state=0)\n",
        "grid_search_LR = GridSearchCV(estimator=LR,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=3\n",
        "                          )\n",
        "\n",
        "grid_search_LR.fit(X_train, y_train)\n",
        "print(\"Best parameters found: \", grid_search_LR.best_params_)\n",
        "print(\"Best Score: {}\".format(grid_search_LR.best_score_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clkKx6fqMXZI",
        "outputId": "e1c952bc-8b2a-4f93-a326-db8c7e69991a"
      },
      "id": "clkKx6fqMXZI",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'C': 0.1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Best Score: 0.5537766830870279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LR=LogisticRegression(C=2.5,penalty='elasticnet',solver='saga',max_iter=1000,l1_ratio= 0.2)\n",
        "LR.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "3YKdfSzOMXck",
        "outputId": "435154e0-6b7a-44dc-921f-e1c80832548b"
      },
      "id": "3YKdfSzOMXck",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=2.5, l1_ratio=0.2, max_iter=1000, penalty='elasticnet',\n",
              "                   solver='saga')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"â–¸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"â–¾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=2.5, l1_ratio=0.2, max_iter=1000, penalty=&#x27;elasticnet&#x27;,\n",
              "                   solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=2.5, l1_ratio=0.2, max_iter=1000, penalty=&#x27;elasticnet&#x27;,\n",
              "                   solver=&#x27;saga&#x27;)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)\n",
        "\n",
        "LR_pred = LR.predict(X_train)\n",
        "acc_train=accuracy_score(y_train,LR_pred)\n",
        "print('train_accuracy',acc_train)\n",
        "LR_pred = LR.predict(X_test)\n",
        "acc=accuracy_score(y_test,LR_pred)\n",
        "print('test_accuracy',acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aygfsy3PMXfS",
        "outputId": "432f3f6f-d870-48e1-8018-88dff66eb0a7"
      },
      "id": "Aygfsy3PMXfS",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_accuracy 0.9647058823529412\n",
            "test_accuracy 0.6190476190476191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_values = X_train.values\n",
        "X_test_values = X_test.values\n",
        "#X_valid_values = X_valid.values\n",
        "\n",
        "# Reshape input data to 3D [samples, timesteps, features] so it'll fit the LSTM layer\n",
        "# Use a timestep of 1.\n",
        "X_train_reshaped = np.reshape(X_train_values, (X_train_values.shape[0], 1, X_train_values.shape[1]))\n",
        "X_test_reshaped = np.reshape(X_test_values, (X_test_values.shape[0], 1, X_test_values.shape[1]))\n",
        "#X_valid_reshaped = np.reshape(X_valid_values, (X_valid_values.shape[0], 1, X_valid_values.shape[1]))\n",
        "\n",
        "# LSTM model\n",
        "lstm = Sequential()\n",
        "lstm.add(LSTM(100,return_sequences=True,activation='selu',input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
        "lstm.add(Dropout(0.7))\n",
        "\n",
        "lstm.add(LSTM(50,activation='selu',return_sequences=False,kernel_regularizer=l2(0.01)))\n",
        "lstm.add(Dropout(0.7))\n",
        "lstm.add(Dense(1,activation='sigmoid'))  # Prediction of the next closing value\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "lstm.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "lstm.fit(X_train_reshaped, y_train, epochs=1000, batch_size=32, verbose=1)\n",
        "\n",
        "# Predicting and inverse transforming the predictions\n",
        "y_pred_train_lstm = lstm.predict(X_train_reshaped)\n",
        "y_pred_test_lstm = lstm.predict(X_test_reshaped)\n",
        "\n",
        "#y_pred = scaler.inverse_transform(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpoUmgRxMXiW",
        "outputId": "01c5caa9-bc67-4284-9246-6e786f999311"
      },
      "id": "SpoUmgRxMXiW",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 2.1248\n",
            "Epoch 2/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.0006 \n",
            "Epoch 3/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.9278\n",
            "Epoch 4/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.8928\n",
            "Epoch 5/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.7990\n",
            "Epoch 6/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.7001\n",
            "Epoch 7/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.6359\n",
            "Epoch 8/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.5926\n",
            "Epoch 9/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.5599\n",
            "Epoch 10/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.5221 \n",
            "Epoch 11/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.4523\n",
            "Epoch 12/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4274 \n",
            "Epoch 13/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.3750\n",
            "Epoch 14/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.3070\n",
            "Epoch 15/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.3425\n",
            "Epoch 16/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.2669\n",
            "Epoch 17/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.2274\n",
            "Epoch 18/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.2046\n",
            "Epoch 19/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.1137\n",
            "Epoch 20/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.1866\n",
            "Epoch 21/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.1288\n",
            "Epoch 22/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.1411\n",
            "Epoch 23/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.0696\n",
            "Epoch 24/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.0735\n",
            "Epoch 25/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.0234 \n",
            "Epoch 26/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.0043\n",
            "Epoch 27/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.9829\n",
            "Epoch 28/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.9538\n",
            "Epoch 29/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.9221\n",
            "Epoch 30/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.9113\n",
            "Epoch 31/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.8903\n",
            "Epoch 32/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.9149\n",
            "Epoch 33/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.8857\n",
            "Epoch 34/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.8509\n",
            "Epoch 35/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.8435\n",
            "Epoch 36/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7894\n",
            "Epoch 37/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7882\n",
            "Epoch 38/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7463\n",
            "Epoch 39/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.8100 \n",
            "Epoch 40/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.7228\n",
            "Epoch 41/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6863\n",
            "Epoch 42/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6829\n",
            "Epoch 43/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6624\n",
            "Epoch 44/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6144\n",
            "Epoch 45/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6461\n",
            "Epoch 46/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6167\n",
            "Epoch 47/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5708\n",
            "Epoch 48/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5174\n",
            "Epoch 49/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6369\n",
            "Epoch 50/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5159\n",
            "Epoch 51/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.5061\n",
            "Epoch 52/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5109\n",
            "Epoch 53/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4571 \n",
            "Epoch 54/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4516\n",
            "Epoch 55/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4138 \n",
            "Epoch 56/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4806\n",
            "Epoch 57/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4021 \n",
            "Epoch 58/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3930\n",
            "Epoch 59/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3862\n",
            "Epoch 60/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4204\n",
            "Epoch 61/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3618\n",
            "Epoch 62/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3338\n",
            "Epoch 63/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3562\n",
            "Epoch 64/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3395\n",
            "Epoch 65/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3329\n",
            "Epoch 66/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2890 \n",
            "Epoch 67/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3766 \n",
            "Epoch 68/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2965\n",
            "Epoch 69/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3035\n",
            "Epoch 70/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2676\n",
            "Epoch 71/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2559\n",
            "Epoch 72/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2583\n",
            "Epoch 73/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2664\n",
            "Epoch 74/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2475 \n",
            "Epoch 75/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2671\n",
            "Epoch 76/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2383\n",
            "Epoch 77/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2209\n",
            "Epoch 78/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1825\n",
            "Epoch 79/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2144\n",
            "Epoch 80/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1749 \n",
            "Epoch 81/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2028\n",
            "Epoch 82/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2231\n",
            "Epoch 83/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1711\n",
            "Epoch 84/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2058\n",
            "Epoch 85/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2120\n",
            "Epoch 86/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2146\n",
            "Epoch 87/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1678\n",
            "Epoch 88/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1931\n",
            "Epoch 89/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1546\n",
            "Epoch 90/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1915\n",
            "Epoch 91/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1748\n",
            "Epoch 92/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1870 \n",
            "Epoch 93/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1471\n",
            "Epoch 94/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1684\n",
            "Epoch 95/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1811 \n",
            "Epoch 96/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1462\n",
            "Epoch 97/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1434\n",
            "Epoch 98/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1500\n",
            "Epoch 99/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1419\n",
            "Epoch 100/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1531\n",
            "Epoch 101/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1658\n",
            "Epoch 102/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1272\n",
            "Epoch 103/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1281\n",
            "Epoch 104/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1258 \n",
            "Epoch 105/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1329\n",
            "Epoch 106/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1321 \n",
            "Epoch 107/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1520\n",
            "Epoch 108/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1252\n",
            "Epoch 109/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1261\n",
            "Epoch 110/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2453\n",
            "Epoch 111/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1229\n",
            "Epoch 112/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1285\n",
            "Epoch 113/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1244\n",
            "Epoch 114/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1123\n",
            "Epoch 115/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1296 \n",
            "Epoch 116/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1104\n",
            "Epoch 117/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1180\n",
            "Epoch 118/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1267\n",
            "Epoch 119/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1126 \n",
            "Epoch 120/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1148\n",
            "Epoch 121/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1465\n",
            "Epoch 122/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1221 \n",
            "Epoch 123/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1002\n",
            "Epoch 124/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1064 \n",
            "Epoch 125/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1186\n",
            "Epoch 126/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1134 \n",
            "Epoch 127/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1042\n",
            "Epoch 128/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0963\n",
            "Epoch 129/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1035 \n",
            "Epoch 130/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1000\n",
            "Epoch 131/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0954 \n",
            "Epoch 132/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0888\n",
            "Epoch 133/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0987\n",
            "Epoch 134/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0943\n",
            "Epoch 135/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0876 \n",
            "Epoch 136/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0932 \n",
            "Epoch 137/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0854\n",
            "Epoch 138/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0923\n",
            "Epoch 139/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0890\n",
            "Epoch 140/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0845 \n",
            "Epoch 141/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0943\n",
            "Epoch 142/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1091\n",
            "Epoch 143/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0781\n",
            "Epoch 144/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0889\n",
            "Epoch 145/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0769\n",
            "Epoch 146/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0890\n",
            "Epoch 147/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1192 \n",
            "Epoch 148/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0755\n",
            "Epoch 149/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0748\n",
            "Epoch 150/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0792\n",
            "Epoch 151/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0945\n",
            "Epoch 152/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0877\n",
            "Epoch 153/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0718\n",
            "Epoch 154/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0824\n",
            "Epoch 155/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0806\n",
            "Epoch 156/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0714 \n",
            "Epoch 157/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0817\n",
            "Epoch 158/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0666\n",
            "Epoch 159/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0621\n",
            "Epoch 160/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0775\n",
            "Epoch 161/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0748\n",
            "Epoch 162/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0758\n",
            "Epoch 163/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0721\n",
            "Epoch 164/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0631\n",
            "Epoch 165/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0727\n",
            "Epoch 166/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0722 \n",
            "Epoch 167/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0657\n",
            "Epoch 168/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0715 \n",
            "Epoch 169/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0726\n",
            "Epoch 170/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0772\n",
            "Epoch 171/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0790\n",
            "Epoch 172/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0670 \n",
            "Epoch 173/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0707\n",
            "Epoch 174/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0625\n",
            "Epoch 175/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0687\n",
            "Epoch 176/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0583\n",
            "Epoch 177/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0843 \n",
            "Epoch 178/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0749\n",
            "Epoch 179/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0592\n",
            "Epoch 180/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0575 \n",
            "Epoch 181/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0615\n",
            "Epoch 182/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0699\n",
            "Epoch 183/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0570\n",
            "Epoch 184/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0714\n",
            "Epoch 185/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0532\n",
            "Epoch 186/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0538\n",
            "Epoch 187/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0612\n",
            "Epoch 188/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0536\n",
            "Epoch 189/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0488\n",
            "Epoch 190/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0516 \n",
            "Epoch 191/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0630\n",
            "Epoch 192/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0530\n",
            "Epoch 193/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0651\n",
            "Epoch 194/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0646 \n",
            "Epoch 195/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0591\n",
            "Epoch 196/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0555\n",
            "Epoch 197/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0734\n",
            "Epoch 198/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0503\n",
            "Epoch 199/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1203 \n",
            "Epoch 200/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0533 \n",
            "Epoch 201/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0585\n",
            "Epoch 202/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0666 \n",
            "Epoch 203/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0525\n",
            "Epoch 204/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0580 \n",
            "Epoch 205/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0528\n",
            "Epoch 206/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0541\n",
            "Epoch 207/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0686\n",
            "Epoch 208/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0598 \n",
            "Epoch 209/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0536\n",
            "Epoch 210/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0519 \n",
            "Epoch 211/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0592\n",
            "Epoch 212/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0535 \n",
            "Epoch 213/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0670\n",
            "Epoch 214/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0630\n",
            "Epoch 215/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0834\n",
            "Epoch 216/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0660\n",
            "Epoch 217/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0483 \n",
            "Epoch 218/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0520\n",
            "Epoch 219/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0487\n",
            "Epoch 220/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0476\n",
            "Epoch 221/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0473\n",
            "Epoch 222/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0566\n",
            "Epoch 223/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0458\n",
            "Epoch 224/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0445\n",
            "Epoch 225/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0503\n",
            "Epoch 226/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0568\n",
            "Epoch 227/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0706\n",
            "Epoch 228/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0501\n",
            "Epoch 229/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0504\n",
            "Epoch 230/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0542 \n",
            "Epoch 231/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0619\n",
            "Epoch 232/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0471 \n",
            "Epoch 233/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0579\n",
            "Epoch 234/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0503\n",
            "Epoch 235/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0492 \n",
            "Epoch 236/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0494\n",
            "Epoch 237/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0471\n",
            "Epoch 238/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0473\n",
            "Epoch 239/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0511\n",
            "Epoch 240/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0489\n",
            "Epoch 241/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0447\n",
            "Epoch 242/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0457\n",
            "Epoch 243/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0413\n",
            "Epoch 244/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0466\n",
            "Epoch 245/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0441 \n",
            "Epoch 246/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0475\n",
            "Epoch 247/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0422 \n",
            "Epoch 248/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0384\n",
            "Epoch 249/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0494 \n",
            "Epoch 250/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0430\n",
            "Epoch 251/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0407\n",
            "Epoch 252/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0434\n",
            "Epoch 253/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0498\n",
            "Epoch 254/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0416\n",
            "Epoch 255/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0423 \n",
            "Epoch 256/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0366\n",
            "Epoch 257/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0429 \n",
            "Epoch 258/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0348\n",
            "Epoch 259/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0343 \n",
            "Epoch 260/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0447\n",
            "Epoch 261/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0388\n",
            "Epoch 262/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0507\n",
            "Epoch 263/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0370\n",
            "Epoch 264/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0371 \n",
            "Epoch 265/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0363\n",
            "Epoch 266/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0376\n",
            "Epoch 267/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0348\n",
            "Epoch 268/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0348 \n",
            "Epoch 269/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0353 \n",
            "Epoch 270/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0327 \n",
            "Epoch 271/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0427 \n",
            "Epoch 272/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0315\n",
            "Epoch 273/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0329 \n",
            "Epoch 274/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0319\n",
            "Epoch 275/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0427\n",
            "Epoch 276/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0352 \n",
            "Epoch 277/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0376\n",
            "Epoch 278/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0340\n",
            "Epoch 279/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0300 \n",
            "Epoch 280/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0309\n",
            "Epoch 281/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0299 \n",
            "Epoch 282/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0325\n",
            "Epoch 283/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0375\n",
            "Epoch 284/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0324\n",
            "Epoch 285/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0303\n",
            "Epoch 286/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0350\n",
            "Epoch 287/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0383\n",
            "Epoch 288/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0326 \n",
            "Epoch 289/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0310 \n",
            "Epoch 290/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0283\n",
            "Epoch 291/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0296\n",
            "Epoch 292/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0329\n",
            "Epoch 293/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0316\n",
            "Epoch 294/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0273\n",
            "Epoch 295/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0283\n",
            "Epoch 296/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0699\n",
            "Epoch 297/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0286 \n",
            "Epoch 298/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0300 \n",
            "Epoch 299/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0304 \n",
            "Epoch 300/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0285 \n",
            "Epoch 301/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0410\n",
            "Epoch 302/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0290\n",
            "Epoch 303/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0317\n",
            "Epoch 304/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0312 \n",
            "Epoch 305/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0410\n",
            "Epoch 306/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0429\n",
            "Epoch 307/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0281\n",
            "Epoch 308/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0291\n",
            "Epoch 309/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0371\n",
            "Epoch 310/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0340 \n",
            "Epoch 311/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0381\n",
            "Epoch 312/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0311\n",
            "Epoch 313/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0284\n",
            "Epoch 314/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0293\n",
            "Epoch 315/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0311\n",
            "Epoch 316/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0292\n",
            "Epoch 317/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0278\n",
            "Epoch 318/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0297 \n",
            "Epoch 319/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0332\n",
            "Epoch 320/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0275 \n",
            "Epoch 321/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0256\n",
            "Epoch 322/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0321 \n",
            "Epoch 323/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0292\n",
            "Epoch 324/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0258\n",
            "Epoch 325/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0260\n",
            "Epoch 326/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0307\n",
            "Epoch 327/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0267\n",
            "Epoch 328/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0270 \n",
            "Epoch 329/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0433\n",
            "Epoch 330/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0245\n",
            "Epoch 331/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0280\n",
            "Epoch 332/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0265 \n",
            "Epoch 333/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0266 \n",
            "Epoch 334/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0336 \n",
            "Epoch 335/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0266 \n",
            "Epoch 336/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0285\n",
            "Epoch 337/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0232\n",
            "Epoch 338/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0277 \n",
            "Epoch 339/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0262\n",
            "Epoch 340/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0333\n",
            "Epoch 341/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0244 \n",
            "Epoch 342/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0294\n",
            "Epoch 343/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0256 \n",
            "Epoch 344/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0236\n",
            "Epoch 345/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0259\n",
            "Epoch 346/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0321 \n",
            "Epoch 347/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0250\n",
            "Epoch 348/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0233\n",
            "Epoch 349/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0288\n",
            "Epoch 350/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0234 \n",
            "Epoch 351/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0242 \n",
            "Epoch 352/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0269\n",
            "Epoch 353/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0224\n",
            "Epoch 354/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0305\n",
            "Epoch 355/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0234 \n",
            "Epoch 356/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0276 \n",
            "Epoch 357/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0244 \n",
            "Epoch 358/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0230\n",
            "Epoch 359/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0269 \n",
            "Epoch 360/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0271 \n",
            "Epoch 361/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0218\n",
            "Epoch 362/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0229 \n",
            "Epoch 363/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0201\n",
            "Epoch 364/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0227 \n",
            "Epoch 365/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0203\n",
            "Epoch 366/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0281 \n",
            "Epoch 367/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0473\n",
            "Epoch 368/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0202\n",
            "Epoch 369/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0245 \n",
            "Epoch 370/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0235\n",
            "Epoch 371/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0231 \n",
            "Epoch 372/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0243 \n",
            "Epoch 373/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0209\n",
            "Epoch 374/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0230\n",
            "Epoch 375/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0299 \n",
            "Epoch 376/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0237\n",
            "Epoch 377/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0246 \n",
            "Epoch 378/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0203\n",
            "Epoch 379/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0235\n",
            "Epoch 380/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0229\n",
            "Epoch 381/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0230\n",
            "Epoch 382/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0246\n",
            "Epoch 383/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0225\n",
            "Epoch 384/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0205 \n",
            "Epoch 385/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0261\n",
            "Epoch 386/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0203\n",
            "Epoch 387/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0250 \n",
            "Epoch 388/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0198 \n",
            "Epoch 389/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0320\n",
            "Epoch 390/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0231\n",
            "Epoch 391/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0223\n",
            "Epoch 392/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0205 \n",
            "Epoch 393/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0195\n",
            "Epoch 394/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0196\n",
            "Epoch 395/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0187 \n",
            "Epoch 396/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0178\n",
            "Epoch 397/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0327 \n",
            "Epoch 398/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0199\n",
            "Epoch 399/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0225\n",
            "Epoch 400/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0223 \n",
            "Epoch 401/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0419\n",
            "Epoch 402/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0217\n",
            "Epoch 403/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0224 \n",
            "Epoch 404/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0263\n",
            "Epoch 405/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0204 \n",
            "Epoch 406/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0213\n",
            "Epoch 407/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0219\n",
            "Epoch 408/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0265 \n",
            "Epoch 409/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0239\n",
            "Epoch 410/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0200 \n",
            "Epoch 411/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0248\n",
            "Epoch 412/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0198\n",
            "Epoch 413/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0246 \n",
            "Epoch 414/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0211\n",
            "Epoch 415/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0209\n",
            "Epoch 416/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0253 \n",
            "Epoch 417/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0193\n",
            "Epoch 418/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0252\n",
            "Epoch 419/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0202\n",
            "Epoch 420/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0186\n",
            "Epoch 421/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0204\n",
            "Epoch 422/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0260 \n",
            "Epoch 423/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0215\n",
            "Epoch 424/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0195\n",
            "Epoch 425/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0205 \n",
            "Epoch 426/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0201 \n",
            "Epoch 427/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0220\n",
            "Epoch 428/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0223\n",
            "Epoch 429/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0174\n",
            "Epoch 430/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0176\n",
            "Epoch 431/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0212\n",
            "Epoch 432/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0192\n",
            "Epoch 433/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0245\n",
            "Epoch 434/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0166 \n",
            "Epoch 435/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0202\n",
            "Epoch 436/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0181 \n",
            "Epoch 437/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0259 \n",
            "Epoch 438/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0200\n",
            "Epoch 439/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0238\n",
            "Epoch 440/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0208 \n",
            "Epoch 441/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0206\n",
            "Epoch 442/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0180 \n",
            "Epoch 443/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0231\n",
            "Epoch 444/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0232\n",
            "Epoch 445/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0186\n",
            "Epoch 446/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0210 \n",
            "Epoch 447/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0291\n",
            "Epoch 448/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0183\n",
            "Epoch 449/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0199\n",
            "Epoch 450/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0232 \n",
            "Epoch 451/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0183\n",
            "Epoch 452/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0183 \n",
            "Epoch 453/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0186\n",
            "Epoch 454/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0228\n",
            "Epoch 455/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0240 \n",
            "Epoch 456/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0189\n",
            "Epoch 457/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0162\n",
            "Epoch 458/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0232\n",
            "Epoch 459/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0221 \n",
            "Epoch 460/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0216\n",
            "Epoch 461/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0174\n",
            "Epoch 462/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0172 \n",
            "Epoch 463/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0192 \n",
            "Epoch 464/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0174\n",
            "Epoch 465/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0183\n",
            "Epoch 466/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0197 \n",
            "Epoch 467/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0176\n",
            "Epoch 468/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0176\n",
            "Epoch 469/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0176\n",
            "Epoch 470/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0185\n",
            "Epoch 471/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0180\n",
            "Epoch 472/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0223\n",
            "Epoch 473/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0186 \n",
            "Epoch 474/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0206\n",
            "Epoch 475/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0196 \n",
            "Epoch 476/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0204\n",
            "Epoch 477/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0169\n",
            "Epoch 478/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0183 \n",
            "Epoch 479/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0329\n",
            "Epoch 480/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0176 \n",
            "Epoch 481/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0173 \n",
            "Epoch 482/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0236\n",
            "Epoch 483/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0205\n",
            "Epoch 484/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0176\n",
            "Epoch 485/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0194\n",
            "Epoch 486/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0198\n",
            "Epoch 487/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0150\n",
            "Epoch 488/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0162\n",
            "Epoch 489/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0164\n",
            "Epoch 490/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0252\n",
            "Epoch 491/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0199\n",
            "Epoch 492/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0170\n",
            "Epoch 493/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0224\n",
            "Epoch 494/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0159 \n",
            "Epoch 495/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0182\n",
            "Epoch 496/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0174\n",
            "Epoch 497/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0170\n",
            "Epoch 498/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0166 \n",
            "Epoch 499/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0141\n",
            "Epoch 500/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0198\n",
            "Epoch 501/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0194 \n",
            "Epoch 502/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0245\n",
            "Epoch 503/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0200 \n",
            "Epoch 504/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0156\n",
            "Epoch 505/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0171\n",
            "Epoch 506/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0220 \n",
            "Epoch 507/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0219\n",
            "Epoch 508/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0201\n",
            "Epoch 509/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0146\n",
            "Epoch 510/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0168 \n",
            "Epoch 511/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0186\n",
            "Epoch 512/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0218\n",
            "Epoch 513/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0158\n",
            "Epoch 514/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0162\n",
            "Epoch 515/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0158\n",
            "Epoch 516/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0152\n",
            "Epoch 517/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0157 \n",
            "Epoch 518/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0161\n",
            "Epoch 519/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0178\n",
            "Epoch 520/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0209\n",
            "Epoch 521/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0165 \n",
            "Epoch 522/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0197 \n",
            "Epoch 523/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0156\n",
            "Epoch 524/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0168\n",
            "Epoch 525/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0151\n",
            "Epoch 526/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0165 \n",
            "Epoch 527/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0164 \n",
            "Epoch 528/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0158 \n",
            "Epoch 529/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0174\n",
            "Epoch 530/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0164\n",
            "Epoch 531/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0176 \n",
            "Epoch 532/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0222 \n",
            "Epoch 533/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0153 \n",
            "Epoch 534/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0174 \n",
            "Epoch 535/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0188\n",
            "Epoch 536/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0163 \n",
            "Epoch 537/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0274 \n",
            "Epoch 538/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0154\n",
            "Epoch 539/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0153\n",
            "Epoch 540/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0170\n",
            "Epoch 541/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0164 \n",
            "Epoch 542/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0140\n",
            "Epoch 543/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0174\n",
            "Epoch 544/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0142\n",
            "Epoch 545/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0159\n",
            "Epoch 546/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0123 \n",
            "Epoch 547/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0171 \n",
            "Epoch 548/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0185\n",
            "Epoch 549/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0126\n",
            "Epoch 550/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0129\n",
            "Epoch 551/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0164 \n",
            "Epoch 552/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0149\n",
            "Epoch 553/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0149 \n",
            "Epoch 554/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0156\n",
            "Epoch 555/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0169 \n",
            "Epoch 556/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0138\n",
            "Epoch 557/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0160\n",
            "Epoch 558/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0138\n",
            "Epoch 559/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0167\n",
            "Epoch 560/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0136 \n",
            "Epoch 561/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0160\n",
            "Epoch 562/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0147\n",
            "Epoch 563/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0126 \n",
            "Epoch 564/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0152 \n",
            "Epoch 565/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0152\n",
            "Epoch 566/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0215 \n",
            "Epoch 567/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0167\n",
            "Epoch 568/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0146\n",
            "Epoch 569/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0149 \n",
            "Epoch 570/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0160\n",
            "Epoch 571/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0217 \n",
            "Epoch 572/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0134\n",
            "Epoch 573/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0148\n",
            "Epoch 574/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0143\n",
            "Epoch 575/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0272\n",
            "Epoch 576/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0156\n",
            "Epoch 577/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0191 \n",
            "Epoch 578/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0188\n",
            "Epoch 579/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0140 \n",
            "Epoch 580/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0294\n",
            "Epoch 581/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0189\n",
            "Epoch 582/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0186\n",
            "Epoch 583/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0155\n",
            "Epoch 584/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0174\n",
            "Epoch 585/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0205\n",
            "Epoch 586/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0175\n",
            "Epoch 587/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0177\n",
            "Epoch 588/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0172 \n",
            "Epoch 589/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0175\n",
            "Epoch 590/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0154\n",
            "Epoch 591/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0173\n",
            "Epoch 592/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0146 \n",
            "Epoch 593/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0180\n",
            "Epoch 594/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0162\n",
            "Epoch 595/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0172\n",
            "Epoch 596/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0157\n",
            "Epoch 597/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0169\n",
            "Epoch 598/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0183\n",
            "Epoch 599/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0169 \n",
            "Epoch 600/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0140\n",
            "Epoch 601/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0188\n",
            "Epoch 602/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0141\n",
            "Epoch 603/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0140\n",
            "Epoch 604/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0142 \n",
            "Epoch 605/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0145\n",
            "Epoch 606/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0131\n",
            "Epoch 607/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0140 \n",
            "Epoch 608/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0138\n",
            "Epoch 609/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0140 \n",
            "Epoch 610/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0143\n",
            "Epoch 611/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0254\n",
            "Epoch 612/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0182\n",
            "Epoch 613/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0156 \n",
            "Epoch 614/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0286 \n",
            "Epoch 615/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0157 \n",
            "Epoch 616/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0507 \n",
            "Epoch 617/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0222 \n",
            "Epoch 618/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0181 \n",
            "Epoch 619/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0320 \n",
            "Epoch 620/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0172\n",
            "Epoch 621/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0195 \n",
            "Epoch 622/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0296 \n",
            "Epoch 623/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0281\n",
            "Epoch 624/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0180 \n",
            "Epoch 625/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0186 \n",
            "Epoch 626/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0189\n",
            "Epoch 627/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0216 \n",
            "Epoch 628/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0171\n",
            "Epoch 629/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0173\n",
            "Epoch 630/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0171\n",
            "Epoch 631/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0164\n",
            "Epoch 632/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0167\n",
            "Epoch 633/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0174\n",
            "Epoch 634/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0188 \n",
            "Epoch 635/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0166\n",
            "Epoch 636/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0181 \n",
            "Epoch 637/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0143\n",
            "Epoch 638/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0230\n",
            "Epoch 639/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0146\n",
            "Epoch 640/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0136 \n",
            "Epoch 641/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0157 \n",
            "Epoch 642/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0142\n",
            "Epoch 643/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0164 \n",
            "Epoch 644/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0155\n",
            "Epoch 645/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0198\n",
            "Epoch 646/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0198 \n",
            "Epoch 647/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0148 \n",
            "Epoch 648/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0133 \n",
            "Epoch 649/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0138\n",
            "Epoch 650/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0132\n",
            "Epoch 651/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0145\n",
            "Epoch 652/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0130\n",
            "Epoch 653/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0119\n",
            "Epoch 654/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0279\n",
            "Epoch 655/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0148\n",
            "Epoch 656/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0124 \n",
            "Epoch 657/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0136\n",
            "Epoch 658/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0160\n",
            "Epoch 659/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0153 \n",
            "Epoch 660/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0125\n",
            "Epoch 661/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0151\n",
            "Epoch 662/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0153 \n",
            "Epoch 663/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0129\n",
            "Epoch 664/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0151\n",
            "Epoch 665/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0146\n",
            "Epoch 666/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0119\n",
            "Epoch 667/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0128\n",
            "Epoch 668/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0148 \n",
            "Epoch 669/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0115\n",
            "Epoch 670/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0127 \n",
            "Epoch 671/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0122 \n",
            "Epoch 672/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0124\n",
            "Epoch 673/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0124\n",
            "Epoch 674/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0143\n",
            "Epoch 675/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0225\n",
            "Epoch 676/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0164\n",
            "Epoch 677/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0120\n",
            "Epoch 678/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0224\n",
            "Epoch 679/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0152 \n",
            "Epoch 680/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0187 \n",
            "Epoch 681/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0140\n",
            "Epoch 682/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0161\n",
            "Epoch 683/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0185\n",
            "Epoch 684/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0151 \n",
            "Epoch 685/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0156\n",
            "Epoch 686/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0141 \n",
            "Epoch 687/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0133\n",
            "Epoch 688/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0154 \n",
            "Epoch 689/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0134\n",
            "Epoch 690/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0136 \n",
            "Epoch 691/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0172 \n",
            "Epoch 692/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0132 \n",
            "Epoch 693/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0129\n",
            "Epoch 694/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0252\n",
            "Epoch 695/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0144 \n",
            "Epoch 696/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0162\n",
            "Epoch 697/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0140 \n",
            "Epoch 698/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0135\n",
            "Epoch 699/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0215 \n",
            "Epoch 700/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0140\n",
            "Epoch 701/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0133\n",
            "Epoch 702/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0122\n",
            "Epoch 703/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0118\n",
            "Epoch 704/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0140 \n",
            "Epoch 705/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0123\n",
            "Epoch 706/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0165\n",
            "Epoch 707/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0146 \n",
            "Epoch 708/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0131 \n",
            "Epoch 709/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0146\n",
            "Epoch 710/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0138 \n",
            "Epoch 711/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0140\n",
            "Epoch 712/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0112 \n",
            "Epoch 713/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0139 \n",
            "Epoch 714/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0117 \n",
            "Epoch 715/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0120\n",
            "Epoch 716/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0128\n",
            "Epoch 717/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0187\n",
            "Epoch 718/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0112 \n",
            "Epoch 719/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0139\n",
            "Epoch 720/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0159\n",
            "Epoch 721/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0121\n",
            "Epoch 722/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0130\n",
            "Epoch 723/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0173 \n",
            "Epoch 724/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0114\n",
            "Epoch 725/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0145 \n",
            "Epoch 726/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0124\n",
            "Epoch 727/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0147 \n",
            "Epoch 728/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0152 \n",
            "Epoch 729/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0139\n",
            "Epoch 730/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0122\n",
            "Epoch 731/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0127 \n",
            "Epoch 732/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0142 \n",
            "Epoch 733/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0126 \n",
            "Epoch 734/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0111\n",
            "Epoch 735/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0129 \n",
            "Epoch 736/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0140\n",
            "Epoch 737/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0111\n",
            "Epoch 738/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0142\n",
            "Epoch 739/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0118\n",
            "Epoch 740/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0134\n",
            "Epoch 741/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0105 \n",
            "Epoch 742/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0111\n",
            "Epoch 743/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0117\n",
            "Epoch 744/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0109 \n",
            "Epoch 745/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0112 \n",
            "Epoch 746/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0109 \n",
            "Epoch 747/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0134\n",
            "Epoch 748/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0116 \n",
            "Epoch 749/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0125 \n",
            "Epoch 750/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0102\n",
            "Epoch 751/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0146\n",
            "Epoch 752/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0130\n",
            "Epoch 753/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0094\n",
            "Epoch 754/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0106\n",
            "Epoch 755/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0111\n",
            "Epoch 756/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0102\n",
            "Epoch 757/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0126 \n",
            "Epoch 758/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0101\n",
            "Epoch 759/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0089\n",
            "Epoch 760/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0103\n",
            "Epoch 761/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0091\n",
            "Epoch 762/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0107 \n",
            "Epoch 763/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0094\n",
            "Epoch 764/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0098\n",
            "Epoch 765/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0099\n",
            "Epoch 766/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0100\n",
            "Epoch 767/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0134\n",
            "Epoch 768/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0108 \n",
            "Epoch 769/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0142 \n",
            "Epoch 770/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0109 \n",
            "Epoch 771/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0112 \n",
            "Epoch 772/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0097\n",
            "Epoch 773/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0148 \n",
            "Epoch 774/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0097\n",
            "Epoch 775/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0116\n",
            "Epoch 776/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0109\n",
            "Epoch 777/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0102\n",
            "Epoch 778/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0099\n",
            "Epoch 779/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0093\n",
            "Epoch 780/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0120\n",
            "Epoch 781/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0106\n",
            "Epoch 782/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0113\n",
            "Epoch 783/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0107\n",
            "Epoch 784/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0092\n",
            "Epoch 785/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0098\n",
            "Epoch 786/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0117\n",
            "Epoch 787/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0096 \n",
            "Epoch 788/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0093\n",
            "Epoch 789/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0108 \n",
            "Epoch 790/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0091\n",
            "Epoch 791/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0106\n",
            "Epoch 792/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0108\n",
            "Epoch 793/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0085\n",
            "Epoch 794/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0106\n",
            "Epoch 795/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0099\n",
            "Epoch 796/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0088\n",
            "Epoch 797/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0106\n",
            "Epoch 798/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0100\n",
            "Epoch 799/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0088\n",
            "Epoch 800/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0083\n",
            "Epoch 801/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0088\n",
            "Epoch 802/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0089\n",
            "Epoch 803/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0080\n",
            "Epoch 804/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0091\n",
            "Epoch 805/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0097\n",
            "Epoch 806/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0085\n",
            "Epoch 807/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0099\n",
            "Epoch 808/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0084 \n",
            "Epoch 809/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0079\n",
            "Epoch 810/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0106\n",
            "Epoch 811/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0165 \n",
            "Epoch 812/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0083 \n",
            "Epoch 813/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0080 \n",
            "Epoch 814/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0095\n",
            "Epoch 815/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0100\n",
            "Epoch 816/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0097 \n",
            "Epoch 817/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0095 \n",
            "Epoch 818/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0082\n",
            "Epoch 819/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0085\n",
            "Epoch 820/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0146\n",
            "Epoch 821/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0101\n",
            "Epoch 822/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0084 \n",
            "Epoch 823/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0094\n",
            "Epoch 824/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0083\n",
            "Epoch 825/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0081 \n",
            "Epoch 826/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0083\n",
            "Epoch 827/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0083\n",
            "Epoch 828/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0111\n",
            "Epoch 829/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0092 \n",
            "Epoch 830/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0085\n",
            "Epoch 831/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0098\n",
            "Epoch 832/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0088 \n",
            "Epoch 833/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0076 \n",
            "Epoch 834/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0079 \n",
            "Epoch 835/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0081 \n",
            "Epoch 836/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0090\n",
            "Epoch 837/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0084\n",
            "Epoch 838/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0106 \n",
            "Epoch 839/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0113\n",
            "Epoch 840/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0073 \n",
            "Epoch 841/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0083\n",
            "Epoch 842/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0090\n",
            "Epoch 843/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0089 \n",
            "Epoch 844/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0087\n",
            "Epoch 845/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0072\n",
            "Epoch 846/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0086 \n",
            "Epoch 847/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0098\n",
            "Epoch 848/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0091\n",
            "Epoch 849/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0083 \n",
            "Epoch 850/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0068 \n",
            "Epoch 851/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0080\n",
            "Epoch 852/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0071\n",
            "Epoch 853/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0082 \n",
            "Epoch 854/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0093 \n",
            "Epoch 855/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0086\n",
            "Epoch 856/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0084 \n",
            "Epoch 857/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0088 \n",
            "Epoch 858/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0082 \n",
            "Epoch 859/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0068 \n",
            "Epoch 860/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0077\n",
            "Epoch 861/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0073 \n",
            "Epoch 862/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0076\n",
            "Epoch 863/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0079\n",
            "Epoch 864/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0104 \n",
            "Epoch 865/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0088\n",
            "Epoch 866/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0086 \n",
            "Epoch 867/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0107 \n",
            "Epoch 868/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0113\n",
            "Epoch 869/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0127\n",
            "Epoch 870/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0134 \n",
            "Epoch 871/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0090 \n",
            "Epoch 872/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0088\n",
            "Epoch 873/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0084 \n",
            "Epoch 874/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0103 \n",
            "Epoch 875/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0096 \n",
            "Epoch 876/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0113\n",
            "Epoch 877/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0083\n",
            "Epoch 878/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0092\n",
            "Epoch 879/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0077 \n",
            "Epoch 880/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0114 \n",
            "Epoch 881/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0082 \n",
            "Epoch 882/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0080 \n",
            "Epoch 883/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0093\n",
            "Epoch 884/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0091 \n",
            "Epoch 885/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0087\n",
            "Epoch 886/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0085 \n",
            "Epoch 887/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0114\n",
            "Epoch 888/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0080\n",
            "Epoch 889/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0071 \n",
            "Epoch 890/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0082 \n",
            "Epoch 891/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0082\n",
            "Epoch 892/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0084 \n",
            "Epoch 893/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0092\n",
            "Epoch 894/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0076\n",
            "Epoch 895/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0082 \n",
            "Epoch 896/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0074\n",
            "Epoch 897/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0080 \n",
            "Epoch 898/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0081 \n",
            "Epoch 899/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0080\n",
            "Epoch 900/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0095 \n",
            "Epoch 901/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0075 \n",
            "Epoch 902/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0074 \n",
            "Epoch 903/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0090 \n",
            "Epoch 904/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0070\n",
            "Epoch 905/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0090 \n",
            "Epoch 906/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0078 \n",
            "Epoch 907/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0068\n",
            "Epoch 908/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0075\n",
            "Epoch 909/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0075\n",
            "Epoch 910/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0070\n",
            "Epoch 911/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0072 \n",
            "Epoch 912/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0065 \n",
            "Epoch 913/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0079 \n",
            "Epoch 914/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0069\n",
            "Epoch 915/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0066\n",
            "Epoch 916/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0065\n",
            "Epoch 917/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0068\n",
            "Epoch 918/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0064 \n",
            "Epoch 919/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0064\n",
            "Epoch 920/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0063\n",
            "Epoch 921/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0074\n",
            "Epoch 922/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0062\n",
            "Epoch 923/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0080\n",
            "Epoch 924/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0060\n",
            "Epoch 925/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0083\n",
            "Epoch 926/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0060 \n",
            "Epoch 927/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0087\n",
            "Epoch 928/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0058\n",
            "Epoch 929/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0058 \n",
            "Epoch 930/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0071 \n",
            "Epoch 931/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0060 \n",
            "Epoch 932/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0071\n",
            "Epoch 933/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0061\n",
            "Epoch 934/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0100\n",
            "Epoch 935/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0061 \n",
            "Epoch 936/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0065\n",
            "Epoch 937/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0064\n",
            "Epoch 938/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0070 \n",
            "Epoch 939/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0064 \n",
            "Epoch 940/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0101\n",
            "Epoch 941/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0081 \n",
            "Epoch 942/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0082\n",
            "Epoch 943/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0064\n",
            "Epoch 944/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0071 \n",
            "Epoch 945/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0098\n",
            "Epoch 946/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0104\n",
            "Epoch 947/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0067 \n",
            "Epoch 948/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0094\n",
            "Epoch 949/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0057\n",
            "Epoch 950/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0059 \n",
            "Epoch 951/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0069\n",
            "Epoch 952/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0068\n",
            "Epoch 953/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0100 \n",
            "Epoch 954/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0081 \n",
            "Epoch 955/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0069 \n",
            "Epoch 956/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0072 \n",
            "Epoch 957/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0085 \n",
            "Epoch 958/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0064 \n",
            "Epoch 959/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0064\n",
            "Epoch 960/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0073 \n",
            "Epoch 961/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0083\n",
            "Epoch 962/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0101 \n",
            "Epoch 963/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0086 \n",
            "Epoch 964/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0071\n",
            "Epoch 965/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0067 \n",
            "Epoch 966/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0065 \n",
            "Epoch 967/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0079\n",
            "Epoch 968/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0078 \n",
            "Epoch 969/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0063 \n",
            "Epoch 970/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0077\n",
            "Epoch 971/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0061\n",
            "Epoch 972/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0078\n",
            "Epoch 973/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0064\n",
            "Epoch 974/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0057 \n",
            "Epoch 975/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0061\n",
            "Epoch 976/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0064\n",
            "Epoch 977/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0056\n",
            "Epoch 978/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0066 \n",
            "Epoch 979/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0060\n",
            "Epoch 980/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0070\n",
            "Epoch 981/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0092 \n",
            "Epoch 982/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0058 \n",
            "Epoch 983/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0086 \n",
            "Epoch 984/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0065 \n",
            "Epoch 985/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0053\n",
            "Epoch 986/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0054 \n",
            "Epoch 987/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0059 \n",
            "Epoch 988/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0055\n",
            "Epoch 989/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0061 \n",
            "Epoch 990/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0194\n",
            "Epoch 991/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0070 \n",
            "Epoch 992/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0122 \n",
            "Epoch 993/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0269\n",
            "Epoch 994/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0098 \n",
            "Epoch 995/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0491 \n",
            "Epoch 996/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0303 \n",
            "Epoch 997/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0135\n",
            "Epoch 998/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0210\n",
            "Epoch 999/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0553 \n",
            "Epoch 1000/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0385\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train_lstm = np.squeeze(y_pred_train_lstm)\n",
        "y_pred_test_lstm = np.squeeze(y_pred_test_lstm)\n",
        "y_pred_train_lstm = (y_pred_train_lstm > 0.5).astype(int)\n",
        "acc_train=accuracy_score(y_train,y_pred_train_lstm)\n",
        "print('train accuracy',acc_train)\n",
        "y_pred_test_lstm = (y_pred_test_lstm > 0.5).astype(int)\n",
        "acc_test=accuracy_score(y_test,y_pred_test_lstm)\n",
        "print('test accuracy',acc_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV-kL64TMXle",
        "outputId": "277449bc-a463-4c71-8c81-2ebd23cf023d"
      },
      "id": "uV-kL64TMXle",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train accuracy 1.0\n",
            "test accuracy 0.6190476190476191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bilstm = Sequential()\n",
        "bilstm.add(Bidirectional(LSTM(100, return_sequences=True,kernel_regularizer=l2(0.01)), input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
        "#bilstm.add(Dropout(0.7))\n",
        "bilstm.add(Bidirectional(LSTM(50, return_sequences=False,kernel_regularizer=l2(0.01))))\n",
        "#bilstm.add(Dropout(0.7))\n",
        "bilstm.add(Dense(1, activation='sigmoid',kernel_regularizer=l2(0.01)))  # Binary classification output\n",
        "\n",
        "# Compile the model\n",
        "bilstm.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "bilstm.fit(X_train_reshaped, y_train, epochs=500, batch_size=32, verbose=1)\n",
        "\n",
        "# Predicting\n",
        "y_pred_train_bilstm = bilstm.predict(X_train_reshaped)\n",
        "y_pred_test_bilstm = bilstm.predict(X_test_reshaped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eei4skWSMXow",
        "outputId": "5e4995d5-b2e3-40a7-db03-b6dec3138793"
      },
      "id": "eei4skWSMXow",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - loss: 15.3273\n",
            "Epoch 2/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 13.6840\n",
            "Epoch 3/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 12.2031\n",
            "Epoch 4/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 10.8550 \n",
            "Epoch 5/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 9.6381\n",
            "Epoch 6/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.5575\n",
            "Epoch 7/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.5936\n",
            "Epoch 8/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.7202\n",
            "Epoch 9/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.9360 \n",
            "Epoch 10/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.2649\n",
            "Epoch 11/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.6872\n",
            "Epoch 12/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.1660 \n",
            "Epoch 13/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.6999 \n",
            "Epoch 14/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.3006\n",
            "Epoch 15/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.9349\n",
            "Epoch 16/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.6324 \n",
            "Epoch 17/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.4125 \n",
            "Epoch 18/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.1894\n",
            "Epoch 19/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.0003\n",
            "Epoch 20/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.8027 \n",
            "Epoch 21/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.6701\n",
            "Epoch 22/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.5546 \n",
            "Epoch 23/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.4225\n",
            "Epoch 24/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.3182 \n",
            "Epoch 25/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.2394 \n",
            "Epoch 26/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.1488\n",
            "Epoch 27/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.1050\n",
            "Epoch 28/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.0313\n",
            "Epoch 29/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.0007\n",
            "Epoch 30/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.8983\n",
            "Epoch 31/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.8643 \n",
            "Epoch 32/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.8182\n",
            "Epoch 33/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.7753\n",
            "Epoch 34/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.7509 \n",
            "Epoch 35/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6842\n",
            "Epoch 36/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6620\n",
            "Epoch 37/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.6352 \n",
            "Epoch 38/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.5962 \n",
            "Epoch 39/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.5466 \n",
            "Epoch 40/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5311\n",
            "Epoch 41/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4660 \n",
            "Epoch 42/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.4731 \n",
            "Epoch 43/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4341 \n",
            "Epoch 44/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.4075 \n",
            "Epoch 45/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.4154\n",
            "Epoch 46/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3976\n",
            "Epoch 47/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3820\n",
            "Epoch 48/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3735 \n",
            "Epoch 49/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3667\n",
            "Epoch 50/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3539\n",
            "Epoch 51/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3427 \n",
            "Epoch 52/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3390 \n",
            "Epoch 53/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3343\n",
            "Epoch 54/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3216\n",
            "Epoch 55/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3225 \n",
            "Epoch 56/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3149\n",
            "Epoch 57/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3154 \n",
            "Epoch 58/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.3015 \n",
            "Epoch 59/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2997\n",
            "Epoch 60/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.3043\n",
            "Epoch 61/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.3009\n",
            "Epoch 62/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2982\n",
            "Epoch 63/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2932\n",
            "Epoch 64/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.2875\n",
            "Epoch 65/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2868\n",
            "Epoch 66/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.2944\n",
            "Epoch 67/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.2883\n",
            "Epoch 68/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2851\n",
            "Epoch 69/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2785\n",
            "Epoch 70/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2817\n",
            "Epoch 71/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2814\n",
            "Epoch 72/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.2840\n",
            "Epoch 73/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2803\n",
            "Epoch 74/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2780\n",
            "Epoch 75/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2754\n",
            "Epoch 76/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.2723\n",
            "Epoch 77/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.2760\n",
            "Epoch 78/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.2736\n",
            "Epoch 79/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2706\n",
            "Epoch 80/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2743\n",
            "Epoch 81/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2792\n",
            "Epoch 82/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2749 \n",
            "Epoch 83/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2746\n",
            "Epoch 84/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2742 \n",
            "Epoch 85/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2674\n",
            "Epoch 86/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2651\n",
            "Epoch 87/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2693\n",
            "Epoch 88/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2670\n",
            "Epoch 89/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2654\n",
            "Epoch 90/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2628\n",
            "Epoch 91/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2625 \n",
            "Epoch 92/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2642 \n",
            "Epoch 93/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2622\n",
            "Epoch 94/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2652\n",
            "Epoch 95/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2617\n",
            "Epoch 96/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2648\n",
            "Epoch 97/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2597\n",
            "Epoch 98/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2597 \n",
            "Epoch 99/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2626\n",
            "Epoch 100/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2562\n",
            "Epoch 101/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2543 \n",
            "Epoch 102/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2598\n",
            "Epoch 103/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2581 \n",
            "Epoch 104/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2617\n",
            "Epoch 105/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2535\n",
            "Epoch 106/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2605\n",
            "Epoch 107/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2585 \n",
            "Epoch 108/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2582\n",
            "Epoch 109/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2611 \n",
            "Epoch 110/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2618 \n",
            "Epoch 111/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2618\n",
            "Epoch 112/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2612 \n",
            "Epoch 113/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2634\n",
            "Epoch 114/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2717\n",
            "Epoch 115/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2561\n",
            "Epoch 116/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2593\n",
            "Epoch 117/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2552 \n",
            "Epoch 118/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2571\n",
            "Epoch 119/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2580 \n",
            "Epoch 120/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2540 \n",
            "Epoch 121/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2527\n",
            "Epoch 122/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2509 \n",
            "Epoch 123/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2483\n",
            "Epoch 124/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2500\n",
            "Epoch 125/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2554 \n",
            "Epoch 126/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2499\n",
            "Epoch 127/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2486 \n",
            "Epoch 128/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2501 \n",
            "Epoch 129/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2533\n",
            "Epoch 130/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2493\n",
            "Epoch 131/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2499\n",
            "Epoch 132/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2478 \n",
            "Epoch 133/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2521\n",
            "Epoch 134/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2523\n",
            "Epoch 135/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2504 \n",
            "Epoch 136/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2542 \n",
            "Epoch 137/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2503\n",
            "Epoch 138/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2465\n",
            "Epoch 139/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2475 \n",
            "Epoch 140/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2500\n",
            "Epoch 141/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2467\n",
            "Epoch 142/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2460\n",
            "Epoch 143/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2506\n",
            "Epoch 144/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2521\n",
            "Epoch 145/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2435\n",
            "Epoch 146/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2414\n",
            "Epoch 147/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2466\n",
            "Epoch 148/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2452\n",
            "Epoch 149/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2450\n",
            "Epoch 150/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2426\n",
            "Epoch 151/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2461\n",
            "Epoch 152/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2429\n",
            "Epoch 153/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2419 \n",
            "Epoch 154/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2433\n",
            "Epoch 155/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2420\n",
            "Epoch 156/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2477\n",
            "Epoch 157/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2449\n",
            "Epoch 158/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2377\n",
            "Epoch 159/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2450\n",
            "Epoch 160/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2397 \n",
            "Epoch 161/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2415 \n",
            "Epoch 162/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2383 \n",
            "Epoch 163/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2419 \n",
            "Epoch 164/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2359\n",
            "Epoch 165/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2392\n",
            "Epoch 166/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2397 \n",
            "Epoch 167/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2370\n",
            "Epoch 168/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.2415\n",
            "Epoch 169/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2415\n",
            "Epoch 170/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.2371\n",
            "Epoch 171/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.2407\n",
            "Epoch 172/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2362\n",
            "Epoch 173/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2395\n",
            "Epoch 174/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2412\n",
            "Epoch 175/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2393\n",
            "Epoch 176/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.2438\n",
            "Epoch 177/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.2391\n",
            "Epoch 178/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.2458\n",
            "Epoch 179/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2391\n",
            "Epoch 180/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2355\n",
            "Epoch 181/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.2355\n",
            "Epoch 182/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2348\n",
            "Epoch 183/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.2374\n",
            "Epoch 184/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.2365\n",
            "Epoch 185/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2364\n",
            "Epoch 186/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2356\n",
            "Epoch 187/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2352\n",
            "Epoch 188/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2407\n",
            "Epoch 189/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.2324\n",
            "Epoch 190/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.2335\n",
            "Epoch 191/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2372\n",
            "Epoch 192/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2383\n",
            "Epoch 193/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2350\n",
            "Epoch 194/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2318\n",
            "Epoch 195/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2318 \n",
            "Epoch 196/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2372\n",
            "Epoch 197/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2340\n",
            "Epoch 198/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2310\n",
            "Epoch 199/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2372 \n",
            "Epoch 200/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2344 \n",
            "Epoch 201/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2390\n",
            "Epoch 202/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2335 \n",
            "Epoch 203/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2385 \n",
            "Epoch 204/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2320\n",
            "Epoch 205/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2348\n",
            "Epoch 206/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2323\n",
            "Epoch 207/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2351\n",
            "Epoch 208/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2305\n",
            "Epoch 209/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2303 \n",
            "Epoch 210/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2298\n",
            "Epoch 211/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2284\n",
            "Epoch 212/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2297\n",
            "Epoch 213/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2307 \n",
            "Epoch 214/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2296\n",
            "Epoch 215/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2266\n",
            "Epoch 216/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2278 \n",
            "Epoch 217/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2342 \n",
            "Epoch 218/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2329 \n",
            "Epoch 219/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2346 \n",
            "Epoch 220/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2354\n",
            "Epoch 221/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2305 \n",
            "Epoch 222/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2290\n",
            "Epoch 223/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2295 \n",
            "Epoch 224/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2250\n",
            "Epoch 225/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2291\n",
            "Epoch 226/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2287\n",
            "Epoch 227/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2299\n",
            "Epoch 228/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2265\n",
            "Epoch 229/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2314\n",
            "Epoch 230/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2290 \n",
            "Epoch 231/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2289\n",
            "Epoch 232/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2299\n",
            "Epoch 233/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2282 \n",
            "Epoch 234/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2270 \n",
            "Epoch 235/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2248\n",
            "Epoch 236/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2263\n",
            "Epoch 237/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2240\n",
            "Epoch 238/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2285\n",
            "Epoch 239/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2271 \n",
            "Epoch 240/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.2243\n",
            "Epoch 241/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.2296\n",
            "Epoch 242/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.2232\n",
            "Epoch 243/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.2274\n",
            "Epoch 244/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.2245\n",
            "Epoch 245/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2229\n",
            "Epoch 246/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2243\n",
            "Epoch 247/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2265\n",
            "Epoch 248/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2236 \n",
            "Epoch 249/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2214\n",
            "Epoch 250/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2237\n",
            "Epoch 251/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2222 \n",
            "Epoch 252/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2227\n",
            "Epoch 253/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2232\n",
            "Epoch 254/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2222 \n",
            "Epoch 255/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2208\n",
            "Epoch 256/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.2223\n",
            "Epoch 257/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2207\n",
            "Epoch 258/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2219\n",
            "Epoch 259/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2194 \n",
            "Epoch 260/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2188\n",
            "Epoch 261/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2213\n",
            "Epoch 262/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2197\n",
            "Epoch 263/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2209 \n",
            "Epoch 264/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2208\n",
            "Epoch 265/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.2199\n",
            "Epoch 266/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.2209\n",
            "Epoch 267/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2204\n",
            "Epoch 268/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2176\n",
            "Epoch 269/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.2236\n",
            "Epoch 270/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2182\n",
            "Epoch 271/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2251\n",
            "Epoch 272/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2200\n",
            "Epoch 273/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2182\n",
            "Epoch 274/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2187\n",
            "Epoch 275/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2182\n",
            "Epoch 276/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2188\n",
            "Epoch 277/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.2188\n",
            "Epoch 278/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.2171\n",
            "Epoch 279/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.2204\n",
            "Epoch 280/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2176\n",
            "Epoch 281/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.2214\n",
            "Epoch 282/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2209 \n",
            "Epoch 283/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2200 \n",
            "Epoch 284/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2239\n",
            "Epoch 285/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2205 \n",
            "Epoch 286/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2187 \n",
            "Epoch 287/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2217 \n",
            "Epoch 288/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2161 \n",
            "Epoch 289/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2243\n",
            "Epoch 290/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2147 \n",
            "Epoch 291/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2182\n",
            "Epoch 292/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2203 \n",
            "Epoch 293/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2223 \n",
            "Epoch 294/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2153 \n",
            "Epoch 295/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2150 \n",
            "Epoch 296/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2152\n",
            "Epoch 297/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2181\n",
            "Epoch 298/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2181 \n",
            "Epoch 299/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2169\n",
            "Epoch 300/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2193\n",
            "Epoch 301/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2212\n",
            "Epoch 302/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2154 \n",
            "Epoch 303/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2196\n",
            "Epoch 304/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2137 \n",
            "Epoch 305/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2165\n",
            "Epoch 306/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2132\n",
            "Epoch 307/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2152 \n",
            "Epoch 308/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2183\n",
            "Epoch 309/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2158 \n",
            "Epoch 310/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2145 \n",
            "Epoch 311/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2136\n",
            "Epoch 312/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2127 \n",
            "Epoch 313/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2188\n",
            "Epoch 314/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2175\n",
            "Epoch 315/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2164 \n",
            "Epoch 316/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2213\n",
            "Epoch 317/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2179\n",
            "Epoch 318/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2125\n",
            "Epoch 319/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2190 \n",
            "Epoch 320/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2168\n",
            "Epoch 321/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2168\n",
            "Epoch 322/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2165 \n",
            "Epoch 323/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2193\n",
            "Epoch 324/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2146\n",
            "Epoch 325/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2141\n",
            "Epoch 326/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2178\n",
            "Epoch 327/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2134\n",
            "Epoch 328/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2121\n",
            "Epoch 329/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2128 \n",
            "Epoch 330/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2133\n",
            "Epoch 331/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2109\n",
            "Epoch 332/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2137\n",
            "Epoch 333/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2108 \n",
            "Epoch 334/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2154\n",
            "Epoch 335/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2127 \n",
            "Epoch 336/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2096\n",
            "Epoch 337/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2132\n",
            "Epoch 338/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2093\n",
            "Epoch 339/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2094\n",
            "Epoch 340/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2119\n",
            "Epoch 341/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2111\n",
            "Epoch 342/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2123 \n",
            "Epoch 343/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2130\n",
            "Epoch 344/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2092 \n",
            "Epoch 345/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2067 \n",
            "Epoch 346/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2100 \n",
            "Epoch 347/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2106\n",
            "Epoch 348/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2100\n",
            "Epoch 349/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2110 \n",
            "Epoch 350/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2112 \n",
            "Epoch 351/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2089\n",
            "Epoch 352/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2119\n",
            "Epoch 353/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2123\n",
            "Epoch 354/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2212\n",
            "Epoch 355/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2153\n",
            "Epoch 356/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2109 \n",
            "Epoch 357/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2120\n",
            "Epoch 358/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2112\n",
            "Epoch 359/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2096\n",
            "Epoch 360/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2102\n",
            "Epoch 361/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2093\n",
            "Epoch 362/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.2074\n",
            "Epoch 363/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.2089\n",
            "Epoch 364/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2056\n",
            "Epoch 365/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.2104\n",
            "Epoch 366/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.2082\n",
            "Epoch 367/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2067\n",
            "Epoch 368/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2058\n",
            "Epoch 369/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2111\n",
            "Epoch 370/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2083\n",
            "Epoch 371/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2096\n",
            "Epoch 372/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2075\n",
            "Epoch 373/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.2092\n",
            "Epoch 374/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.2090\n",
            "Epoch 375/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2065\n",
            "Epoch 376/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2113\n",
            "Epoch 377/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2105\n",
            "Epoch 378/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2077\n",
            "Epoch 379/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2111 \n",
            "Epoch 380/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2080\n",
            "Epoch 381/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2065\n",
            "Epoch 382/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2132\n",
            "Epoch 383/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2106\n",
            "Epoch 384/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2041 \n",
            "Epoch 385/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2059\n",
            "Epoch 386/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2066\n",
            "Epoch 387/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2083\n",
            "Epoch 388/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2087 \n",
            "Epoch 389/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2081\n",
            "Epoch 390/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2067\n",
            "Epoch 391/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2061 \n",
            "Epoch 392/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2103\n",
            "Epoch 393/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2081\n",
            "Epoch 394/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2047\n",
            "Epoch 395/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2058\n",
            "Epoch 396/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2080\n",
            "Epoch 397/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2053\n",
            "Epoch 398/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2044\n",
            "Epoch 399/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2055\n",
            "Epoch 400/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2082\n",
            "Epoch 401/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2066\n",
            "Epoch 402/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2040 \n",
            "Epoch 403/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2052\n",
            "Epoch 404/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2048 \n",
            "Epoch 405/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2046\n",
            "Epoch 406/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2061\n",
            "Epoch 407/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2054 \n",
            "Epoch 408/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2054 \n",
            "Epoch 409/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2062\n",
            "Epoch 410/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2029\n",
            "Epoch 411/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2067\n",
            "Epoch 412/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2042\n",
            "Epoch 413/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2011\n",
            "Epoch 414/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2049 \n",
            "Epoch 415/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2057 \n",
            "Epoch 416/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2094 \n",
            "Epoch 417/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2103\n",
            "Epoch 418/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2080 \n",
            "Epoch 419/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2047\n",
            "Epoch 420/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2056\n",
            "Epoch 421/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2064 \n",
            "Epoch 422/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2044\n",
            "Epoch 423/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2014\n",
            "Epoch 424/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2039\n",
            "Epoch 425/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2054\n",
            "Epoch 426/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2039 \n",
            "Epoch 427/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2058 \n",
            "Epoch 428/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2033\n",
            "Epoch 429/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2031\n",
            "Epoch 430/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2030\n",
            "Epoch 431/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2049\n",
            "Epoch 432/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2027\n",
            "Epoch 433/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2045\n",
            "Epoch 434/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2015 \n",
            "Epoch 435/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2051 \n",
            "Epoch 436/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2062\n",
            "Epoch 437/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2015\n",
            "Epoch 438/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2034 \n",
            "Epoch 439/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2041\n",
            "Epoch 440/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2104\n",
            "Epoch 441/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2036\n",
            "Epoch 442/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2202\n",
            "Epoch 443/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2145\n",
            "Epoch 444/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2186\n",
            "Epoch 445/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2001\n",
            "Epoch 446/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2080\n",
            "Epoch 447/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2022\n",
            "Epoch 448/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2028\n",
            "Epoch 449/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2006 \n",
            "Epoch 450/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2039 \n",
            "Epoch 451/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2011\n",
            "Epoch 452/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2067\n",
            "Epoch 453/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2045\n",
            "Epoch 454/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2050\n",
            "Epoch 455/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2034\n",
            "Epoch 456/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2007\n",
            "Epoch 457/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2048\n",
            "Epoch 458/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1999 \n",
            "Epoch 459/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2012\n",
            "Epoch 460/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2002\n",
            "Epoch 461/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1989\n",
            "Epoch 462/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1983\n",
            "Epoch 463/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2008\n",
            "Epoch 464/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2009\n",
            "Epoch 465/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1999\n",
            "Epoch 466/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2015\n",
            "Epoch 467/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2024\n",
            "Epoch 468/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.2016\n",
            "Epoch 469/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.2027\n",
            "Epoch 470/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2002\n",
            "Epoch 471/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.2008\n",
            "Epoch 472/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2017\n",
            "Epoch 473/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.2010\n",
            "Epoch 474/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.1996\n",
            "Epoch 475/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.1990\n",
            "Epoch 476/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2005\n",
            "Epoch 477/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1975\n",
            "Epoch 478/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2004\n",
            "Epoch 479/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.1980\n",
            "Epoch 480/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.2011\n",
            "Epoch 481/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.1986\n",
            "Epoch 482/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.2020\n",
            "Epoch 483/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1960\n",
            "Epoch 484/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.2055\n",
            "Epoch 485/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1973\n",
            "Epoch 486/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2038\n",
            "Epoch 487/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.1986\n",
            "Epoch 488/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2030 \n",
            "Epoch 489/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2012\n",
            "Epoch 490/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.2000\n",
            "Epoch 491/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1999\n",
            "Epoch 492/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1981\n",
            "Epoch 493/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2027\n",
            "Epoch 494/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2000 \n",
            "Epoch 495/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2016 \n",
            "Epoch 496/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1999\n",
            "Epoch 497/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1975\n",
            "Epoch 498/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1953\n",
            "Epoch 499/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1988\n",
            "Epoch 500/500\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1989 \n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train_bilstm = np.squeeze(y_pred_train_bilstm)\n",
        "y_pred_test_bilstm = np.squeeze(y_pred_test_bilstm)"
      ],
      "metadata": {
        "id": "9eH7gRN-MIyI"
      },
      "id": "9eH7gRN-MIyI",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train_bilstm = (y_pred_train_bilstm > 0.5).astype(int)\n",
        "acc_train=accuracy_score(y_train,y_pred_train_bilstm)\n",
        "print('train accuracy',acc_train)\n",
        "y_pred_test_bilstm = (y_pred_test_bilstm > 0.5).astype(int)\n",
        "acc_test=accuracy_score(y_test,y_pred_test_bilstm)\n",
        "print('test accuracy',acc_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZdn2gcDMm-Q",
        "outputId": "315ae6a2-92d0-4594-f6a9-dfa41a689285"
      },
      "id": "aZdn2gcDMm-Q",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train accuracy 1.0\n",
            "test accuracy 0.5238095238095238\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}